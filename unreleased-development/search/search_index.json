{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"readme/","title":"Overview","text":""},{"location":"readme/#multi-cluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multi-cluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":"<p>KubeStellar is a Cloud Native Computing Foundation (CNCF) Sandbox project that simplifies the deployment and configuration of applications across multiple Kubernetes clusters. It provides a seamless experience akin to using a single cluster, and it integrates with the tools you're already familiar with, eliminating the need to modify existing resources.</p> <p>KubeStellar is particularly beneficial if you're currently deploying in a single cluster and are looking to expand to multiple clusters, or if you're already using multiple clusters and are seeking a more streamlined developer experience.</p> <p> KubeStellar High Level View </p> <p>The use of multiple clusters offers several advantages, including:</p> <ul> <li>Separation of environments (e.g., development, testing, staging)</li> <li>Isolation of groups, teams, or departments</li> <li>Compliance with enterprise security or data governance requirements</li> <li>Enhanced resiliency, including across different clouds</li> <li>Improved resource availability</li> <li>Access to heterogeneous resources</li> <li>Capability to run applications on the edge, including in disconnected environments</li> </ul> <p>In a single-cluster setup, developers typically access the cluster and deploy Kubernetes objects directly. Without KubeStellar, multiple clusters are usually deployed and configured individually, which can be time-consuming and complex.</p> <p>KubeStellar simplifies this process by allowing developers to define a binding policy between clusters and Kubernetes objects. It then uses your regular single-cluster tooling to deploy and configure each cluster based on these binding policies, making multi-cluster operations as straightforward as managing a single cluster. This approach enhances productivity and efficiency, making KubeStellar a valuable tool in a multi-cluster Kubernetes environment.</p>"},{"location":"readme/#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"readme/#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"readme/#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Effi Ofer\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40 Andrey Odarenko\ud83d\udc40 Aashni Manroa\ud83d\udc40 Kevin Roche\ud83d\udc40 Nick Masluk\ud83d\udc40 Francois Abel\ud83d\udc40 Nir Rozenbaum\ud83d\udc40 Maroon Ayoub\ud83d\udc40 Graham White\ud83d\udc40"},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\nresource.exclusions: |\n- apiGroups:\n- \"tenancy.kcp.io\"\nkinds:\n- \"ClusterWorkspace\"\nclusters:\n- \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/kubestellar), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"Check out KubeStellar working with IBM's Maximo Visual Inspection (MVI):","text":"<p>Medium - Deployment and configuration of MVI-Edge using KubeStellar</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-demo-day","title":"MVI and KubeStellar Demo Day","text":""},{"location":"Community/partners/mvi/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-in-the-news","title":"MVI and KubeStellar in the news","text":""},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>As we can see from the blog and the demo, Turbonomic talks to KubeStellar via GitOps. The scheduling decisions are passed from Turbonomic to KubeStellar in two steps: 1. Turbo -&gt; GitHub repository. 2. GitHub repository -&gt; KubeStellar.</p> <p>For the 1st step (Turbonomic -&gt; GitHub repository), a controller named \"change reconciler\" creates PRs against the GitHub repository, where the PRs contains changes to scheduling decisions.</p> <p>There's also a piece of code which intercepts Turbonomic actions and creates CRs for the above change reconciler.</p> <p>For the 2nd step (GitHub repository-&gt; KubeStellar), we can use Argo CD. The detailed procedure to integrate Argo CD with KubeStellar is documented here.</p> <p>As we can see from the blog and the demo, Turbonomic collects data from edge clusters. This is made possible by installing kubeturbo into each of the edge clusters.</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#see-also","title":"See Also","text":"<p>Part of our documentation of how to contribute is meant to be viewed directly at GitHub. See it there.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#see-here","title":"See Here","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>This project is following the CNCF Code of Conduct. </p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>As contributors, maintainers, and participants in the CNCF community, and in the interest of fostering an open and welcoming community, we pledge to respect all people who participate or contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, attending conferences or events, or engaging in other community or project activities.</p> <p>We are committed to making participation in the CNCF community a harassment-free experience for everyone, regardless of age, body size, caste, disability, ethnicity, level of experience, family status, gender, gender identity and expression, marital status, military or veteran status, nationality, personal appearance, race, religion, sexual orientation, socioeconomic status, tribe, or any other dimension of diversity.</p>"},{"location":"Contribution%20guidelines/coc/#scope","title":"Scope","text":"<p>This code of conduct applies: * within project and community spaces, * in other spaces when an individual CNCF community participant's words or actions are directed at or are about a CNCF project, the CNCF community, or another CNCF community participant.</p>"},{"location":"Contribution%20guidelines/coc/#cncf-events","title":"CNCF Events","text":"<p>CNCF events that are produced by the Linux Foundation with professional events staff are governed by the Linux Foundation Events Code of Conduct available on the event page. This is designed to be used in conjunction with the CNCF Code of Conduct.</p>"},{"location":"Contribution%20guidelines/coc/#our-standards","title":"Our Standards","text":"<p>The CNCF Community is open, inclusive and respectful. Every member of our community has the right to have their identity respected.</p> <p>Examples of behavior that contributes to a positive environment include but are not limited to:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> <li>Using welcoming and inclusive language</li> </ul> <p>Examples of unacceptable behavior include but are not limited to:</p> <ul> <li>The use of sexualized language or imagery</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment in any form</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Violence, threatening violence, or encouraging others to engage in violent behavior</li> <li>Stalking or following someone without their consent</li> <li>Unwelcome physical contact</li> <li>Unwelcome sexual or romantic attention or advances</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul> <p>The following behaviors are also prohibited: * Providing knowingly false or misleading information in connection with a Code of Conduct investigation or otherwise intentionally tampering with an investigation. * Retaliating against a person because they reported an incident or provided information about an incident as a witness.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct.  By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing a CNCF project.  Project maintainers who do not follow or enforce the Code of Conduct may be temporarily or permanently removed from the project team.</p>"},{"location":"Contribution%20guidelines/coc/#reporting","title":"Reporting","text":"<p>For incidents occurring in the KubeStellar community, contact the KubeStellar Code of Conduct Committee of Conduct Committee. You can expect a response within three business days.</p> <p>For other projects, or for incidents that are project-agnostic or impact multiple CNCF projects, please contact the CNCF Code of Conduct Committee via conduct@cncf.io.  Alternatively, you can contact any of the individual members of the CNCF Code of Conduct Committee to submit your report. For more detailed instructions on how to submit a report, including how to submit a report anonymously, please see our Incident Resolution Procedures. You can expect a response within three business days.</p> <p>For incidents occurring at CNCF event that is produced by the Linux Foundation, please contact eventconduct@cncf.io.</p>"},{"location":"Contribution%20guidelines/coc/#enforcement","title":"Enforcement","text":"<p>Upon review and investigation of a reported incident, the CoC response team that has jurisdiction will determine what action is appropriate based on this Code of Conduct and its related documentation. </p> <p>For information about which Code of Conduct incidents are handled by project leadership, which incidents are handled by the CNCF Code of Conduct Committee, and which incidents are handled by the Linux Foundation (including its events team), see our Jurisdiction Policy.</p>"},{"location":"Contribution%20guidelines/coc/#amendments","title":"Amendments","text":"<p>Consistent with the CNCF Charter, any substantive changes to this Code of Conduct must be approved by the Technical Oversight Committee.</p>"},{"location":"Contribution%20guidelines/coc/#acknowledgements","title":"Acknowledgements","text":"<p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 2.0 available at http://contributor-covenant.org/version/2/0/code_of_conduct/</p>"},{"location":"Contribution%20guidelines/governance/","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"Contribution%20guidelines/governance/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"Contribution%20guidelines/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"Contribution%20guidelines/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"Contribution%20guidelines/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"Contribution%20guidelines/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"Contribution%20guidelines/governance/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"Contribution%20guidelines/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p>"},{"location":"Contribution%20guidelines/onboarding/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"Contribution%20guidelines/operations/code-management/","title":"Code management","text":""},{"location":"Contribution%20guidelines/operations/code-management/#code-management","title":"Code Management","text":"<p>Fork kubestellar into your own repo, create a local branch, set upstream to kubestellar, add and commit changes to local branch, and squash your commits</p>"},{"location":"Contribution%20guidelines/operations/code-management/#initial-setup","title":"Initial setup","text":""},{"location":"Contribution%20guidelines/operations/code-management/#fork-the-github-kubestellar-repo-into-your-own-github-repo","title":"Fork the Github kubestellar repo into your own Github repo:","text":"<p>You can do this either 1: from the kubestellar Github website using the \"Fork\" button or 2: by using the git fork command from your local git command line interface, such as git bash.</p> <p>copy the forked repo from Github to your local system by using the \"git clone\" command or by downloading the repository's zip file.</p> <p>In your new local forked repo, set upstream to kubestellar main</p> <p>check what your repository's remote settings are <pre><code>git remote -v\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#set-upstream-to-use-kubestellar","title":"Set upstream to use kubestellar:","text":"<pre><code>git remote add upstream git@github.com:kubestellar/kubestellar.git\n</code></pre> <p>For example: <pre><code>$ git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\n\n$ git remote add upstream git@github.com:kubestellar/kubestellar.git\n\n$ git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\nupstream        git@github.com:kubestellar/kubestellar.git (fetch)\nupstream        git@github.com:kubestellar/kubestellar.git (push)\n\n$ git fetch upstream\nEnter passphrase for key '/c/Users/owner/.ssh/id_rsa':\nremote: Enumerating objects: 60394, done.\nremote: Counting objects: 100% (5568/5568), done.\nremote: Compressing objects: 100% (255/255), done.\nremote: Total 60394 (delta 4768), reused 5457 (delta 4706), pack-reused 54826\nReceiving objects: 100% (60394/60394), 52.38 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (34496/34496), completed with 415 local objects.\n\n$ git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#ongoing-contributions","title":"Ongoing contributions","text":""},{"location":"Contribution%20guidelines/operations/code-management/#prior-to-working-on-an-issue","title":"Prior to working on an issue","text":"<p>Ensure that you personal repository if up to date with the kubestellar repository. You can do this by opening your github repository page, check that the selected branch is \"main\", and press the \"sync fork\" button.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#select-an-issue-to-work-on-and-create-a-local-branch","title":"Select an issue to work on and create a local branch,","text":"<p>Create a local branch for your work, preferably including the issue number in the branch name</p> <p>for example if working on issue #11187, then you might name your local branch \"issue-1187\" <pre><code>git checkout -b issue-1187\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#as-you-work-and-change-files-you-should-try-to-commit-relatively-small-pieces-of-work-using-the-following-commands","title":"As you work and change files, you should try to commit relatively small pieces of work, using the following commands","text":"<pre><code>git add (there are several options you can specify for the git add command)\n\ngit commit -m \"your message\"\n\ngit push -u origin branch-name (-u sets upstream to origin which is your remote github repository)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-have-completed-your-work-and-tested-it-locally-then-you-should-perform-a-squash-of-the-git-commits-to-make-the-upcoming-push-request-more-manageable","title":"When you have completed your work and tested it locally, then you should perform a squash of the git commits to make the upcoming push request more manageable.","text":"<p>To perform a squash, checkout the branch you want to squash, 1. use the \"git log\" command to see the history of commits to the branch 2. Count the number of commits you want to squash 3. use the \"git rebase -i HEAD~n\" where n is the number of commits you would like to squash together. (There are other ways to do this) 4. The text editor you have configured to use with git should automatically open your source and you will see a list of commits preceded by \"pick\". Leaving the first \"pick\" as it is, replace the remaining \"pick\"s with \"squash\"es.  5. Save the text file and exit the editor. 6. The text editor will open again to let you edit comments for your new squashed commit. 7. Make your edits if any and save and exit the file. The commits will then be squashed into one commit.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-are-done-with-the-squash-push-your-changes-to-your-remote-branch-you-can-either","title":"When you are done with the squash, push your changes to your remote branch. You can either:","text":"<p><pre><code>git push -u origin &lt;branch-name&gt;\n\nor \n\ngit push --force-with-lease\n</code></pre> Note: if using the git push -u origin  command, the -u only needs to specified the first time you push. It will set tracking for subsequent pushes to the branch. On the other hand, keeping the -u in the command does no particular harm."},{"location":"Contribution%20guidelines/operations/code-management/#run-actions-automated-workflow-tests-manually-in-your-personal-github-repository","title":"Run Actions (automated workflow tests) manually in your personal Github repository","text":"<ol> <li>Select the \"Actions\" tab toward the upper left of your github personal web page. This will cause a list of Actions to show.</li> <li>Select the action you wish to execute from the list of Actions. For example you might chose \"docs-ecutable - example1\". Note: docs-ecutable should be described in a separate section. But in a nutshell it's a Continuous Integration automation technique of embedding scripts and data within the body of documentation, and then parsing and executing those scripts which in turn interpret and execute source code from a branch that you designate. It's somewhat similar to Travis. So the Action \"docs-ecutable - example1\" executes scripts and data embedded within the documentation for the Example 1 scenario, described in the Kubestellar documents. Those scripts will run using the source code pointed to by the next step, step 3.</li> <li>Select the source code branch you wish to exercise by following the next 3 steps:</li> <li>select the black and white \"Run Workflow\" on the right side of your github web page. This will open a dialog box.</li> <li>within the dialog box, select the branch you wish to exercise by opening the dropdown labeled \"use workflow from\"</li> <li>within the dialog box, select the green \"Run Workflow\" button  Your selected Action workflow will execute and the results will be available when it completes.  </li> </ol>"},{"location":"Contribution%20guidelines/operations/code-management/#create-a-pull-request-pr-from-your-github-repo-branch-in-order-to-request-review-and-approval-from-the-kubestellar-team","title":"Create a Pull Request (PR) from your Github repo branch in order to request review and approval from the Kubestellar team","text":"<p>Take a look at https://github.com/kubestellar/kubestellar/blob/main/CONTRIBUTING.md</p> <p>You can create a Pull Request from your Github web repository by selecting the \"Compare &amp; pull request\" button.</p> <p>You will be presented with a Github web page titled Comparing Changes, which allows you to enter metadata regarding your pull request</p> <p>Reference the issue you are addressing ( add #issue-number) Add one of the listed emojis to the first character of the title of your new PR indicating the type of issue (bug fix, feature, etc) Complete the summary description field Complete the Related issue field by inserting the issue number preceded by the # character, for example \"#1187\" Decide whether this is a draft PR or if it's ready for review, and select the option you want by expanding on the Create Pull Request button. Assign a label to the PR from the available list of labels (a drop down list on the right side of the web page)</p> <p>Kubestellar CI pipeline:</p> <p>Prow (https://docs.prow.k8s.io/docs/overview/)</p>"},{"location":"Contribution%20guidelines/operations/document-management/","title":"Overview","text":""},{"location":"Contribution%20guidelines/operations/document-management/#overview","title":"Overview","text":""},{"location":"Contribution%20guidelines/operations/document-management/#websites","title":"Websites","text":"<p>We have two web sites, as follows.</p> <ul> <li><code>https://kubestellar.io</code>. This is hosted by GoDaddy and administered by Andy Anderson. It contains a few redirects. The most important is that <code>https://kubestellar.io/</code> redirects to <code>https://docs.kubestellar.io/</code>.</li> <li><code>https://docs.kubestellar.io</code>. This is a GitHub pages website based on the <code>github.com/kubestellar/kubestellar/</code> repository.</li> </ul> <p>Also, a contributor has their own copy of the website, at <code>https://${repo_owner}.github.io/${fork_name}</code>.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#github-pages","title":"GitHub pages","text":"<p>Our documentation is powered by mike and MkDocs. MkDocs is powered by Python-Markdown. These are immensely configurable and extensible. You can see our MkDocs configuration in <code>docs/mkdocs.yml</code>. Following are some of the choices we have made.</p> <ul> <li>The MkDocs theme is Material for MkDocs.</li> <li>MkDocs plugin awesome-pages for greater control over how navigation links are shown.</li> <li>MkDocs plugin macros.</li> <li>Our own slightly improved vintage of the <code>include-markdown</code> MkDocs plugin, allowing the source to be factored into re-used files.</li> <li>Python-Markdown extension SuperFences, supporting fenced code blocks that play nice with other markdown features.</li> <li>Python-Markdown extension Highlight, for syntax highlighting of fenced code.</li> <li>Pygments for even fancier code highlighting.</li> <li>MkDocs plugin mkdocs-static-i18n to support multiple languages. We currently only have documentation in English. If you're interested in contributing translations, please let us know!</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs.  We have a Python requirements file in <code>requirements.txt</code>, and a Makefile target that builds a Python virtual environment and installs the requirements there.  You can either install those requirements into your global Python environment or use the Makefile target.  To install those requirements into your global Python environment, do the following usual thing.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Alternatively, use the following commands to use the Makefile target to construct an adequate virtual environment and enter it.</p> <pre><code>( cd ..; make venv )\n. venv/bin/activate\n</code></pre> <p>Then, using your chosen environment with the requirements installed, build and serve the documents with the following command.</p> <p><pre><code>mkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\ncd docs\nmike set-default main\ncd ..\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"Contribution%20guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p><code>mike</code> has a concept of aliases. We currently maintain only one alias.</p> <ul> <li><code>latest</code> (https://docs.kubestellar.io/latest), for the latest regular release.</li> </ul> <p>The publishing workflow updates these aliases. The latest regular release is determined by picking the first version listed by <code>mike list</code> that matches the regexp <code>release-[0-9.]*</code>.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-from-the-branch-named-main","title":"Publishing from the branch named \"main\"","text":"<p>The branch named \"main\" also gets published as a \"version\" on the website, but with a different name. This is not done by <code>mike</code> aliasing, because that only adds a version. The branch named \"main\" is published as the version named \"unreleased-development\".</p>"},{"location":"Contribution%20guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' Getting Started recipe</p>"},{"location":"Contribution%20guidelines/operations/document-management/#jinja-templating","title":"Jinja templating","text":"<p>Our documentation stack includes Jinja. The Jinja constructs --- {# comment #}, {{ expression }}, and {% statement %} --- can appear in the markdown sources.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"Contribution%20guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kubestellar/kubestellar\n- site_url: https://docs.kubestellar.io/unreleased-development\n- repo_default_file_path: kubestellar\n- repo_short_name: kubestellar/kubestellar\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kubestellar/kubestellar\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: latest\n- ks_latest_regular_release: 0.23.1\n- ks_latest_release: 0.24.0-alpha.2\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"Contribution%20guidelines/operations/document-management/#navigation-website-menu","title":"Navigation (website menu)","text":"<p>The navigation for the documentation is also configured in  docs/mkdocs.yml. The section which begins with nav: lays out the navigation structure and which markdown files correspond to each topic. </p>"},{"location":"Contribution%20guidelines/operations/document-management/#page-variables","title":"Page variables","text":"<p>A markdown source file can contribute additional variables by defining them in <code>name: value</code> lines at the start of the file, set off by lines of triple dashes. For example, suppose a markdown file begins with the following.</p> <pre><code>---\nshort_name: example1\nmanifest_name: 'docs/content/Coding Milestones/PoC2023q1/example1.md'\n---\n</code></pre> <p>These variables can be referenced as {{ page.meta.short_name }} and {{ page.meta.manifest_name }}.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: Include Markdown </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: Included Markdown </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>NOTE: the docs-ecutable technology does not apply Jinja, at any stage; Jinja source inside executed code blocks will not be expanded by Jinja but rather seen directly by <code>bash</code>.</p> <p>Here are some examples of how we use codeblocks.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-and-executed","title":"Seen and executed","text":"<p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#executed-but-not-seen","title":"Executed but not seen","text":"<p>(Think hard before hiding stuff from your reader.)</p> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed","title":"Seen but not executed","text":"<p>(To avoid confusing readers of the HTML, this should be used only for output seen in a shell session.)</p> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed-and-no-copy-button","title":"Seen but not executed and no copy button","text":"<p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#other-language-specific-highlighting","title":"Other language-specific highlighting","text":"<p>For other language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n- Home: index.md\n- QuickStart: Getting-Started/quickstart.md\n- Contributing: - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblock-with-a-title","title":"Codeblock with a title","text":"<p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"Contribution%20guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kubestellar/kubestellar at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-where-resolver.yml</li> <li>An example workflow using the newer technology is located in our kubestellar/kubestellar repo at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-example1.yml</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-original-secret-sauce","title":"The original secret sauce:","text":"<ul> <li>The original code that made all this possible is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>The Jinja constructs are not expanded by this code.</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kubestellar/kubestellar/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-new-and-improved-secret-sauce","title":"The new and improved secret sauce:","text":"<ul> <li>The newer code for executing bash snippets in documentation is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/execute-html.sh<ul> <li>This code parses the HTML generated by MkDocs to extract all the fenced code blocks tagged for the \"shell\" language.</li> <li>This HTML scraping is relatively easy because it does not have to work on general HTML but only the HTML generated by our stack from our sources. The use of the option setting <code>pygments_lang_class: true</code> for the Python-Markdown extension <code>pymdownx.highlight</code> plays a critical role, getting the source language into the generated HTML.</li> <li>Because it reads the generated HTML, invisible code blocks are not extracted.</li> <li>Because it reads the generated HTML, the Jinja constructs have their usual effects.</li> <li>This script is given the name of the HTML file to read and the current working directory to establish at the start of the extracted bash.</li> <li>It then creates a file called 'generated_script.sh' which is then run.</li> </ul> </li> </ul> <p>All of this is invoked in a target in our Makefile <pre><code>.PHONY: execute-html\nexecute-html: venv\n    . $(VENV)/activate; \\\ncd docs; \\\nmkdocs build; \\\nscripts/execute-html.sh \"$$PWD/..\" \"generated/$(MANIFEST)/index.html\"\n</code></pre></p> <p>The <code>make</code> target requires the variable <code>MANIFEST</code> to be set to the directory that contains the generated <code>index.html</code> file, relative to 'https://github.com/kubestellar/kubestellar/docs/generated'. This is the name of the markdown source file, relative to 'https://github.com/kubestellar/kubestellar/docs/content' and with the <code>.md</code> extension dropped.</p> How to 'make' a docs-ecutable target<pre><code>make MANIFEST=\"Coding Milestones/PoC2023q1/example1\" execute-html\n</code></pre> <p>note: this target has no special needs for quoting --- which is not to deny the quoting that your shell needs.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"Contribution%20guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>These appear in the branch named <code>gh-pages</code> and redirect from the root to the version named <code>latest</code>. The one named <code>index.html</code> is managed by <code>mike set-default</code>. The other should be kept consistent.</p> <ul> <li>https://github.com/kubestellar/kubestellar/blob/gh-pages/home.html</li> <li>https://github.com/kubestellar/kubestellar/blob/gh-pages/index.html</li> </ul> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/latest\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.22.0\", \"title\": \"release-0.22.0\", \"aliases\": [\"latest\"]}, {\"version\": \"release-0.22.0-rc3\", \"title\": \"release-0.22.0-rc3\", \"aliases\": []}, {\"version\": \"release-0.21.2\", \"title\": \"release-0.21.2\", \"aliases\": []}, {\"version\": \"release-0.21.2-rc1\", \"title\": \"release-0.21.2-rc1\", \"aliases\": []}, {\"version\": \"release-0.21.1\", \"title\": \"release-0.21.1\", \"aliases\": []}, {\"version\": \"release-0.21.0\", \"title\": \"release-0.21.0\", \"aliases\": []}, {\"version\": \"release-0.14\", \"title\": \"release-0.14\", \"aliases\": []}]\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kubestellar/kubestellar KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> - switch to the 'release' branch, switch to /docs and run 'mike deploy' for the release branch. Add the alias 'latest' for the latest release  <pre><code>git checkout release-0.22\ngit pull\nmike deploy --push --rebase --update-aliases release-0.22 latest\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-delete-a-rendering-of-a-branch","title":"How to delete a rendering of a branch","text":"<p>Use <code>mike delete $branch_name</code>, either acting locally on your checked out <code>gh-pages</code> branch (after pull and before git commit and push) or acting more directly on the remote repo using <code>--remote</code> and <code>--push</code>. See the mike delete command doc.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in <code>.github/workflows/docs-gen-and-push.yml</code>. This workflow is triggered either manually or by a push to a branch named <code>main</code> or <code>release-&lt;something&gt;</code> or <code>doc-&lt;something&gt;</code>. This workflow will actually do something ONLY if either (a) it is acting on the shared GitHub repository at <code>github.com/kubestellar/kubestellar</code> and on behalf of the repository owner or (b) it is acting on a contributor's fork of that repo and on behalf of that same contributor. The published site appears at <code>https://pages.github.io/kubestellar/${branch}</code> in case (a) and at <code>https://${repo_owner}.github.io/${fork_name}/${branch}</code> in case (b). This workflow will build and publish a website version whose name is the same as the name of the branch that it is working on. This workflow will also update the relevant <code>mike</code> alias, if necessary.</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/","title":"Testing website PRs","text":""},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#testing-a-kubestellar-documentation-pr","title":"Testing a KubeStellar documentation PR","text":"<p>Here are the steps to checkout a git pull request for local testing.</p> <p>STEP-1: Checkout the Pull Request</p> <p>Helpers: GitHub, DevOpsCube</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#11-use-the-pull-request-number-to-fetch-origin-note-be-sure-to-check-out-the-right-branch","title":"1.1 Use the pull request number to fetch origin (note: be sure to check out the right branch!)","text":"<p>\u00a0\u00a0\u00a0\u00a0Fetch the reference to the pull request based on its ID number, creating a new branch locally. Replace ID with your PR # and BRANCH_NAME with the desired branch name.</p> <p> <code>git fetch origin pull/ID/head:BRANCH_NAME</code> </p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#12-switch-to-the-new-branch","title":"1.2 Switch to the new branch","text":"<p>\u00a0\u00a0\u00a0Checkout the BRANCH_NAME where you have all the changes from the pull request.</p> <p> <code>git switch BRANCH_NAME</code> </p> <p>\u00a0\u00a0\u00a0\u00a0At this point, you can do anything you want with this branch. You can run some local tests, or merge other branches into the branch.</p> <p>STEP-2: Test and Build the Documentation (optional)</p> <p>\u00a0\u00a0\u00a0\u00a0Use this procedure if you want to view and modify the documentation in the branch you have checked out.</p> <p>Helpers: KubeStellar/docs, MkDocs</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#21-install-mkdocs-and-its-requirements","title":"2.1 Install MkDocs and its requirements","text":"<pre><code>  cd docs\n  pip install mkdocs\n  pip install -r requirements.txt  \n</code></pre>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#22-build-and-view-the-documentation","title":"2.2 Build and view the documentation","text":"<p> <code>mkdocs serve</code></p> <p>\u00a0\u00a0\u00a0\u00a0Next, open a browser to http://127.0.0.1:8000 and review the changes.</p>"},{"location":"Contribution%20guidelines/security/security/","title":"Policy","text":""},{"location":"Contribution%20guidelines/security/security/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"Contribution%20guidelines/security/security/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"Contribution%20guidelines/security/security/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"Contribution%20guidelines/security/security_contacts/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"common-subs/placeholder/","title":"Placeholder","text":"<ul> <li>This is a placeholder file to allow  building the navigation</li> </ul>"},{"location":"direct/acquire-hosting-cluster/","title":"A cluster for KubeFlex hosting","text":"<p>This document tells you what makes a Kubernetes cluster suitable to serve as the KubeFlex hosting cluster and shows some ways to create such a cluster.</p>"},{"location":"direct/acquire-hosting-cluster/#requirements-on-the-kubeflex-hosting-cluster","title":"Requirements on the KubeFlex hosting cluster","text":"<p>The KubeFlex hosting cluster needs to run an Ingress controller with SSL passthrough enabled.</p>"},{"location":"direct/acquire-hosting-cluster/#connectivity-from-clients","title":"Connectivity from clients","text":"<p>The clients in KubeStellar need to be able to open a TCP connection to where the Ingress controller is listening for HTTPS connections.</p> <p>The clients in KubeStellar comprise the following.</p> <ul> <li>The OCM Agent and the OCM Status Add-On Agent in each WEC.</li> <li>The KubeStellar controller-manager and the transport controller for each WDS, running in the KubeFlex hosting cluster.</li> </ul> <p>TODO: finish writing this subsection for real. Following are some clues.</p> <p>When everything runs on one machine, the defaults just work. When core and some WECs are on different machines, it gets more challenging. When the KubeFlex hosting cluster is an OpenShift cluster with a public domain name, the defaults just work.</p> <p>After the Getting Started setup, I looked at an OCM Agent (klusterlet-agent, to be specific) and did not find a clear passing of kubeconfig. I found adjacent Secrets holding kubeconfigs in which <code>cluster[0].cluster.server</code> was <code>https://kubeflex-control-plane:31048</code>. Note that <code>kubeflex-control-plane</code> is the name of the Docker container running <code>kind</code> cluster serving as KubeFlex hosting cluster. I could not find an explanation for the port number 31048; that Docker container maps port 443 inside to 9443 on the outside.</p> <p><code>kflex init</code> takes a command line flag <code>--domain string</code> described as <code>domain for FQDN (default \"localtest.me\")</code>.</p>"},{"location":"direct/acquire-hosting-cluster/#creating-a-hosting-cluster","title":"Creating a hosting cluster","text":"<p>Following are some ways to create a Kubernetes cluster that is suitable to use as a KubeFlex hosting cluster. This is not an exhaustive list.</p>"},{"location":"direct/acquire-hosting-cluster/#create-and-init-a-kind-cluster-as-hosting-cluster-with-kflex","title":"Create and init a kind cluster as hosting cluster with kflex","text":"<p>The following command will use <code>kind</code> to create a cluster with an Ingress controller with SSL passthrough AND ALSO proceed to install the KubeFlex implementation in it and set your current kubeconfig context to access that cluster as admin.</p> <pre><code>kflex init --create-kind\n</code></pre>"},{"location":"direct/acquire-hosting-cluster/#create-and-init-a-kind-cluster-as-hosting-cluster-with-curl-to-bash-script","title":"Create and init a kind cluster as hosting cluster with curl-to-bash script","text":"<p>There is a bash script at <code>https://raw.githubusercontent.com/kubestellar/kubestellar/v0.23.1/scripts/create-kind-cluster-with-SSL-passthrough.sh</code> that can be fed directly into <code>bash</code> and will create a <code>kind</code> cluster AND ALSO initialize it as the KubeFlex hosting cluster. This script accepts the following command line flags.</p> <ul> <li><code>--name name</code>: set a specific name of the kind cluster (default: kubestellar).</li> <li><code>--port port</code>: map the specified host port to the kind cluster port 443 (default: 9443).</li> <li><code>--nowait</code>: when given, the script proceeds without waiting for the nginx ingress patching to complete.</li> <li><code>--nosetcontext</code>: when given, the script does not change the current kubectl context to the newly created cluster.</li> <li><code>-X</code> enable verbose execution of the script for debugging.</li> </ul>"},{"location":"direct/acquire-hosting-cluster/#create-a-k3d-cluster","title":"Create a k3d cluster","text":"<p>This has been tested with version 5.6.0 of k3d.</p> <ol> <li> <p>Create a K3D hosting cluster with nginx ingress controller:     <pre><code>k3d cluster create -p \"9443:443@loadbalancer\" --k3s-arg \"--disable=traefik@server:*\" kubeflex\nhelm install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --version 4.6.1 --namespace ingress-nginx --create-namespace\n</code></pre></p> </li> <li> <p>When we use kind, the name of the container is kubeflex-control-plane and that is what we use     in the internal URL for <code>--force-internal-endpoint-lookup</code>.    Here the name of the container created by K3D is <code>k3d-kubeflex-server-0</code> so we rename it:     <pre><code>docker stop k3d-kubeflex-server-0\ndocker rename k3d-kubeflex-server-0 kubeflex-control-plane\ndocker start kubeflex-control-plane\n</code></pre>     Wait 1-2 minutes for all pods to be restarted.     Use the following command to confirm all are fully running:     <pre><code>kubectl --context k3d-kubeflex get po -A\n</code></pre></p> </li> <li> <p>Enable SSL passthrough:    We are using nginx ingress with tls passthrough.    The current install for kubeflex installs also nginx ingress but specifically for kind.    To specify passthrough for K3D, edit the ingress placement controller with the following command and add <code>--enable-ssl-passthrough</code> to the list of arguments for the container     <pre><code>kubectl edit deployment ingress-nginx-controller -n ingress-nginx  </code></pre></p> </li> </ol>"},{"location":"direct/architecture/","title":"KubeStellar Architecture","text":"<p>KubeStellar provides multi-cluster deployment of Kubernetes objects, controlled by simple <code>BindingPolicy</code> objects, where Kubernetes objects are expressed in their native format with no wrapping or bundling. The high-level architecture for KubeStellar is illustrated in Figure 1.</p> <p> Figure 1 - High Level Architecture </p> <p>KubeStellar relies on the concept of spaces. A Space is an abstraction to represent an API service that  behaves like a Kubernetes kube-apiserver (including the persistent storage behind it)  and the subset of controllers in the kube-controller-manager that are concerned with  API machinery generalities (not management of containerized workloads).  A KubeFlex <code>ControlPlane</code> is an example. A regular Kubernetes cluster is another example. Users can use spaces to perform these tasks:</p> <ol> <li>Create Workload Definition Spaces (WDSes) to store the definitions of their workloads. A Kubernetes workload is an application that runs on Kubernetes. A workload can be made by a  single Kubernetes object or several objects that work together.</li> <li>Create Inventory and Transport Spaces (ITSes) to manage the inventory of clusters and  the transport of workloads.</li> <li>Register and label Workload Execution Clusters (WECs) with the Inventory and  Transport Space, to keep track of the available clusters and their characteristics.</li> <li>Define <code>BindingPolicy</code> to specify what objects and where should be  deployed on the WECs.</li> <li>Submit objects in the native Kubernetes format to the WDSes,  and let the <code>BindingPolicy</code> govern which WECs should receive them.</li> <li>Check the status of submitted objects from the WDS.</li> </ol> <p>In KubeStellar, users can assume a variety of roles and responsibilities.  These roles could range from system administrators and application owners  to CISOs and DevOps Engineers. However, for the purpose of this document,  we will not differentiate between these roles. Instead we will use the term  'user' broadly, without attempting to make distinctions among roles.</p> <p>Examples of user interactions with KubeStellar are illustrated in the KubeStellar Usage Example Scenarios document.</p> <p>The KubeStellar architecture has the following main modules.</p> <ul> <li> <p>KubeFlex. KubeStellar builds on the services of KubeFlex, using it to keep track of, and possibly provide, the Inventory and Transport spaces and the Workload Description spaces. Each of those appears as a <code>ControlPlane</code> object in the KubeFlex hosting cluster.</p> </li> <li> <p>KubeStellar Controller Manager: this module is instantiated once per WDS and is responsible for watching <code>BindingPolicy</code> objects and create from it a matching <code>Binding</code> object that contains list of references to the concrete objects and list of references to the concrete clusters, and for updating the status of objects in the WDS.</p> </li> <li> <p>Pluggable Transport Controller: this module is instantiated once per WDS and is responsible for delivering workload objects from the WDS to the ITS according to <code>Binding</code> objects.</p> </li> <li> <p>Space Manager: This module manages the lifecycle of spaces.</p> </li> <li> <p>OCM Cluster Manager: This module is instantiated once per ITS and syncs objects from that ITS to the Workload Execution  Clusters (WECs). In the ITS, each mailbox namespace is associated with one WEC. Objects  that are put in a mailbox namespace are delivered to the matching WEC.</p> </li> <li> <p>OCM Agent: This module registers the WEC to the OCM Hub, watches for  ManifestWork.v1.work.open-cluster-management.io objects and unwraps and syncs the objects into the WEC.</p> </li> <li> <p>OCM Status Add-On Controller: This module is instantiated once per ITS and uses the OCM Add-on Framework to get the OCM Status Add-On Agent installed in each WEC along with supporting RBAC objects.</p> </li> <li> <p>OCM Status Add-On Agent: This module watches AppliedManifestWork.v1.work.open-cluster-management.io objects  to find objects that are synced by the OCM agent, gets their status  and updates <code>WorkStatus</code> objects in the ITS namespace associated with the WEC.</p> </li> </ul> <p> Figure 2 - Main Modules </p>"},{"location":"direct/architecture/#kubestellar-controller-manager","title":"KubeStellar Controller Manager","text":"<p>This module manages binding controller, and status controller. </p> <ul> <li> <p>The binding controller watches <code>BindingPolicy</code> and workload objects on the Workload Definition Space (WDS), and maintains a <code>Binding</code> object for each <code>BindingPolicy</code> in the WDS. The <code>Binding</code> object contains references to the concrete list of workload objects and references to the concrete list of clusters that were selected by the <code>BindingPolicy</code> selectors.</p> </li> <li> <p>The status controller watches for WorkStatus objects on the ITS and updates the status of objects in the WDS when singleton status is requested in the <code>BindingPolicy</code> for those objects. </p> </li> </ul> <p>There is one instance of a KubeStellar Controller Manager for each WDS.  Currently this controller-manager runs in the KubeFlex hosting cluster and is responsible for installing the required  CRDs in the associated WDS. More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#pluggable-transport-controller","title":"Pluggable Transport Controller","text":"<ul> <li>The pluggable transport controller watches <code>Binding</code> objects on the WDS, and maintains in the Inventory and Transport Space (ITS)  a wrapped object per <code>Binding</code> to be delivered.</li> <li>This controller is pluggable and can potentially be implemented using different options. Currently the only option we support is based on the Open Cluster Management Project </li> </ul> <p>There is one instance of the pluggable transport controller for each WDS.  Currently this controller runs in an executable process. This is a work in progress and we're working on running this controller in a dedicated pod. More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#space-manager","title":"Space Manager","text":"<p>The Space Manager handles the lifecycle of spaces.  KubeStellar uses the KubeFlex project for space management. In KubeFlex, a space is named a <code>ControlPlane</code>, and we will use  both terms in this document. KubeStellar currently prereqs KubeFlex to  provide one or more spaces. We plan to make this optional in the near future.</p> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as k8s, a basic Kubernetes API Server with a subset of kube controllers, and  vcluster: a virtual cluster that runs on the hosting cluster based on the vCluster Project. More detailed information on the different types of control planes and architecture are described in the KubeFlex Architecture.</p> <p>There are currently two roles for spaces managed by KubeFlex: Inventory and Transport Space  (ITS) and Workload Description Space (WDS). The former runs the OCM Cluster Manager on a vcluster-type control plane, and the latter runs on a k8s-type control plane.</p> <p>An ITS holds the inventory and the mailbox namespaces. The inventory is anchored by ManagedCluster.v1.cluster.open-cluster-management.io objects that describe the WECs. For each WEC there may also be a <code>ConfigMap</code> object (in the <code>customization-properties</code> namespace) that carries additional properties of that WEC; this <code>ConfigMap</code> is used in customizing the workload to the WEC. The mailbox namespaces and their contents are transport implementation details that users do not need to deal with. Each mailbox namespace corresponds 1:1 with a WEC and holds <code>ManifestWork</code> objects managed by the central KubeStellar controllers.</p> <p>A WDS holds user workload objects and the user's objects that form the interface to KubeStellar control.  Currently, the user control objects are <code>BindingPolicy</code> and <code>Binding</code> objects. Future development may define more kinds of control objects hosted in the WDS.</p> <p>KubeFlex provides the ability to start controllers connected to a Control Plane API Server or to deploy Helm Charts into a Control Plane API server with post-create hooks. This feature is currently adopted for KubeStellar modules startup, as it allows to create a Workload Description Space (WDS) and start the KubeStellar Controller Manager, and create an Inventory and Transport Space (ITS) in a <code>vcluster</code> and install the Open Cluster Management Hub there.</p>"},{"location":"direct/architecture/#ocm-cluster-manager","title":"OCM Cluster Manager","text":"<p>This module is based on the Open Cluster Management Project, a community-driven project that focuses on multicluster and multicloud scenarios for Kubernetes apps.  It provides APIs for cluster registration, work distribution and much more.  The project is based on a hub-spoke architecture, where a single hub cluster  handles the distribution of workloads through manifests, and one or more spoke clusters  receive and apply the workload objects from the manifests. In Open Cluster Management, spoke clusters  are called managed clusters, and the component running on the hub cluster is the cluster manager. Manifests provide a summary for the status of each object, however in some use  cases this might not be sufficient as the full status for objects may be required.  OCM provides an add-on framework that allows to automatically install additional  agents on the managed clusters to provide specific features. This framework is used to install the status add-on on all managed clusters. KubeStellar currently exposes users directly to OCM inventory management and WEC registration.</p>"},{"location":"direct/architecture/#ocm-agent","title":"OCM Agent","text":"<p>The OCM Agent Module (a.k.a klusterlet) has two main controllers: the registration agent and the work agent. </p> <p>The registration agent is responsible for registering  a new cluster into OCM. The agent creates an unaccepted ManagedCluster into  the hub cluster along with a temporary CertificateSigningRequest.v1.certificates (CSR) object.  The cluster will be accepted by the hub control plane if the CSR is approved and  signed by any certificate provider setting filling <code>.status.certificate</code> with legit  X.509 certificates, and the ManagedCluster resource is approved by setting  <code>.spec.hubAcceptsClient</code> to true in the spec. Upon approval, the registration  agent observes the signed certificate and persists them as a local secret  named <code>hub-kubeconfig-secret</code> (by default in the <code>open-cluster-management-agent</code> namespace)  which will be mounted to the other fundamental components of klusterlet such as  the work agent. The registration process in OCM is called double opt-in mechanism,  which means that a successful cluster registration requires both sides of approval  and commitment from the hub cluster and the managed cluster.</p> <p>The work agent monitors the <code>ManifestWork</code> resource in the cluster namespace  on the hub cluster. The work agent tracks all the resources defined in ManifestWork  and updates its status. There are two types of status in ManifestWork: the resourceStatus  tracks the status of each manifest in the ManifestWork, and conditions reflects the overall  status of the ManifestWork. The work agent checks whether a resource is Available,  meaning the resource exists on the managed cluster, and Applied, meaning the resource  defined in ManifestWork has been applied to the managed cluster. To ensure the resources  applied by ManifestWork are reliably recorded, the work agent creates an <code>AppliedManifestWork</code>  on the managed cluster for each ManifestWork as an anchor for resources relating to ManifestWork.  When ManifestWork is deleted, the work agent runs a Foreground deletion, and that ManifestWork  will stay in deleting state until all its related resources have been fully cleaned in the managed  cluster.</p>"},{"location":"direct/architecture/#ocm-status-add-on-controller","title":"OCM Status Add-On Controller","text":"<p>This module automates the installation of the OCM status add-on agent  on all managed clusters. It is based on the  OCM Add-on Framework,  which is a framework that helps developers to develop extensions  for working with multiple clusters in custom cases. A module based on  the add-on framework has two components: a controller and an  agent. The controller interacts with the add-on manager to register  the add-on, manage the distribution of the add-on to all clusters, and set  up the RBAC permissions required by the add-on agent to interact with the mailbox  namespace associated with the managed cluster. More specifically, the status  add-on controller sets up RBAC permissions to allow the add-on agent to  list and get <code>ManifestWork</code> objects and create and update WorkStatus objects.</p>"},{"location":"direct/architecture/#ocm-status-add-on-agent","title":"OCM Status Add-On Agent","text":"<p>The OCM Status Add-On Agent is a controller that runs alongside the OCM Agent  in the managed cluster. Its primary function is to track objects delivered  by the work agent and report the full status of those objects back to the ITS.  Other KubeStellar controller(s) then propagate and/or summarize that status information into the WDS. The OCM Status Add-On Agent watches AppliedManifestWork.v1.work.open-cluster-management.io objects in the WEC to observe the status reported there by the OCM Agent. Each <code>AppliedManifestWork</code> object is specific to one workload object, and holds both the local (in the WEC) status from that object and a reference to that object. For each <code>AppliedManifest</code>, the OCM Status Add-On Agent maintains a corresponding <code>WorkStatus</code> object in the relevant mailbox namespace in the ITS. Such a <code>WorkStatus</code> object also is about exactly one workload object, so that status updates for one object do not require updates of a whole bundle. A <code>WorkStatus</code> object holds the status of a workload object and a reference to that object. </p> <p>Installing the Status Add-On Agent in the WEC causes status to be returned to <code>WorkStatus</code> objects for all downsynced objects.</p>"},{"location":"direct/architecture/#kubestellar-controllers-architecture","title":"KubeStellar Controllers Architecture","text":"<p>The KubeStellar controllers architecture is based on common patterns and best  practices for Kubernetes controllers, such as the  Kubernetes Sample Controller.  A Kubernetes controller uses informers to watch for changes in Kubernetes objects, caches to store the objects, event handlers to react to events, work queues for parallel processing of tasks, and a reconciler to ensure the actual state matches the desired state. However, that pattern has been extended to provide the following features:</p> <ul> <li>Using dynamic informers</li> <li>Starting informers on all API Resources (except some that do not need   watching)</li> <li>Informers and Listers references are maintained in a hash map and   indexed by GVR (Group, Version, Resource) of the watched objects.</li> <li>Using a common work queue and set of workers, where the key is defined as follows:<ul> <li>Key is a struct instead than a string, and contains the following:<ul> <li>GVR of the informer and lister for the object that generated the   event</li> <li>Structured Name of the object </li> <li>For delete event: Shallow copy of the object being deleted. This   is required for objects that need to be deleted   from the managed clusters (WECs)</li> </ul> </li> </ul> </li> <li>Starting &amp; stopping informers dynamically based on creation or   deletion of CRDs (which add/remove APIs on the WDS).</li> <li>One client connected to the WDS space and one (or more in the future)   to connect to one or more OCM shards.<ul> <li>The WDS-connected client is used to start the dynamic   informers/listers for most API resources in the WDS</li> <li>The OCM-connected client is used to start informers/listers for OCM   ManagedClusters and to copy/update/remove the wrapped objects   into/from the OCM mailbox namespaces.</li> </ul> </li> </ul> <p>There are three controllers in the KubeStellar controller manager:</p> <ul> <li>Binding Controller - one client connected to the WDS space and one   (or more in the future) to connect to one or more ITS shards.<ul> <li>The WDS-connected client is used to start the dynamic   informers/listers for most API resources in the WDS.</li> <li>The OCM-connected client is used to start informers/listers for OCM   ManagedClusters. This is a temporary state until cluster inventory abstraction is implemented and decoupled from OCM (and then this client should be removed and we would need to use client to inventory space).</li> </ul> </li> <li>Transport controller - one client connected to the WDS space    and one client (or more in the future) to connect to one or more ITS shards.<ul> <li>The OCM-connected client is used to copy/update/remove the wrapped objects   into/from the OCM mailbox namespaces.</li> </ul> </li> <li>Status controller - TODO </li> </ul>"},{"location":"direct/architecture/#binding-controller","title":"Binding Controller","text":"<p>The Binding controller is responsible for watching workload objects and  <code>BindingPolicy</code> objects, and maintains for each of the latter a matching <code>Binding</code> object in the WDS.  A <code>Binding</code> object is mapped 1:1 to a <code>BindingPolicy</code> object and contains references to the concrete list of workload  objects and references to the concrete list of destinations that were selected by the policy.</p> <p>The architecture and the event flow of the code for create/update object events is illustrated in Figure 3 (some details might be omitted to make the flow easier to understand). </p> <p> Figure 3 - Binding Controller </p> <p>At startup, the controller code sets up the dynamic informers, the event handler and the work queue as follows:</p> <ul> <li>lists all API preferred resources (using discovery client's ServerPreferredResources()   to return only one preferred storage version for API group)</li> <li>Filters out some resources</li> <li>For each resource:<ul> <li>Creates GVR key</li> <li>Registers Event Handler</li> <li>Starts Informer</li> <li>Indexes informer and lister in a map by GVR key</li> </ul> </li> <li>Waits for all caches to sync</li> <li>Starts N workers to process work queue</li> </ul> <p>The reflector is started as part of the informer and watches specific resources on the WDS API Server; on create/update/delete object events it puts a copy of the object into the local cache. The informer invokes the event handler. The handler implements the event handling functions (AddFunc, UpdateFunc, DeleteFunc)</p> <p>A typical event flow for a create/update object event will run as follows:</p> <ol> <li> <p>Informer invokes the event handler AddFunc or UpdateFunc</p> </li> <li> <p>The event handler does some filtering (for example, to ignore update     events where the object content is not modified) and then creates a     key to store a reference to the object in the work queue. The key     contains the GVR key used to retrieve a reference to the informer     and lister for that kind of object, and a namespace + name key to     retrieve the actual object. Storing the key in the work queue is a     common pattern in client-go as the object may have changed in the     cache (which is kept updated by the reflector) by the time a worker     gets a copy from the work queue. Workers should always receive the     key and use it to retrieve the object from the cache.</p> </li> <li> <p>A worker pulls a key from the work queue, and then does the     following processing:</p> <ul> <li>Uses the GVR key to get the reference to the lister for the     object</li> <li>Gets the object from the lister cache using the NamespacedName of     the object.</li> <li>If the object was not found (because it was deleted) worker returns.     A delete event for that object consumed by the event handler enqueues     a key for Object Deleted.</li> <li>Gets the lister for BindingPolicy objects and list all binding-policies</li> <li>Iterates on all binding-policies, and for each of them:<ul> <li>Evaluates whether the object matches the downsync selection    criteria in the <code>BindingPolicy</code>.</li> <li>Whether the object is a match or not, the worker notes it through the in-memory representation     of the relevant <code>Binding</code>. If the noting of an object results in a change in the in-memory representation of the  <code>Binding</code>, the worker enqueues the latter for syncing.</li> <li>If a matched <code>BindingPolicy</code> has <code>WantSingletonReportedState</code> set to true (**see note below), the object is     labeled with a special label in order to be able to track it for status reporting.</li> <li>If no matching <code>BindingPolicy</code> has <code>WantSingletonReportedState</code> set to true, the worker sets the label value    to false if the label exists.</li> </ul> </li> <li>Worker returns and is ready to pick other keys from the queue.</li> </ul> </li> </ol> <p>WantSingletonReportedState: currently it is the user's responsibility to make sure that a binding-policy that sets <code>WantSingletonReportedState</code> to true is not in conflict with other binding-policies that do the same, and that the binding-policy selects only a single cluster.</p> <p>There are other event flows, based on the object GVK and type of event.  Error conditions may cause the re-enqueing of keys, resulting in retries. The following sections broadly describe these flows.</p>"},{"location":"direct/architecture/#object-deleted","title":"Object Deleted","text":"<p>When an object is deleted from the WDS, the handler\u2019s DeleteFunc is invoked. A shallow copy of the object is added to a field in the key  before pushing it to the work queue. Then:</p> <ul> <li> <p>Worker pulls the key from the work queue</p> </li> <li> <p>Flow continues the same way as in the create/update scenario, however   the deletedObject field in the key indicates that the object has been   deleted and that it needs to be removed from all clusters.</p> </li> <li> <p>In-memory representations of affected bindings are updated to remove the object. If any are indeed affected, they are enqueued for syncing.</p> </li> <li> <p>Worker returns and is ready to pick other keys from the queue.</p> </li> </ul>"},{"location":"direct/architecture/#bindingpolicy-created-or-updated","title":"BindingPolicy Created or Updated","text":"<p>Worker pulls key from queue; if it is a binding-policy and it has not been deleted (deletion timestamp not set) it follows the following flow:</p> <ul> <li>Re-enqueues all objects to force re-evaluation: this is done by    iterating all GVR-indexed listers, listing objects for each lister   and re-enqueuing the key for each object.</li> <li>Notes the <code>BindingPolicy</code> to create an empty in-memory representation for its <code>Binding</code>.</li> <li>Lists ManagedClusters and finds the matching clusters using the label selector expression for clusters.</li> <li>If there are matching clusters, the in-memory <code>Binding</code> representation is updated with the list of clusters.</li> <li>Enqueues the representation of the relevant <code>Binding</code> for syncing.</li> </ul> <p>Re-enqueuing all object keys forces the re-evaluation of all objects vs. all binding-policies. This is a shortcut as it would be more efficient to re-evaluate all objects vs. the changed binding-policy only, but it saves some additional complexity in the code.</p>"},{"location":"direct/architecture/#bindingpolicy-deleted","title":"BindingPolicy Deleted","text":"<p>When the binding controller first processes a new <code>BindingPolicy</code>, the binding  controller sets a finalizer on it. The Worker pulls a key from queue; if it is  a <code>BindingPolicy</code> and it has been deleted (deletion timestamp is set) it follows the flow below:</p> <ul> <li>Deletes the in-memory representation of the <code>Binding</code>. Note that the actual <code>Binding</code> object   is garbage collected due to the deletion of the <code>BindingPolicy</code> and the latter being an owner of the former (using <code>OwnerReference</code>).</li> <li>If the <code>BindingPolicy</code> had <code>WantSingletonReportedState</code> set to true, the worker enqueues all objects selected by the <code>BindingPolicy</code> for re-evaluation of the label.</li> <li>Deletes <code>BindingPolicy</code> finalizer.</li> </ul>"},{"location":"direct/architecture/#binding-syncing","title":"Binding Syncing","text":"<p>When a binding is enqueued for syncing, the worker pulls the key from the queue and follows the flow below:</p> <ul> <li>If an in memory representation of the binding is not found, the worker returns.</li> <li>The key is used to retrieve the object from the WDS.<ul> <li>If the object is not found, then a <code>Binding</code> object should be created. For the convenience of the flow,   the worker sets the \"retrived object\" as an empty <code>Binding</code> object with the appropriate Meta fields,   including the <code>BindingPolicy</code> object as the single owner reference.</li> </ul> </li> <li>The worker compares the in-memory representation of the binding with the retrieved object.<ul> <li>If the two are the same, the worker returns.</li> <li>If the two are different, the worker updates the binding object resource to reflect the state of the   in-memory representation.</li> </ul> </li> </ul>"},{"location":"direct/architecture/#new-crd-added","title":"New CRD Added","text":"<p>When a new CRD is added, the binding controller needs to start a new informer to watch instances of the new CRD on the WDS.</p> <p>The worker pulls a key from queue and creates a GVR Key; if it is a CRD and not deleted:</p> <ul> <li>Checks if an informer for that GVR was already started, return if that   is the case.</li> <li>If not, creates a new informer</li> <li>Registers the event handler for the informer (same one used for all   other api resources)</li> <li>Starts the new informer with a stopper channel, so that it can be   stopped later on by closing the channel.</li> <li>adds informer, lister and stopper channel references to the hashmap   indexed by the GVR key.</li> </ul>"},{"location":"direct/architecture/#crd-deleted","title":"CRD Deleted","text":"<p>When a CRD is deleted, the controller needs to stop the informer that was used to watch instances of that CRD on the WDS. This is because informers on CRs will keep on throwing exceptions for missing CRDs.</p> <p>The worker pulls a key from queue and creates a GVR Key; if it is a CRD and it has been deleted:</p> <ul> <li>Uses the GVR key to retrieve the stopper channel for the informer.</li> <li>Closes the stopper channel</li> <li>Removes informer, lister and stopper channel references from the   hashmap indexed by the GVR key.</li> </ul>"},{"location":"direct/architecture/#status-controller","title":"Status Controller","text":"<p>The status controller watches for <code>Binding</code> objects in the WDS and <code>WorkStatus</code> objects in the ITS. For WDS objects propagated by a <code>BindingPolicy</code> with the flag <code>wantSingletonReportedState</code> set to true, the status controller manages the statuses of those objects with the corresponding statuses found in the <code>Workstatus</code> objects, if necessary.</p> <p>The <code>WorkStatus</code> objects are created and updated on the ITS by the OCM Status Add-On Agent described above.</p> <p>The high-level flow for the singleton status update is described in Figure 4.</p> <p> Figure 4 - Status Controller </p>"},{"location":"direct/architecture/#transport-controller","title":"Transport Controller","text":"<p>The transport controller is pluggable and allows the option to plug different implementations of the transport interface. The interface between the plugin and the generic code is a Go language interface (in <code>pkg/transport/transport.go</code>) that the plugin has to implement. This interface requires the following from the plugin.</p> <ul> <li>Upon registration of a new WEC, plugin should create a namespace for the WEC in the ITS and delete the namespace once the WEC registration goes away (mailbox namespace per WEC);</li> <li>Plugin must be able to wrap any number of objects into a single wrapped object;</li> <li>Have an agent that can be used to pull the wrapped objects from the mailbox namespace and apply them to the WEC. A single example for such an agent is an agent that runs on the WEC and watches the wrapped object in the corresponding namespace in the central hub and is able to unwrap it and apply the objects to the WEC. </li> <li>Have inventory representation for the clusters.</li> </ul> <p>The above list is required in order to comply with SIG Multi-Cluster Work API.</p> <p>Each plugin has an executable with a <code>main</code> function that calls the generic code (in <code>pkg/transport/cmd/generic-main.go</code>), passing the plugin object that implements the plugin interface. The generic code does the rule-based customization; the plugin is given customized objects. The generic code also ensures that the namespace named \"customization-properties\" exists in the ITS.</p> <p>KubeStellar currently has one transport plugin implementation which is based on CNCF Sandbox project Open Cluster Management. OCM transport plugin implements the above interface and supplies a function to start the transport controller using the specific OCM implementation. Code is available here. We expect to have more transport plugin options in the future.</p> <p>The following section describes how transport controller works, while the described behavior remains the same no matter which transport plugin is selected. The high level flow for the transport controller is described in Figure 5.</p> <p> Figure 5 - Transport Controller </p> <p>The transport controller is driven by <code>Binding</code> objects in the WDS. There is a 1:1 correspondence between <code>Binding</code> objects and <code>BindingPolicy</code> objects, but the transport controller does not care about the latter. A <code>Binding</code> object contains (a) a list of references to workload objects that are selected for distribution and (b) a list of references to the destinations for those workload objects.</p> <p>The transport controller watches for <code>Binding</code> objects on the WDS, using an informer. Upon every add, update, and delete event from that informer, the controller puts a reference to that <code>Binding</code> object in its work queue. The transport controller also has informers on the inventory objects (both <code>ManagedCluster</code> and their associated <code>ConfigMap</code>) and on the wrapped objects (<code>ManifestWork</code>). Forked goroutines process items from the work queue. For a reference to a control or workload object, that processing starts with retrieving the informer's cached copy of that object. </p> <p>The transport controller also maintains a finalizer on each Binding object. When processing a reference to a <code>Binding</code> object that no longer exists, the transport controller has nothing more to do (because it processes the deletion before removing its finalizer).</p> <p>When processing a reference to a <code>Binding</code> object that still exists, the transport controller looks at whether that <code>Binding</code> is in the process of being deleted. If so then the controller ensures that the corresponding wrapped object (<code>ManifestWork</code>) in the ITS no longer exists and then removes the finalizer from the <code>Binding</code>.</p> <p>When processing a <code>Binding</code> object that is not being deleted, the transport controller first ensures that the finalizer is on that object. Then the controller constructs an internal function from destination to the customized wrapped object for that destination. The controller then iterates over the <code>Binding</code>'s list of destinations and propagates the corresponding wrapped object (reported by the function just described) to the corresponding mailbox namespace.  Once the wrapped object is in the mailbox namespace of a cluster on the ITS, it's the agent responsibility to pull the wrapped object from there and apply/update/delete the workload objects on the WEC.</p> <p>To construct the function from destination to customized wrapped object, the transport controller reads the <code>Binding</code>'s list of references to workload objects. The controller reads those objects from the WDS using a Kubernetes \"dynamic\" client. Immediately upon reading each workload object, the controller applies the WEC-independent transforms (from the <code>CustomTransform</code> objects). After doing that for all the listed workload objects, the controller goes through those objects one-by-one and applies template expansion for each destination if the object requests template expansion. If any of those objects requests template expansion and has a string that actually involves template expansion: the controller accumulates a map from destination to slice of customized objects and then invokes the transport plugin on each of those slices, to ultimately produce the function from destination to wrapped object. If none of the selected workload objects actually involved any template expansion then the controller wraps the slice of workload objects to get one wrapped object and produces a constant function from destination to that one wrapped object. </p> <p>Transport controller is based on the controller design pattern and aims to bring the current state to the desired state. If a WEC was removed from the <code>Binding</code>, the transport controller will also make sure to remove the matching wrapped object(s) from the WEC's mailbox namespace.</p>"},{"location":"direct/architecture/#custom-transform-cache","title":"Custom transform cache","text":"<p>To support efficient application of the <code>CustomTransform</code> objects, the transport controller maintains a cache of the results of internalizing what the users are asking for. In relational algebra terms, that cache consists of the following relations.</p> <p>Relation \"USES\": has a row whenever the <code>Binding</code>'s list of workload objects uses the <code>GroupResource</code>.</p> column name type in key bindingName string yes gr metav1.GroupResource yes <p>Relation \"INSTRUCTIONS\": has a row saying what to do for each <code>GroupResource</code>.</p> column name type in key gr metav1.GroupResource yes removes SET(jsonpath.Query) no <p>Relation \"SPECS\": remembers the specs of <code>CustomTransform</code> objects.</p> column name type in key ctName string yes gr metav1.GroupResource no removes SET(string) no <p>The cache maintains the following invariants on those relations. Note how these invariants require removal of data that is no longer interesting.</p> <ol> <li>INSTRUCTIONS has a row for a given <code>GroupResource</code> if and only if USES has one or more rows for that <code>GroupResource</code>.</li> <li>SPECS has a row for a given <code>CustomTransform</code> name if and only if that <code>CustomTransform</code> contributed to an existing row in INSTRUCTIONS.</li> </ol> <p>Whenever it removes a row from INSTRUCTIONS due to loss of confidence in that row, the cache has the controller enqueue a reference to every related <code>Binding</code> from USES, so that eventually a revised row will be derived and applied to every dependent <code>Binding</code>.</p> <p>The interface to the cache is <code>customTransformCollection</code> and the implementation is in a <code>*customTransformCollectionImpl</code>. This represents those relations as follows.</p> <ol> <li>USES is represented by two indices, each a map from one column value to the set of related other column values. The two indices are in <code>bindingNameToGroupResources</code> and <code>grToTransformData/bindingsThatCare</code>.</li> <li>INSTRUCTIONS is represented by <code>grToTransformData/removes</code>.</li> <li>SPECS is represented by <code>ctNameToSpec</code> and an index, <code>grToTransformData/ctNames</code>.</li> </ol> <p>The cache interface has the following methods.</p> <ul> <li> <p><code>getCustomTransformChanges</code> ensures that the cache has an entry for a given usage (a (<code>Binding</code>, <code>GroupResource</code>) pair) and returns the corresponding instructions (i.e., set of JSONPath to remove) for that <code>GroupResource</code>. This method sets the status of each <code>CustomTransform</code> API object that it processes.</p> <p>Of course this method maintains the cache's invariants. That means adding rows to SPECS as necessary. It also means removing a row from INSTRUCTIONS upon discovery that a <code>CustomTransform</code>'s Spec has changed its <code>GroupResource</code>. Note that the cache's invariants require this removal by this method, not relying on an eventual call to <code>NoteCustomTransform</code> (because the cache records at most the latest Spec for each <code>CustomTransform</code>, a later cache operation will not know about the previous <code>GroupResource</code>).</p> <p>Removing a row from INSTRUCTIONS also entails removing the corresponding rows from SPECS, to maintain the cache's invariants.</p> </li> <li> <p><code>noteCustomTransform</code> reacts to a create/update/delete of a <code>CustomTransform</code> object. In the update case, if the <code>CustomResourceSpec</code> changed its <code>GroupResource</code> then this method removes two rows from INSTRUCTIONS (if they were present): the one for the old <code>GroupResource</code> and the one for the new. In case of create, delete, or other change in Spec, this method removes the one relevant row (if present) in INSTRUCTIONS.</p> </li> <li> <p><code>setBindingGroupResources</code> reacts to knowing the full set of <code>GroupResource</code> that a given <code>Binding</code> uses. This removes outdated rows from USES (updates the two indices that represent it) and removes rows from INSTRUCTIONS that are no longer allowed.</p> </li> </ul>"},{"location":"direct/architecture/#customization-properties-cache","title":"Customization properties cache","text":"<p>The transport controller maintains a cached set of customization properties for each destination, and an association between <code>Binding</code> and the set of destinations that it references. When a relevant informer delivers an event about an inventory object (either a <code>ManagedCluster</code> object or a <code>ConfigMap</code> object that adds properties for that destination) the controller enqueues a work item of type <code>recollectProperties</code>. This work item carries the name of the inventory object. Processing that work item starts by re-computing the full map of properties for that destination. If the cache has an entry for that destination and the cached properties differ from the ones freshly computed, the controller updates that cache entry and enqueues a reference to every <code>Binding</code> object that depends on the properties of that destination.</p>"},{"location":"direct/argo-to-wds1/","title":"Install ArgoCD for delivery to a WDS","text":"<p>This document tells you how to install ArgoCD in the KubeFlex hosting cluster and configure ArgoCD to deliver applications to a WDS.  The commands shown here assume that you access the KubeFlex hosting cluster via a kubeconfig context named \"kind-kubeflex\" and that you access the WDS via a kubeconfig context named \"wds1\"; adapt as appropriate to your particular circumstances.</p> <p>Install ArgoCD on kind-kubeflex:</p> <pre><code>kubectl --context kind-kubeflex create namespace argocd\nkubectl --context kind-kubeflex apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p>Install CLI:</p> <p>on MacOS:</p> <pre><code>brew install argocd\n</code></pre> <p>on Linux:</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre> <p>Check the ArgoCD releases page for the obtaining the latest  stable release for other architectures and operating systems.</p> <p>Configure Argo to work with the ingress installed in the hosting cluster:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.localtest.me\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\nEOF\n</code></pre> <p>Open a browser to ArgoCD console:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Note: if you are working on a VM via SSH, just take the IP of the VM (VM_IP) and add the line <code>&lt;VM_IP&gt; argocd.localtest.me</code> to your '/etc/hosts' file, replacing  with the actual IP of your desktop. <p>Get the password for Argo with:</p> <pre><code>kubectl config use-context kind-kubeflex\nargocd admin initial-password -n argocd\n</code></pre> <p>Login into the ArgoCD console with <code>admin</code> and the password just retrieved. Type the following on a shell terminal in your desktop (or just enter the address https://argocd.localtest.me:9443 on your browser):</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Also, login with the argocd CLI with the same credentials.</p> <pre><code>argocd login --insecure argocd.localtest.me:9443\n</code></pre> <p>Add the <code>wds1</code> space as cluster to ArgoCD:</p> <pre><code>CONTEXT=wds1\nkubectl config view --minify --context=${CONTEXT} --flatten &gt; /tmp/${CONTEXT}.kubeconfig\nkubectl config --kubeconfig=/tmp/${CONTEXT}.kubeconfig set-cluster ${CONTEXT}-cluster --server=https://${CONTEXT}.${CONTEXT}-system 2&gt;/dev/null\nkubectl config use-context kind-kubeflex\nARGO_SERVER_POD=$(kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o 'jsonpath={.items[0].metadata.name}')\nkubectl cp /tmp/${CONTEXT}.kubeconfig -n argocd ${ARGO_SERVER_POD}:/tmp\nPASSWORD=$(argocd admin initial-password -n argocd | cut -d \" \" -f 1)\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd login argocd-server.argocd --username admin --password $PASSWORD --insecure\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd cluster add ${CONTEXT} --kubeconfig /tmp/${CONTEXT}.kubeconfig -y\n</code></pre> <p>Configure Argo to label resources with the \"argocd.argoproj.io/instance\" label:</p> <pre><code>kubectl --context kind-kubeflex patch cm -n argocd argocd-cm -p '{\"data\": {\"application.instanceLabelKey\": \"argocd.argoproj.io/instance\"}}'\n</code></pre>"},{"location":"direct/binding/","title":"Binding workload with WEC","text":"<p>This document is about associating WECs with workload objects. The primary concept is sometimes called \"downsync\", which confusingly refers to both the propagation and transformation of desired state from core to WECs and the propagation and summarization of reported state from WECs to core.</p>"},{"location":"direct/binding/#binding-basics","title":"Binding Basics","text":"<p>The user controls downsync primarily through API objects of kinds <code>BindingPolicy</code> and <code>Binding</code>. These go in a WDS and associate workload objects in that WDS with WECs, along with adding some modulations on how downsync is done.</p> <p><code>BindingPolicy</code> is a higher level concept than <code>Binding</code>. KubeStellar has a controller that translates each <code>BindingPolicy</code> to a <code>Binding</code>. A user could eschew the <code>BindingPolicy</code> and directly maintain a <code>Binding</code> object or let a different controller maintain the <code>Binding</code> object (TODO: check that this is true). The <code>Binding</code> object shows which workload objects and which WECs matched the predicates in the <code>BindingPolicy</code> and so is also useful as feedback to the user about that.</p>"},{"location":"direct/binding/#bindingpolicy","title":"BindingPolicy","text":"<p>The <code>spec</code> of a <code>BindingPolicy</code> has two predicates that (1) identify a subset of the WECs in the inventory of the ITS associated with the WDS and (2) identify a subset of the workload objects in the WDS. The primary function of the <code>BindingPolicy</code> is to assert the desired association between (1) and (2). A <code>BindingPolicy</code> can also add some modulations on how those workload objects are downsynced to/from those WECs.</p> <p>The WEC-selecting predicate is an array of label selectors in <code>spec.clusterSelectors</code>. These label selectors test the labels of the inventory objects describing the WECs. The bound WECs are the ones whose inventory object passes at least one of the the label selectors in <code>spec.clusterSelectors</code>.</p> <p>The workload object selection predicate is in <code>spec.downsync</code>, which holds a list of <code>DownsyncPolicyClause</code>s; each includes both a workload object selection predicate and also two kinds of information that modulate the downsync.</p> <p>For more definitional details about a <code>BindingPolicy</code>, see the API reference.</p> <p>Following is an example of a <code>BindingPolicy</code> object, used in the end-to-end test of <code>createOnly</code> functionality.</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\nname: nginx\nspec:\nclusterSelectors:\n- matchLabels:\nlocation-group: edge\ndownsync:\n- objectSelectors:\n- matchLabels:\napp.kubernetes.io/name: nginx\nresources:\n- namespaces\n- createOnly: true\nobjectSelectors:\n- matchLabels:\napp.kubernetes.io/name: nginx\nresources:\n- deployments\n</code></pre>"},{"location":"direct/binding/#binding","title":"Binding","text":"<p>TODO: write this</p>"},{"location":"direct/combined-status/","title":"Combined Status from WECs","text":"<p>Note on terminology: the general idea that we wish to address is returning reported state about a workload object to its WDS. At the current level of development, we equate reported state with the status section of an object --- while anticipating a more general treatment in the future.</p> <p>There are two methods of returning reported state: a general one and a special case. The general method returns reported state from any number of WECs. The special case applies when the number of WECs is exactly 1, and returns the reported state into the original object in the WDS. It may make sense to expand the latter into a general feature in the future, accommodating features such as scheduling.</p>"},{"location":"direct/combined-status/#introduction-to-the-general-technique","title":"Introduction to the General Technique","text":"<p>The general technique for combining reported state from WECs is built upon the following ideas:</p> <ol> <li> <p>The way that reported state is combined is specified by the user, in a simple but powerful way modeled on SQL. This is chosen because it is a well worked out set of ideas, is widely known, and is something that we may someday want to use in our implementation. We do not need to support anything like full SQL (for any version of SQL). This proposal only involves one particular pattern of relatively simple SELECT statement, and a different expression language (CEL, which is the most prominent expression language in the Kubernetes milieu).</p> </li> <li> <p>The expressions may reference the content of the workload object as it sits in the WDS as well as the state returned from the WECs.</p> </li> <li> <p>The specification of how to combine reported state is defined in <code>StatusCollector</code> objects. These objects are referenced in the <code>BindingPolicy</code> objects right next to the criteria for selecting workload objects. This saves users the trouble of having to write selection criteria twice. With the specification being separate rather than embedded, it is possible to have a library of <code>StatusCollectors</code> that can be reused across different <code>BindingPolicy</code> objects. In the future KubeStellar would provide a library of <code>StatusCollector</code> objects that cover convenient use-cases for kubernetes built-in resources such as deployments.</p> </li> <li> <p>The <code>Binding</code> objects also hold references to <code>StatusCollector</code> objects. Each reference to a workload object is paired with references to all the <code>StatusCollectors</code> mentioned in all the <code>DownsyncObjectTestAndStatusCollection</code> structs that matched the workload object.</p> </li> <li> <p>The combined reported state appears in a new kind of object, one per (workload object, <code>Binding</code> object) pair.</p> </li> <li> <p>A user can request a list without aggregation, possibly after filtering, but certainly with a limit on list length. The expectation is that such a list makes sense only if the length of the list will be modest. For users that want access to the full reported state from each WEC for a large number of WECs, KubeStellar should have an abstraction that gives the users access --- in a functional way, not by making another copy --- to that state (which is already in the mailbox namespaces).</p> </li> <li> <p>The reported state for a given workload object from a given WEC is implicitly augmented with metadata about the WEC and about the end-to-end propagation from WDS to that WEC. This extra information is available just like the regular contents of the object, for use in combining reported state.</p> </li> <li>The specifics of queryable objects and implicit augmentations can be found in types.go and is specified in Queryable Objects.</li> </ol>"},{"location":"direct/combined-status/#relation-with-sql","title":"Relation with SQL","text":""},{"location":"direct/combined-status/#overview-of-relation-with-sql","title":"Overview of Relation with SQL","text":"<p>To a given workload object, and in the context of a given <code>Binding</code> object, the user has bound some <code>StatusCollector</code> objects. The meaning of a <code>StatusCollector</code> in the context of a (workload object, <code>Binding</code> object) pair is analogous to an SQL SELECT statement that does the following things.</p> <ol> <li> <p>The SELECT statement has one input table, which has a row per WEC that the Binding says the workload object should go to.</p> </li> <li> <p>The SELECT statement can have a WHERE clause that filters out some of the rows.</p> </li> <li> <p>The SELECT statement either does aggregation or does not. In the case of not doing aggregation, the SELECT statement simply has a collection of named expressions defining the columns of its output.</p> </li> <li> <p>In the case of aggregation, the SELECT statement has the following.</p> <ul> <li> <p>An optional <code>GROUP BY</code> clause saying how the rows (WECs) are   grouped to form the inputs for aggregation, in terms of named   expressions. For convenience here, each of these named   expressions is implicitly included in the output columns.</p> </li> <li> <p>A collection of named expressions using aggregation functions to define   additional output columns.</p> </li> </ul> </li> <li> <p>The SELECT statement has a LIMIT on the number of rows that it will yield.</p> </li> </ol>"},{"location":"direct/combined-status/#detailed-relation-with-sql","title":"Detailed Relation with SQL","text":"<p>For a given workload object, <code>Binding</code>, and <code>StatusCollector</code>: start with a table named <code>PerWEC</code>. This table has one primary key column and it holds the name of the WEC that the reported state is from. The dependent columns hold the workload object content from the WDS, the workload object content returned from the WEC, and the augmentations.</p> <p>There are three forms of <code>StatusCollector</code>, equivalent to three forms of SQL statement.</p>"},{"location":"direct/combined-status/#plain-selection","title":"Plain selection","text":"<p>When the <code>StatusCollector</code> has selection but no \"GROUOP BY\" and no aggregation, this is equivalent to the following form of SELECT statement. The List of stale WECs example below is an example of this form.</p> <pre><code>SELECT &lt;selected columns&gt;\nFROM PerWEC WHERE &lt;filter condition&gt;\nLIMIT &lt;limit&gt;\n</code></pre>"},{"location":"direct/combined-status/#aggregation-without-group-by","title":"Aggregation without GROUP BY","text":"<p>When there is aggregation but no plain selection and no \"GROUP BY\", this is equivalent to the following form of SELECT statement. The Number of WECs example below is an example of this form.</p> <pre><code>SELECT &lt;aggregation columns&gt;\nFROM PerWEC WHERE &lt;filter condition&gt;\nLIMIT &lt;limit&gt;\n</code></pre>"},{"location":"direct/combined-status/#aggregation-with-group-by","title":"Aggregation with GROUP BY","text":"<p>When there is \"GROUP BY\" and aggregation but no plain selection, this is equivalent to the following form of SELECT statement. The Histogram of Pod phase example below is an example of this form.</p> <pre><code>SELECT &lt;group-by column names&gt;, &lt;aggregation columns&gt;\nFROM (SELECT &lt;group-by column 1 expr&gt; AS &lt;group-by column 1 name&gt;,\n...\n&lt;group-by column N expr&gt; AS &lt;group-by column N name&gt;,\n*\nFROM PerWEC WHERE &lt;filter condition&gt;)\nGROUP BY &lt;group-by column names&gt;\nLIMIT &lt;limit&gt;\n</code></pre> <p>When there are N \"GROUP BY\" columns, the result has a row for each tuple of values (v1, v2, ... vN) such that there exists a WEC for which (v1, v2, ... vN) are the values of the \"GROUP BY\" columns. The result has no more rows than that.</p>"},{"location":"direct/combined-status/#specification-of-the-general-technique","title":"Specification of the general technique","text":"<p>See <code>types.go</code>.</p>"},{"location":"direct/combined-status/#queryable-objects","title":"Queryable Objects","text":"<p>A CEL expression within a <code>StatusCollector</code> can reference the following objects:</p> <ol> <li><code>inventory</code>: The inventory object for the workload object:</li> <li> <p><code>inventory.name</code>: The name of the inventory object.</p> </li> <li> <p><code>obj</code>: The workload object from the WDS:</p> </li> <li> <p>All fields of the workload object except the status subresource.</p> </li> <li> <p><code>returned</code>: The reported state from the WEC:</p> </li> <li> <p><code>returned.status</code>: The status section of the object returned from the WEC.</p> </li> <li> <p><code>propagation</code>: Metadata about the end-to-end propagation process:</p> </li> <li><code>propagation.lastReturnedUpdateTimestamp</code>: metav1.Time of last update to any returned state.</li> </ol>"},{"location":"direct/combined-status/#examples-of-using-the-general-technique","title":"Examples of using the general technique","text":""},{"location":"direct/combined-status/#number-of-wecs","title":"Number of WECs","text":"<p>The <code>StatusCollectorSpec</code> would look like the following.</p> <pre><code>combinedFields:\n- name: count\ntype: COUNT\n</code></pre> <p>The analogous SQL statement would look something like the following.</p> <pre><code>SELECT COUNT(*) AS count FROM PerWEC LIMIT &lt;something&gt;\n</code></pre> <p>The table resulting from this would have one column and one row. The one value in this table would be the number of WECs.</p>"},{"location":"direct/combined-status/#histogram-of-pod-phase","title":"Histogram of Pod phase","text":"<p>The <code>StatusCollectorSpec</code> would look like the following.</p> <pre><code>groupBy:\n- name: phase\ndef: returned.status.phase\ncombinedFields:\n- name: count\ntype: COUNT\n</code></pre> <p>The analogous SQL statement would look something like the following.</p> <pre><code>SELECT phase, COUNT(*) AS count\nFROM (SELECT &lt;SQL expression for returned.status.phase&gt; AS phase, *\nFROM PerWEC)\nGROUP BY phase\nLIMIT &lt;something&gt;\n</code></pre> <p>The result would have two columns, holding a phase value and a count. The number of rows equals the number of different values of <code>returned.status.phase</code> that appear among the WECs. For each row (P, N): P is a phase value that appears in at least one WEC, and N is the number of WECs where the phase value is P.</p>"},{"location":"direct/combined-status/#histogram-of-number-of-available-replicas-of-a-deployment","title":"Histogram of number of available replicas of a Deployment","text":"<p>This reports, for each number of available replicas, how many WECs have that number.</p> <pre><code>groupBy:\n- name: numAvailable\ndef: returned.status.availableReplicas\ncombinedFields:\n- name: count\ntype: COUNT\n</code></pre>"},{"location":"direct/combined-status/#list-of-wecs-where-the-deployment-is-not-as-available-as-desired","title":"List of WECs where the Deployment is not as available as desired","text":"<pre><code>filter: \"obj.spec.replicas != returned.status.availableReplicas\"\nselect:\n- name: wec\ndef: inventory.name\n</code></pre>"},{"location":"direct/combined-status/#full-status-from-each-wec-with-information-retrieval-time","title":"Full status from each WEC with information retrieval time","text":"<p>This produces a listing of object status paired with inventory object name.</p> <pre><code>select:\n- name: wec\ndef: inventory.name\n- name: status\ndef: returned.status\n- name: retrievalTime\ndef: propagation.lastReturnedUpdateTimestamp\n</code></pre>"},{"location":"direct/combined-status/#special-case-for-1-wec","title":"Special case for 1 WEC","text":"<p>This proposal refines the meaning of <code>BindingPolicySpec.WantSingletonReportedState</code> to request the special case when it applies.</p>"},{"location":"direct/contribute/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through familiarize yourself with our code of conduct.</p>"},{"location":"direct/contribute/#talk-to-us-on-slack","title":"Talk to us on Slack!","text":"<p>If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"direct/contribute/#work-with-us-on-github","title":"Work with us on GitHub!","text":"<p>Full documentation on how to contribute to the development of KubeStellar is available as part of our GitHub repository. See it there.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"direct/contributor/","title":"Notes for KubeStellar Contributors","text":"<p>These notes are incomplete; as mentioned in the introduction, if you are interested in contributing directly to the development of KubeStellar, we recommend joining in via the KubeStellar github repository and KubeStellar Slack</p> <p>Make sure all pre-requisites are installed as described in pre-reqs.</p>"},{"location":"direct/contributor/#unit-testing","title":"Unit testing","text":"<p>The Makefile has a target for running all the unit tests.</p> <pre><code>make test\n</code></pre>"},{"location":"direct/contributor/#integration-testing","title":"Integration testing","text":"<p>There are currently two integration tests. Contributors can run them. There is also a GitHub Actions workflow (in <code>.github/workflows/pr-test-integration.yml</code>) that runs these tests.</p> <p>These tests require you to already have <code>etcd</code> on your <code>$PATH</code>. See https://github.com/kubernetes/kubernetes/blob/v1.28.2/hack/install-etcd.sh for an example of how to do that.</p> <p>To run the tests sequentially, issue a command like the following.</p> <pre><code>CONTROLLER_TEST_NUM_OBJECTS=24 go test -v ./test/integration/controller-manager &amp;&gt; /tmp/test.log\n</code></pre> <p>If <code>CONTROLLER_TEST_NUM_OBJECTS</code> is not set then the number of objects will be 18. This parameterization by an environment variable is only a point-in-time hack, it is expected to go away once we have a test that runs reliably on a large number of objects.</p> <p>To run one of the individual tests, issue a command like the following example.</p> <pre><code>go test -v -timeout 60s -run ^TestCRDHandling$ ./test/integration/controller-manager\n</code></pre>"},{"location":"direct/contributor/#making-releases","title":"Making releases","text":"<p>See the release process document.</p>"},{"location":"direct/control/","title":"Controlling KubeStellar","text":"<p>This is the parent document for docs about particular kinds of control.</p> <ul> <li>Binding between workload objects and WECs</li> <li>Transforming workload objects on their way to WECs</li> <li>Combining returned status</li> </ul> <p>TODO: write this for real.</p>"},{"location":"direct/core-chart/","title":"KubeStellar Core chart usage","text":"<p>This documents explains how to use KubeStellar Core chart to do three of the 11 installation and usage steps; please see the full outline for generalities and Getting Started for an example of usage.</p> <p>This Helm chart can do any subset of the following things.</p> <ul> <li>Initialize a pre-existing cluster to serve as the KubeFlex hosting cluster.</li> <li>Create some ITSes.</li> <li>Create some WDSes.</li> </ul> <p>The information provided is specific for the following release:</p> <pre><code>export KUBESTELLAR_VERSION=0.24.0-alpha.2\n</code></pre>"},{"location":"direct/core-chart/#pre-requisites","title":"Pre-requisites","text":"<p>To install the Helm chart the only requirement is Helm. However, additional executables may be required to create/manage the cluster(s) (e.g., Kind and kubectl), to join Workload Execution Clusters (WECs) (e.g., clusteradm), and to interact with Control Planes (e.g., kubectl), etc. For such purpose, a full list of executable that may be required can be found here.</p> <p>The setup of KubeStellar via the Core chart requires the existence of a KubeFlex hosting cluster.</p> <p>This can be:</p> <ol> <li> <p>A local Kind or k3s cluster with an ingress with SSL passthrough and a mapping to host port 9443</p> <p>This option is particularly useful for first time users or users that would like to have a local deployment.</p> <p>It is important to note that, when the hosting cluster was created by kind or k3s and its Ingress domain name is left to default to localtest.me, then the name of the container running hosting cluster must be also be referenced during the Helm chart installation by setting <code>--set \"kubeflex-operator.hostContainer=&lt;control-plane-container-name&gt;\"</code>. The <code>&lt;control-plane-container-name&gt;</code> is the name of the container in which kind or k3d is running the relevant control plane. One may use <code>docker ps</code> to find the <code>&lt;control-plane-container-name&gt;</code>.</p> <p>If a host port number different from the expected 9443 is used for the Kind cluster, then the same port number must be specified during the chart installation by adding the following argument <code>--set \"kubeflex-operator.externalPort=&lt;port&gt;\"</code>.</p> <p>By default the KubeStelalr Core chart uses a test domain <code>localtest.me</code>, which is ok for testing on a single host machine. However, scenarios that span more than one machine, it is necessary to set <code>--set \"kubeflex-operator.domain=&lt;domain&gt;\"</code> to a more appropriate <code>&lt;domain&gt;</code> that can be reached from Workload Execution CLusters (WECs).</p> <p>For convenience, a new local Kind cluster that satisfies the requirements for KubeStellar setup and that can be used to exercises the examples can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-kind-cluster-with-SSL-passthrough.sh) --name kubeflex --port 9443\n</code></pre> <p>Alternatively, a new local k3s cluster that satisfies the requirements for KubeStellar setup and that can be used to exercises the examples can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-k3s-cluster-with-SSL-passthrough.sh) --port 9443\n</code></pre> </li> <li> <p>An OpenShift cluster</p> <p>When using this option, one is required to explicitely set the <code>isOpenShift</code> variable to <code>true</code> by including <code>--set \"kubeflex-operator.isOpenShift=true\"</code> in the Helm chart installation command.</p> </li> </ol>"},{"location":"direct/core-chart/#kubestellar-core-chart-values","title":"KubeStellar Core Chart values","text":"<p>The KubeStellar chart makes available to the user several values that may be used to customize its installation into an existing cluster:</p> <pre><code># Control controller log verbosity\n# The \"default\" verbosity value will be used for all controllers unless a specific controller verbosity override is specified\nverbosity:\ndefault: 2\n# Specific controller verbosity overrides:\n# kubestellar: 6 (controller-manager)\n# clusteradm: 6\n# transport: 6\n# KubeFlex override values\nkubeflex-operator:\ninstall: true # enable/disable the installation of KubeFlex by the chart (default: true)\ninstallPostgreSQL: true # enable/disable the installation of the appropriate version of PostgreSQL required by KubeFlex (default: true)\nisOpenShift: false # set this variable to true when installing the chart in an OpenShift cluster (default: false)\n# Kind cluster specific settings:\ndomain: localtest.me # used to define the DNS domain name used from outside the KubeFlex hosting cluster to reach that cluster's Ingress endpoint (default: localtest.me)\nexternalPort: 9443 # used to set the port to access the Control Planes API (default: 9443)\nhostContainer: kubeflex-control-plane # used to set the name of the container that runs the KubeFlex hosting cluster (default: kubeflex-control-plane, which corresponds to a Kind cluster with name kubeflex)\n# Determine if the Post Create Hooks should be installed by the chart\nInstallPCHs: true\n# List the Inventory and Transport Spaces (ITSes) to be created by the chart\n# Each ITS consists of a mandatory unique name and an optional type, which could be either host or vcluster (default to vcluster, if not specified)\nITSes: # ==&gt; installs ocm + ocm-status-addon\n# List the Workload Description Spaces (WDSes) to be created by the chart\n# Each WDS consists of a mandatory unique name and several optional parameters:\n# - type: host or k8s (default to k8s, if not specified)\n# - APIGroups: a comma separated list of APIGroups\n# - ITSName: the name of the ITS control plane to be used by the WDS. Note that the ITSName MUST be specified if more than one ITS exists.\nWDSes: # ==&gt; installs kubestellar + ocm-transport-plugin\n</code></pre> <p>The first section of the <code>values.yaml</code> file refers to parameters that are specific to the KubeFlex installation, see here for more information.</p> <p>In particular: - <code>kubeflex-operator.install</code> accepts a boolean value to enable/disable the installation of KubeFlex into the cluster by the chart - <code>kubeflex-operator.isOpenShift</code> must be set to true by the user when installing the chart into a OpenShift cluster</p> <p>By default, the chart will install the KubeFlex and its PostgreSQL dependency.</p> <p>The second section allows a user of the chart to determine if Post Create Hooks (PCHes) needed for creating ITSes and WDSes control planes should be installed by the chart. By default <code>InstallPCHs</code> is set to <code>true</code> to enable the installation of the PCHes, however one may want to set this value to <code>false</code> when installing multiple copies of the chart to avoid conflicts. A single copy of the PCHes is required and allowed per cluster.</p> <p>The third section of the <code>values.yaml</code> file allows one to create a list of Inventory and Transport Spaces (ITSes). By default, this list is empty and no ITS will be created by the chart. A list of ITSes can be specified using the following format:</p> <pre><code>ITSes: # all the CPs in this list will execute the its.yaml PCH\n- name: &lt;its1&gt;          # mandatory name of the control plane\ntype: &lt;vcluster|host&gt; # optional type of control plane host or vcluster (default to vcluster, if not specified)\n- name: &lt;its2&gt;          # mandatory name of the control plane\ntype: &lt;vcluster|host&gt; # optional type of control plane host or vcluster (default to vcluster, if not specified)\n...\n</code></pre> <p>where <code>name</code> must specify a name unique among all the control planes in that KubeFlex deployment and the optional <code>type</code> can be either vcluster (default) or host, see here for more information.</p> <p>The fourth section of the <code>values.yaml</code> file allows one to create a list of Workload Description Spaces (WDSes). By default, this list is empty and no WDS will be created by the chart. A list of WDSes can be specified using the following format:</p> <pre><code>WDSes: # all the CPs in this list will execute the wds.yaml PCH\n- name: &lt;wds1&gt;     # mandatory name of the control plane\ntype: &lt;host|k8s&gt; # optional type of control plane host or k8s (default to k8s, if not specified)\nAPIGroups: \"\"    # optional string holding a comma-separated list of APIGroups\nITSName: &lt;its1&gt;  # optional name of the ITS control plane, this MUST be specified if more than one ITS exists at the moment the WDS PCH starts\n- name: &lt;wds2&gt;     # mandatory name of the control plane\ntype: &lt;host|k8s&gt; # optional type of control plane host or k8s (default to k8s, if not specified)\nAPIGroups: \"\"    # optional string holding a comma-separated list of APIGroups\nITSName: &lt;its2&gt;  # optional name of the ITS control plane, this MUST be specified if more than one ITS exists at the moment the WDS PCH starts\n...\n</code></pre> <p>where <code>name</code> must specify a name unique among all the control planes in that KubeFlex deployment (note that this must be unique among both ITSes and WDSes), the optional <code>type</code> can be either k8s (default) or host, see here for more information, the optional <code>APIGroups</code> provides a list of APIGroups, see here for more information, and <code>ITSName</code> specify the ITS connected to the new WDS being created (this parameter MUST be specified if more that one ITS exists in the cluster, if no value is specified and only one ITS exists in the cluster, then it will be automatically selected).</p>"},{"location":"direct/core-chart/#kubestellar-core-chart-usage_1","title":"KubeStellar Core Chart usage","text":"<p>A specific version of the KubeStellar core chart can be simply installed in an existing cluster using the following command:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION\n</code></pre> <p>The above command will install KubeFlex and the Post Create Hooks, but no Control Planes. Please remember to add <code>--set \"kubeflex-operator.isOpenShift=true\"</code>, when installing into an OpenShift cluster.</p> <p>User defined control planes can be added using additional value files of <code>--set</code> arguments, e.g.:</p> <ul> <li>add a single ITS named its1 of default vcluster type: <code>--set-json='ITSes=[{\"name\":\"its1\"}]'</code></li> <li>add two ITSes named its1 and its2 of of type vcluster and host, respectively: <code>--set-json='ITSes=[{\"name\":\"its1\"},{\"name\":\"its2\",\"type\":\"host\"}]'</code></li> <li>add a single WDS named wds1 of default k8s type connected to the one and only ITS: <code>--set-json='WDSes=[{\"name\":\"wds1\"}]'</code></li> </ul> <p>A KubeStellar Core installation that is consistent with Getting Started and and supports the example scenarios could be achieved with the following command:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n--set-json='ITSes=[{\"name\":\"its1\"}]' \\\n--set-json='WDSes=[{\"name\":\"wds1\"}]'\n</code></pre> <p>After the initial installation is completed, there are two main ways to install additional control planes (e.g., create a second <code>wds2</code> WDS):</p> <ol> <li> <p>Upgrade the initial chart. This choice requires to relist the existing control planes, which would otherwise be deleted:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n--set-json='ITSes=[{\"name\":\"its1\"}]' \\\n--set-json='WDSes=[{\"name\":\"wds1\"},{\"name\":\"wds2\"}]'\n</code></pre> </li> <li> <p>Install a new chart with a different name. This choice does not requires to relist the existing control planes, but requires to disable the reinstallation of KubeFlex and PCHes:</p> <pre><code>helm upgrade --install add-wds2 oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n--set='kubeflex-operator.install=false,InstallPCHs=false' \\\n--set-json='WDSes=[{name\":\"wds2\"}]'\n</code></pre> </li> </ol>"},{"location":"direct/core-chart/#kubeconfig-files-and-contexts-for-control-planes","title":"Kubeconfig files and contexts for Control Planes","text":"<p>Unless you are using the <code>kflex</code> CLI from release 0.6.2 or later of KubeFlex, before proceeding further you should wait for each of the new control planes whose type is not \"host\" to be \"ready\". A host type control plane is an alias for the hosting cluster and is born ready.</p> <p>The following commands will wait for every ControlPlane to be \"ready\".</p> <pre><code>echo \"Waiting for all KubeFlex Control Planes to be Ready:\"\nfor cpname in `kubectl get controlplane -o name`; do\ncpname=${cpname##*/}\nwhile [[ `kubectl get cp $cpname -o 'jsonpath={..status.conditions[?(@.type==\"Ready\")].status}'` != \"True\" ]]; do\necho \"Waiting for \\\"$cpname\\\"...\"\nsleep 5\ndone\necho \"\\\"$cpname\\\" is ready.\"\ndone\n</code></pre> <p>It is convenient to use one kubeconfig file that has a context for each of your control planes. That can be done in two ways, one using the <code>kflex</code> CLI and one not.</p> <ol> <li> <p>Using <code>kflex</code> CLI</p> <p>The following commands will add a context, named after the given control plane, to your current kubeconfig file and make that the current context. The deletion is to remove an older vintage if it is present.</p> <pre><code>kubectl config delete-context $cpname\nkflex ctx $cpname\n</code></pre> <p>The <code>kflex ctx</code> command is unable to create a new context if the current context does not access the KubeFlex hosting cluster AND the KubeFlex kubeconfig extension remembering that context's name is not set; see the KubeFlex user guide for your release of KubeFlex for more information.</p> <p>To automatically add all Control Planes as contexts of the current kubeconfig, one can use the convenience script below:</p> <pre><code>echo \"Getting the kubeconfig of all Control Planes...\"\nfor cpname in `kubectl get controlplane -o name`; do\ncpname=${cpname##*/}\necho \"Getting the kubeconfig of Control Planes \\\"$cpname\\\"...\"\nkflex ctx $cpname\ndone\n</code></pre> <p>After doing the above context switching you may wish to use <code>kflex ctx</code> to switch back to the hosting cluster context.</p> <p>Afterwards the content of a Control Plane <code>$cpname</code> can be accessed by specifying its context:</p> <pre><code>kubectl --context \"$cpname\" ...\n</code></pre> </li> <li> <p>Using plain <code>kubectl</code> commands</p> <p>The following commands can be used to create a fresh kubeconfig file for each of the KubeFlex Control Planes in the hosting cluster:</p> <pre><code>echo \"Creating a kubeconfig for each KubeFlex Control Plane:\"\nfor cpname in `kubectl get controlplane -o name`; do\ncpname=${cpname##*/}\necho \"Getting the kubeconfig of \\\"$cpname\\\" ==&gt; \\\"kubeconfig-$cpname\\\"...\"\nif [[ \"$(kubectl get controlplane $cpname -o=jsonpath='{.spec.type}')\" == \"host\" ]] ; then\nkubectl config view --minify --flatten &gt; \"kubeconfig-$cpname\"\nelse\nkubectl get secret $(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.name}') \\\n-n $(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.namespace}') \\\n-o=jsonpath=\"{.data.$(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.key}')}\" \\\n| base64 -d &gt; \"kubeconfig-$cpname\"\nfi\ncurname=$(kubectl --kubeconfig \"kubeconfig-$cpname\" config current-context)\nif [ \"$curname\" != \"$cpname\" ]\nthen kubectl --kubeconfig \"kubeconfig-$cpname\" config rename-context \"$curname\" $cpname\nfi\ndone\n</code></pre> <p>The code above puts the kubeconfig for a control plane <code>$cpname</code> into a file name <code>kubeconfig-$cpname</code> in the local folder. The current context will be renamed to <code>$cpname</code>, if it does not already have that name (which it will for control planes of type \"k8s\", for example).</p> <p>With the above kubeconfig files in place, the control plane named <code>$cpname</code> can be accessed as follows.</p> <pre><code>kubectl --kubeconfig \"kubeconfig-$cpname\" ...\n</code></pre> <p>The individual kubeconfigs can also be merged as contexts of the current <code>~/.kube/config</code> with the following commands:</p> <pre><code>echo \"Merging the Control Planes kubeconfigs into ~/.kube/config ...\"\ncp ~/.kube/config ~/.kube/config.bak\nKUBECONFIG=~/.kube/config:$(find . -maxdepth 1 -type f -name 'kubeconfig-*' | tr '\\n' ':') kubectl config view --flatten &gt; ~/.kube/kubeconfig-merged\nmv ~/.kube/kubeconfig-merged ~/.kube/config\n</code></pre> <p>Afterwards the content of a Control Plane <code>$cpname</code> can be accessed by specifying its context:</p> <pre><code>kubectl --context \"$cpname\" ...\n</code></pre> </li> <li> <p>Using <code>import-cp-contexts.sh</code> script</p> <p>The following covenience command can also be used to import all the KubeFlex Control Planes in the current hosting cluster as contexts of the current kubeconfig:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/import-cp-contexts.sh) --merge\n</code></pre> <p>The script above only requires <code>kubectl</code> and <code>yq</code>.</p> <p>The script accepts the following arguments:</p> <ul> <li><code>--kubeconfig &lt;filename&gt;</code> specify the kubeconfig of the hosting cluster where the KubeFlex Control Planes are located. Note that this argument will override the content of the <code>KUBECONFIG</code> environment variable</li> <li><code>--context &lt;name&gt;</code> specify a context of the current kubeconfig where to look for KubeFlex Control Planes. If this argument is not specified, then all contexts will be searched.</li> <li><code>--names|-n &lt;name1&gt;,&lt;name2&gt;,..</code> comma separated list of KubeFlex Control Planes names to import. If this argument is not specified then all available KubeFlex Control Planes will be imported.</li> <li><code>--replace-localhost|-r &lt;host&gt;</code> replaces server addresses \"127.0.0.1\" with a desired <code>&lt;host&gt;</code>. This parameter is useful for making KubeFlex Control Planes of type <code>host</code> accessible from outside the machine hosting the cluster.</li> <li><code>--merge|-m</code> merge the kubeconfig with the contexts of the control planes with the existing cluster kubeconfig. If this flag is not specified, then only the kubeconfig with the contexts of the KubeFlex Control Planes will be produced.</li> <li><code>--output|-o &lt;filename&gt;|-</code> specify a kubeconfig file to save the kubeconfig to. Use <code>-</code> for stdout. If this argument is not provided, then the kubeconfig will be saved to the input specified kubeconfig, if provided, or to <code>~/.kube/config</code>.</li> <li><code>--silent|-s</code> quiet mode, do not print informarmation. This may be useful when using <code>-o -</code>.</li> <li><code>-X</code> enable verbose execution of the script for debugging</li> </ul> </li> </ol>"},{"location":"direct/core-chart/#uninstalling-the-kubestellar-core-chart","title":"Uninstalling the KubeStellar Core chart","text":"<p>The chart can be uninstalled using the command:</p> <pre><code>helm uninstall ks-core\n</code></pre> <p>This will remove KubeFlex, PostgreSQL, Post Create Hooks (PCHes), and all KubeFlex Control Planes (i.e., ITSes and WDSes) that were created by the chart.</p> <p>Additionally, if a Kind cluster was created with the provide script, it can be deleted with the command:</p> <pre><code>kind delete cluster --name kubeflex\n</code></pre> <p>Alternatively, if a k3s cluster was created with the provide script, it can be deleted with the command:</p> <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"direct/example-scenarios/","title":"KubeStellar Example Scenarios","text":"<p>This document shows some simple examples of using the release that contains this version of this document. These scenarios can be used to test a KubeStellar installation for proper functionality. These scenarios suppose that you have done \"setup\". General setup instructions are outlined in the User Guide Overview; a simple example setup is in the Setup section of Getting Started.</p>"},{"location":"direct/example-scenarios/#assumptions-and-variables","title":"Assumptions and variables","text":"<p>Each scenario supposes that one ITS and one WDS have been created, and that two WECs have been created and registered and also labeled for selection by KubeStellar control objects. These scenarios are written as shell commands (bash or zsh). These commands assume that you have defined the following shell variables to convey the needed information about that ITS and WDS and those WECs. For a concrete example of settings of these variables, see the end of Getting Started.</p> <ul> <li><code>host_context</code>: the name of the kubeconfig context to use when accessing the KubeFlex hosting cluster.</li> <li><code>its_cp</code>: the name of the KubeFlex control plane that is playing the role of ITS.</li> <li><code>its_context</code>: the name of the kubeconfig context to use when accessing the ITS.</li> <li><code>wds_cp</code>: the name of the KubeFlex control plane that is playing the role of WDS.</li> <li><code>wds_context</code>: the name of the kubeconfig context to use when accessing the WDS.</li> <li><code>wec1_name</code>, <code>wec2_name</code>: the names of the <code>ManagedCluster</code> objects in the ITS representing the two WECs.</li> <li><code>wec1_context</code>, <code>wec2_context</code>: the names of the kubeconfig contexts to use when accessing the two WECs.</li> <li><code>label_query_both</code>: a restricted <code>kubectl</code> label query over <code>ManagedCluster</code> objects in the ITS that matches both WECs. The general form of label query usable here is a comma-separated series of <code>key=value</code> requirements.</li> <li><code>label_query_one</code>: a restricted <code>kubectl</code> label query over <code>ManagedCluster</code> objects that picks out just one of the WECs.</li> </ul> <p>Each example scenario concludes with instructions on how to undo its effects.</p> <p>There are also end-to-end (E2E) tests that are based on scenario 4 and an extended variant of scenario 1. These tests normally exercise the copy of the repo containing them (rather than a release). They can alternatively test a release. See the e2e tests (in <code>test/e2e</code>). Contributors can run these tests, and CI includes checking that these E2E tests pass. Some of these tests, and the setup for all of them, are written in <code>bash</code> so that contributors can easily follow them.</p>"},{"location":"direct/example-scenarios/#scenario-0-look-around","title":"Scenario 0 - look around","text":"<p>The following command will list all the <code>ManagedCluster</code> objects that will be relevant to these scenarios.</p> <pre><code>kubectl --context \"$its_context\" get managedclusters -l \"$label_query_both\"\n</code></pre> <p>Expect to get a listing of your two <code>ManagedCluster</code> objects.</p>"},{"location":"direct/example-scenarios/#scenario-1-multi-cluster-workload-deployment-with-kubectl","title":"Scenario 1 - multi-cluster workload deployment with kubectl","text":"<p>Create a BindingPolicy to deliver an app to all clusters in the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx\"}\nEOF\n</code></pre> <p>This BindingPolicy configuration determines where to deploy the workload by using the label selector expressions found in clusterSelectors. It also specifies what to deploy through the downsync.labelSelectors expressions. Each matchLabels expression is a criterion for selecting a set of objects based on their labels. Other criteria can be added to filter objects based on their namespace, api group, resource, and name. If these criteria are not specified, all objects with the matching labels are selected. If an object has multiple labels, it is selected only if it matches all the labels in the matchLabels expression. If there are multiple objectSelectors, an object is selected if it matches any of them.</p> <p>Now deploy the app:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx\n  name: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\n  labels:\n    app.kubernetes.io/name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that manifestworks wrapping the objects have been created in the mailbox namespaces:</p> <pre><code>kubectl --context \"$its_context\" get manifestworks -n \"$wec1_name\"\nkubectl --context \"$its_context\" get manifestworks -n \"$wec2_name\"\n</code></pre> <p>Verify that the deployment has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get deployments -n nginx\nkubectl --context \"$wec2_context\" get deployments -n nginx\n</code></pre> <p>Please note, in line with Kubernetes\u2019 best practices, the order in which you apply a BindingPolicy and the objects doesn\u2019t affect the outcome. You can apply the BindingPolicy first followed by the objects, or vice versa. The result remains consistent because the binding controller identifies any changes in either the BindingPolicy or the objects, triggering the start of the reconciliation loop.</p>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-1","title":"[Optional] Teardown Scenario 1","text":"<pre><code>kubectl --context \"$wds_context\" delete ns nginx\nkubectl --context \"$wds_context\" delete bindingpolicies nginx-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-2-out-of-tree-workload","title":"Scenario 2 - Out-of-tree workload","text":"<p>This scenario is like the previous one but involves a workload whose kind of objects is not built into Kubernetes. Instead, the workload object kind is defined by a <code>CustomResourceDefinition</code> object. While KubeStellar can handle the case where the CRD is part of the workload, this example concerns the case where the CRD is established in the WECs by some other means.</p> <p>In order to run this scenario using the post-create-hook method you need the raise the permissions for the kubeflex controller manager (TODO 1: move this material and its undo to the doc on WDS; TODO 2: why is this needed? Is it needed for the core chart too? can we remove the need for this?):</p> <pre><code>kubectl --context \"$host_context\" apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubeflex-manager-cluster-admin-rolebinding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubeflex-controller-manager\n  namespace: kubeflex-system\nEOF\n</code></pre> <p>For this example, we use the <code>AppWrapper</code> custom resource defined in the multi cluster app dispatcher project.</p> <p>Install the AppWrapper CRD in the WDS and the WECs.</p> <pre><code>clusters=(\"$wds_context\" \"$wec1_context\" \"$wec2_context\");\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/config/crd/bases/workload.codeflare.dev_appwrappers.yaml\ndone\n</code></pre> <p>If desired, you may remove the <code>kubeflex-manager-cluster-admin-rolebinding</code> after the kubestellar-controller-manager is started, with the command <code>kubectl --context \"$host_context\" delete clusterrolebinding kubeflex-manager-cluster-admin-rolebinding</code></p> <p>Run the following command to give permission for the klusterlet to operate on the appwrapper cluster resource.</p> <pre><code>clusters=(\"$wec1_context\" \"$wec2_context\");\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: appwrappers-access\nrules:\n- apiGroups: [\"workload.codeflare.dev\"]\n  resources: [\"appwrappers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: klusterlet-appwrappers-access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: appwrappers-access\nsubjects:\n- kind: ServiceAccount\n  name: klusterlet-work-sa\n  namespace: open-cluster-management-agent\nEOF\ndone\n</code></pre> <p>This step will be eventually automated, see this issue for more details.</p> <p>Next, apply an appwrapper object to the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f  https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/test/yaml/0008-aw-default.yaml\n</code></pre> <p>Label the appwrapper to match the binding policy:</p> <pre><code>kubectl --context \"$wds_context\" label appwrappers.workload.codeflare.dev defaultaw-schd-spec-with-timeout-1 app.kubernetes.io/part-of=my-appwrapper-app\n</code></pre> <p>Finally, apply the BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: aw-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/part-of\":\"my-appwrapper-app\"}\nEOF\n</code></pre> <p>Check that the app wrapper has been delivered to both clusters:</p> <pre><code>kubectl --context \"$wec1_context\" get appwrappers\nkubectl --context \"$wec2_context\" get appwrappers\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-2","title":"[Optional] Teardown Scenario 2","text":"<pre><code>kubectl --context \"$wds_context\" delete bindingpolicies aw-bpolicy\nkubectl --context \"$wds_context\" delete appwrappers --all\n</code></pre> <p>Wait until the following commands show no appwrappers in the two WECs.</p> <pre><code>kubectl --context \"$wec1_context\" get appwrappers -A\nkubectl --context \"$wec2_context\" get appwrappers -A\n</code></pre> <p>Then continue.</p> <pre><code>for cluster in \"$wec1_context\" \"$wec2_context\"; do\nkubectl --context $cluster delete clusterroles appwrappers-access\n  kubectl --context $cluster delete clusterrolebindings klusterlet-appwrappers-access\ndone\n</code></pre> <p>If you have not already done so, then do the following command.</p> <pre><code>kubectl --context \"$host_context\" delete clusterrolebinding kubeflex-manager-cluster-admin-rolebinding\n</code></pre> <p>Delete the CRD from the WDS and the WECs.</p> <pre><code>clusters=(\"$wds_context\" \"$wec1_context\" \"$wec2_context\");\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} delete -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/config/crd/bases/workload.codeflare.dev_appwrappers.yaml\ndone\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-3-multi-cluster-workload-deployment-with-helm","title":"Scenario 3 - multi-cluster workload deployment with helm","text":"<p>Create a BindingPolicy for the helm chart app:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: postgres-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\n      \"app.kubernetes.io/managed-by\": Helm,\n      \"app.kubernetes.io/instance\": postgres}\nEOF\n</code></pre> <p>Note that helm sets <code>app.kubernetes.io/instance</code> to the name of the installed release.</p> <p>Create and label the namespace and install the chart:</p> <pre><code>kubectl --context \"$wds_context\" create ns postgres-system\nkubectl --context \"$wds_context\" label ns postgres-system app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\nhelm --kube-context \"$wds_context\" install -n postgres-system postgres oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre> <p>Verify that <code>StatefulSet</code> has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get statefulsets -n postgres-system\nkubectl --context \"$wec2_context\" get statefulsets -n postgres-system\n</code></pre>"},{"location":"direct/example-scenarios/#optional-propagate-helm-metadata-secret-to-managed-clusters","title":"[Optional] Propagate helm metadata Secret to managed clusters","text":"<p>Run \"helm list\" on the WDS:</p> <pre><code>$ helm --kube-context \"$wds_context\" list -n postgres-system\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS       CHART                    APP VERSION\npostgres        postgres-system 1               2023-10-31 13:39:52.550071 -0400 EDT    deployed     postgresql-13.2.0        16.0.0\n</code></pre> <p>And try that on the managed clusters</p> <pre><code>$ helm list --kube-context \"$wec1_context\" -n postgres-system\n: returns empty\n$ helm list --kube-context \"$wec2_context\" -n postgres-system\n: returns empty\n</code></pre> <p>This is because Helm creates a <code>Secret</code> object to hold its metadata about a \"release\" (chart instance) but Helm does not apply the usual labels to that object, so it is not selected by the <code>BindingPolicy</code> above and thus does not get delivered. The workload is functioning in the WECs, but <code>helm list</code> does not recognize its handiwork there. That labeling could be done for example with:</p> <pre><code>kubectl --context \"$wds_context\" label secret -n postgres-system $(kubectl --context \"$wds_context\" get secrets -n postgres-system -l name=postgres -l owner=helm  -o jsonpath='{.items[0].metadata.name}') app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\n</code></pre> <p>Verify that the chart shows up on the managed clusters:</p> <pre><code>helm list --kube-context \"$wec1_context\" -n postgres-system\nhelm list --kube-context \"$wec2_context\" -n postgres-system\n</code></pre> <p>Implementing this in a controller for automated propagation of helm metadata is tracked in this issue.</p>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-3","title":"[Optional] Teardown Scenario 3","text":"<pre><code>helm --kube-context \"$wds_context\" uninstall -n postgres-system postgres\nkubectl --context \"$wds_context\" delete ns postgres-system\nkubectl --context \"$wds_context\" delete bindingpolicies postgres-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-4-singleton-status","title":"Scenario 4 - Singleton status","text":"<p>This scenario shows how to get the full status updated when setting <code>wantSingletonReportedState</code> in the BindingPolicy. This still an experimental feature.</p> <p>Apply a BindingPolicy with the <code>wantSingletonReportedState</code> flag set:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-singleton-bpolicy\nspec:\n  wantSingletonReportedState: true\n  clusterSelectors:\n  - matchLabels: {\"name\":\"cluster1\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-singleton\"}\nEOF\n</code></pre> <p>Apply a new deployment for the singleton BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-singleton-deployment\n  labels:\n    app.kubernetes.io/name: nginx-singleton\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that the status is available in the WDS for the deployment by running the command:</p> <pre><code>kubectl --context \"$wds_context\" get deployments nginx-singleton-deployment -o yaml\n</code></pre> <p>Finally, scale the deployment from 1 to 2 replicas in the WDS:</p> <pre><code>kubectl --context \"$wds_context\" scale deployment nginx-singleton-deployment --replicas=2\n</code></pre> <p>and verify that replicas has been updated in the WEC and the WDS:</p> <pre><code>kubectl --context \"$wec1_context\" get deployment nginx-singleton-deployment\nkubectl --context \"$wds_context\" get deployment nginx-singleton-deployment\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-4","title":"[Optional] Teardown Scenario 4","text":"<pre><code>kubectl --context \"$wds_context\" delete bindingpolicies nginx-singleton-bpolicy\nkubectl --context \"$wds_context\" delete deployments nginx-singleton-deployment\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-5-resiliency-testing","title":"Scenario 5 - Resiliency testing","text":"<p>This is a test that you can do after finishing Scenario 1.</p> <p>TODO: rewrite this so that it makes sense after Scenario 4.</p> <p>Bring down the control plane: stop and restart the ITS and WDS API servers, KubeFlex and KubeStellar controllers:</p> <p>First stop all:</p> <pre><code>kubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kube-apiserver --replicas=0\nkubectl --context \"$host_context\" scale statefulset -n \"$its_cp\"-system vcluster --replicas=0\nkubectl --context \"$host_context\" scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=0\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kubestellar-controller-manager --replicas=0\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system transport-controller --replicas=0\n</code></pre> <p>Then restart all:</p> <pre><code>kubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kube-apiserver --replicas=1\nkubectl --context \"$host_context\" scale statefulset -n \"$its_cp\"-system vcluster --replicas=1\nkubectl --context \"$host_context\" scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=1\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kubestellar-controller-manager --replicas=1\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system transport-controller --replicas=1\n</code></pre> <p>Wait for about a minute for all pods to restart, then apply a new BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-res-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-res\"}\nEOF\n</code></pre> <p>and a new workload:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx-res\n  name: nginx-res\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-res-deployment\n  namespace: nginx-res\n  labels:\n    app.kubernetes.io/name: nginx-res\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-res\n  template:\n    metadata:\n      labels:\n        app: nginx-res\n    spec:\n      containers:\n      - name: nginx-res\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that deployment has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get deployments -n nginx-res\nkubectl --context \"$wec2_context\" get deployments -n nginx-res\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-5","title":"[Optional] Teardown Scenario 5","text":"<pre><code>kubectl --context \"$wds_context\" delete ns nginx-res\nkubectl --context \"$wds_context\" delete bindingpolicies nginx-res-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-6-multi-cluster-workload-deployment-of-app-with-serviceaccount-with-argocd","title":"Scenario 6 - multi-cluster workload deployment of app with ServiceAccount with ArgoCD","text":"<p>Before running this scenario, install ArgoCD on the hosting cluster and configure it work with the WDS as outlined here.</p> <p>Including a ServiceAccount tests whether there will be a controller fight over a token Secret for that ServiceAccount, which was observed in some situations with older code.</p> <p>Apply the following BindingPolicy to the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: argocd-sa-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"argocd.argoproj.io/instance\":\"nginx-sa\"}\nEOF\n</code></pre> <p>Switch context to hosting cluster and argocd namespace (this is required by argo to create an app with the CLI)</p> <pre><code>kubectl config use-context \"$host_context\"\nkubectl config set-context --current --namespace=argocd\n</code></pre> <p>Create a new application in ArgoCD:</p> <pre><code>argocd app create nginx-sa --repo https://github.com/pdettori/sample-apps.git --path nginx --dest-server https://\"${wds_cp}.${wds_cp}-system\" --dest-namespace nginx-sa\n</code></pre> <p>Open browser to Argo UI:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Open the app <code>nginx-sa</code> and sync it by clicking the \"sync\" button and then \"synchronize\".</p> <p>Alternatively, use the CLI to sync the app:</p> <pre><code>argocd app sync nginx-sa\n</code></pre> <p>Finally, check if the app has been deployed to the two clusters.</p> <pre><code>kubectl --context \"$wec1_context\" -n nginx-sa get deployments,sa,secrets\nkubectl --context \"$wec2_context\" -n nginx-sa get deployments,sa,secrets\n</code></pre> <p>Repeat multiple syncing on Argo and verify that extra secrets for the service account are not created in the WDS and both clusters:</p> <pre><code>kubectl --context \"$wds_context\" -n nginx-sa get secrets\nkubectl --context \"$wec1_context\" -n nginx-sa get secrets\nkubectl --context \"$wec2_context\" -n nginx-sa get secrets\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-6","title":"[Optional] Teardown Scenario 6","text":"<p>(Assuming that kubectl is still using the context for the hosting cluster and namespace <code>argocd</code>.)</p> <pre><code>argocd app delete nginx-sa --cascade\nkubectl --context \"$wds_context\" delete bindingpolicies argocd-sa-bpolicy\n</code></pre>"},{"location":"direct/example-wecs/","title":"Example wecs","text":"<p>The following steps show how to create two new <code>kind</code> clusters and register them with the hub as described in the official open cluster management docs.</p> <p>Note that <code>kind</code> does not support three or more concurrent clusters unless you raise some limits as described in this <code>kind</code> \"known issue\": Pod errors due to \u201ctoo many open files\u201d.</p> <ol> <li> <p>Execute the following commands to create two kind clusters, named <code>cluster1</code> and <code>cluster2</code>, and register them with the OCM hub. These clusters will serve as workload clusters. If you have previously executed these commands, you might already have contexts named <code>cluster1</code> and <code>cluster2</code>. If so, you can remove these contexts using the commands <code>kubectl config delete-context cluster1</code> and <code>kubectl config delete-context cluster2</code>.</p> <pre><code>: set flags to \"\" if you have installed KubeStellar on an OpenShift cluster\nflags=\"--force-internal-endpoint-lookup\"\nclusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkind create cluster --name ${cluster}\nkubectl config rename-context kind-${cluster} ${cluster}\nclusteradm --context its1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${cluster}/\" | awk '{print $0 \" --context '${cluster}' --singleton '${flags}'\"}' | sh\ndone\n</code></pre> <p>The <code>clusteradm</code> command grabs a token from the hub (<code>its1</code> context), and constructs the command to apply the new cluster to be registered as a managed cluster on the OCM hub.</p> </li> <li> <p>Repeatedly issue the command:</p> <pre><code>kubectl --context its1 get csr\n</code></pre> <p>until you see that the certificate signing requests (CSR) for both cluster1 and cluster2 exist. Note that the CSRs condition is supposed to be <code>Pending</code> until you approve them in step 4.</p> </li> <li> <p>Once the CSRs are created, approve the CSRs complete the cluster registration with the command:</p> <pre><code>clusteradm --context its1 accept --clusters cluster1\nclusteradm --context its1 accept --clusters cluster2\n</code></pre> </li> <li> <p>Check the new clusters are in the OCM inventory and label them:</p> <pre><code>kubectl --context its1 get managedclusters\nkubectl --context its1 label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context its1 label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre> </li> </ol>"},{"location":"direct/galaxy-intro/","title":"galaxy","text":"<p>The KubeStellar galaxy is a secondary repository of as-is KubeStellar-related tools and packages that are not part of the regular KubeStellar releases. These integrations are beyond the scope of the core kubestellar repo, so are located here in a separate repository. It's name is galaxy in line with our space theme and to indicate a broader constellation of projects for establishing integrations/collaborations.</p> <p>It includes additional modules, tools and documentation to facilitate KubeStellar integration with other community projects, as well as some more experimental code we may be tinkering with for possible inclusion at some point.</p> <p>Right now, galaxy includes some bash-based utility, and scripts to replicate demos and PoCs such as KFP + KubeStellar integration and Argo Workflows + KubeStellar integration.</p>"},{"location":"direct/galaxy-intro/#utility-scripts","title":"Utility Scripts","text":"<ul> <li> <p>suspend-webhook - webhook used to suspend argo workflows (and in the future other types of workloads supporting the suspend flag)</p> </li> <li> <p>shadow-pods - controller used to support streaming logs in Argo Workflows and KFP.</p> </li> <li> <p>clustermetrics - a CRD and controller that provide basic cluster metrics info for each node in a cluster, designed to work together with KubeStellar sync/status sync mechanisms.</p> </li> <li> <p>mc-scheduling - A Multi-cluster scheduling framework supporting pluggable schedulers.</p> </li> </ul>"},{"location":"direct/galaxy-intro/#kubeflow-pipelines-v2","title":"KubeFlow Pipelines v2","text":""},{"location":"direct/galaxy-intro/#argo-workflows","title":"Argo Workflows","text":""},{"location":"direct/galaxy-intro/#learn-more","title":"Learn More","text":"<p>To learn more visit the repository at https://github.com/kubestellar/galaxy _Note that all the code in the galaxy repo is experimental and is available on an as-is basis </p>"},{"location":"direct/get-started/","title":"Getting Started with KubeStellar","text":"<p>This pages shows one concrete example of steps 2--7 from the full Installation and Usage outline. This example produces a simple single-host system suitable for kicking the tires, using kind to create three new clusters to serve as your KubeFlex hosting cluster and two WECs. This page concludes with forwarding you to one example of the remaining steps.</p> <ol> <li>Setup<ol> <li>Install software prerequisites</li> <li>Cleanup from previous runs</li> <li>Create the KubeFlex hosting cluster and Kubestellar core components</li> <li>Create and register two WECs.</li> </ol> </li> <li>Exercise KubeStellar</li> </ol>"},{"location":"direct/get-started/#setup","title":"Setup","text":"<p>This is one way to produce a very simple system, suitable for study but not production usage. For general setup information, see the full story.</p>"},{"location":"direct/get-started/#install-software-prerequisites","title":"Install software prerequisites","text":"<p>The following command will check for the prerequisites that you will need for the later steps. See the prerequisites doc for more details.</p> <pre><code>bash &lt;(curl https://raw.githubusercontent.com/kubestellar/kubestellar/v0.23.1/hack/check_pre_req.sh) kflex ocm helm kubectl docker kind\n</code></pre> <p>This setup recipe uses kind to create three Kubernetes clusters on your machine. Note that <code>kind</code> does not support three or more concurrent clusters unless you raise some limits as described in this <code>kind</code> \"known issue\": Pod errors due to \u201ctoo many open files\u201d.</p>"},{"location":"direct/get-started/#cleanup-from-previous-runs","title":"Cleanup from previous runs","text":"<p>If you have run this recipe or any related recipe previously then you will first want to remove any related debris. The following commands tear down the state established by this recipe.</p> <pre><code>kind delete cluster --name kubeflex\nkind delete cluster --name cluster1\nkind delete cluster --name cluster2\nkubectl config delete-context kind-kubeflex\nkubectl config delete-context cluster1\nkubectl config delete-context cluster2\n</code></pre>"},{"location":"direct/get-started/#set-the-version-appropriately-as-an-environment-variable","title":"Set the Version appropriately as an environment variable","text":"<pre><code>export KUBESTELLAR_VERSION=0.24.0-alpha.2\n</code></pre>"},{"location":"direct/get-started/#create-a-kind-cluster-to-host-kubeflex","title":"Create a kind cluster to host KubeFlex","text":"<p>For convenience, a new local Kind cluster that satisfies the requirements for playing the role of KubeFlex hosting cluster can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v0.24.0-alpha.2/scripts/create-kind-cluster-with-SSL-passthrough.sh) --name kubeflex --port 9443\n</code></pre>"},{"location":"direct/get-started/#use-core-helm-chart-to-initialize-kubeflex-and-create-its-and-wds","title":"Use Core Helm chart to initialize KubeFlex and create ITS and WDS","text":"<pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n--version $KUBESTELLAR_VERSION \\\n--set-json='ITSes=[{\"name\":\"its1\"}]' \\\n--set-json='WDSes=[{\"name\":\"wds1\"}]'\n</code></pre> <p>That command will print some notes about how to get kubeconfig \"contexts\" named \"its1\" and \"wds1\" defined. Do that, because those contexts are used in the following.</p>"},{"location":"direct/get-started/#create-and-register-two-workload-execution-clusters","title":"Create and register two workload execution cluster(s)","text":"<p>The following steps show how to create two new <code>kind</code> clusters and  register them with the hub as described in the  official open cluster management docs.</p> <p>Note that <code>kind</code> does not support three or more concurrent clusters unless you raise some limits as described in this <code>kind</code> \"known issue\": Pod errors due to \u201ctoo many open files\u201d.</p> <ol> <li> <p>Execute the following commands to create two kind clusters, named <code>cluster1</code> and <code>cluster2</code>, and register them with the OCM hub. These clusters will serve as workload clusters. If you have previously executed these commands, you might already have contexts named <code>cluster1</code> and <code>cluster2</code>. If so, you can remove these contexts using the commands <code>kubectl config delete-context cluster1</code> and <code>kubectl config delete-context cluster2</code>.</p> <pre><code>: set flags to \"\" if you have installed KubeStellar on an OpenShift cluster\nflags=\"--force-internal-endpoint-lookup\"\nclusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkind create cluster --name ${cluster}\nkubectl config rename-context kind-${cluster} ${cluster}\nclusteradm --context its1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${cluster}/\" | awk '{print $0 \" --context '${cluster}' --singleton '${flags}'\"}' | sh\ndone\n</code></pre> <p>The <code>clusteradm</code> command grabs a token from the hub (<code>its1</code> context), and constructs the command to apply the new cluster  to be registered as a managed cluster on the OCM hub.</p> </li> <li> <p>Repeatedly issue the command:</p> <pre><code>kubectl --context its1 get csr\n</code></pre> <p>until you see that the certificate signing requests (CSR) for both cluster1 and cluster2 exist.  Note that the CSRs condition is supposed to be <code>Pending</code> until you approve them in step 4.</p> </li> <li> <p>Once the CSRs are created, approve the CSRs complete the cluster registration with the command:</p> <pre><code>clusteradm --context its1 accept --clusters cluster1\nclusteradm --context its1 accept --clusters cluster2\n</code></pre> </li> <li> <p>Check the new clusters are in the OCM inventory and label them:</p> <pre><code>kubectl --context its1 get managedclusters\nkubectl --context its1 label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context its1 label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre> </li> </ol>"},{"location":"direct/get-started/#exercise-kubestellar","title":"Exercise KubeStellar","text":"<p>Proceed to Scenario 1 (multi-cluster workload deployment with kubectl) in the example scenarios after defining the shell variables that characterize the setup done above. Following are setting for those variables, whose meanings are defined at the start of the example scenarios document.</p> <pre><code>host_context=kind-kubeflex\nits_cp=its1\nits_context=its1\nwds_cp=wds1\nwds_context=wds1\nwec1_name=cluster1\nwec2_name=cluster2\nwec1_context=$wec1_name\nwec2_context=$wec2_name\nlabel_query_both=location-group=edge\nlabel_query_one=name=cluster1\n</code></pre>"},{"location":"direct/init-hosting-cluster/","title":"Initializing the KubeFlex hosting cluster","text":"<p>The KubeFlex implementation has to be installed in the cluster chosen to play the role of KubeFlex hosting cluster. This can be done in any of the following ways.</p>"},{"location":"direct/init-hosting-cluster/#bundled-with-cluster-creation","title":"Bundled with cluster creation","text":"<p>As mentioned earlier, there are a couple of ways to both create the hosting cluster and initialize it for KubeFlex in one operation.</p> <ul> <li>Using kflex init --create-kind.</li> <li>curl-to-bash script.</li> </ul>"},{"location":"direct/init-hosting-cluster/#kflex-init","title":"kflex init","text":"<p>The following command will install the KubeFlex implementation in the cluster that <code>kubectl</code> is configured to access, if you have sufficient privileges.</p> <pre><code>kflex init\n</code></pre>"},{"location":"direct/init-hosting-cluster/#using-an-existing-openshift-cluster-as-the-hosting-cluster","title":"Using an existing OpenShift cluster as the hosting cluster","text":"<p>When the hosting cluster is an OpenShift cluster, the recipe for registering a WEC with the ITS (to be written) needs to be modified. In the <code>clusteradm</code> command, omit the <code>--force-internal-endpoint-lookup</code> flag. If following Getting Started literally, this means to define <code>flags=\"\"</code> rather than <code>flags=\"--force-internal-endpoint-lookup\"</code>.</p>"},{"location":"direct/init-hosting-cluster/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>The KubeStellar core Helm chart will install the KubeFlex implementation in the cluster that <code>kubectl</code> is configured to access, as well as create ITSes and WDSes.</p>"},{"location":"direct/its/","title":"Inventory and Transport Spaces","text":"<p>This document will tell users what they need to know about these.</p> <p>Including the fact that the KubeFlex hosting cluster can play the role of ITS, and how to make that happen.</p> <p>TODO: write this.</p>"},{"location":"direct/kubeflex-intro/","title":"KubeFlex","text":"<p>One of the technologies underlying KubeStellar is KubeFlex, a kubernetes-based platform designed to:</p> <ul> <li>Provide lightweight Kube API Server instances and selected controllers as a service.</li> <li>Provide a flexible architecture for the storage backend</li> <li>Offer flexibility in choice of API Server build</li> <li>Present a single binary command line interface for improved user experience</li> </ul> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as:</p> <ul> <li> <p>k8s: a basic Kubernetes API Server with a subset of kube controllers.  The control plane in this context does not execute workloads, such as pods,  because the controllers associated with these objects are not activated.  This environment is referred to as \u2018denatured\u2019 because it lacks the typical  characteristics and functionalities of a standard Kubernetes cluster It uses about 350 MB of memory per instance with a shared Postgres Database Backend.</p> </li> <li> <p>vcluster: a virtual cluster that runs on the hosting cluster,  based on the  vCluster Project. This type of control  plane can run pods using worker nodes of the hosting cluster.</p> </li> <li> <p>host: the KubeFlex hosting cluster, which is exposed as a control plane.</p> </li> <li> <p>external: an external cluster that is imported as a control plane (this is in the roadmap but not yet implemented)</p> </li> <li> <p>ocm: a control plane that uses the  multicluster-controlplane project  for managing multiple clusters.</p> </li> </ul> <p>When using KubeFlex, users interact with the API server of the hosting cluster to create or delete control planes. KubeFlex defines a ControlPlane CRD that represents a Control Plane.</p>"},{"location":"direct/kubeflex-intro/#learn-more","title":"Learn More","text":"<p>To explore more fully KubeFlex's capabilities visit the repository at https://github.com/kubestellar/kubeflex</p> <p>There is also a introductory video about KubeFlex on the KubeStellar YouTube Channel</p> <p> image info </p>"},{"location":"direct/packaging/","title":"Packaging and Delivery","text":""},{"location":"direct/packaging/#outline-of-github-repositories","title":"Outline of GitHub repositories","text":"<p>The following is a graph of the GitHub repositories in the <code>kubestellar</code> GitHub organization and the dependencies among them. The repo at the tail of an arrow depends on the repo at the head of the arrow. These are not just build-time dependencies but any reference from one repo to another.</p> flowchart LR     kubestellar --&gt; kubeflex     kubestellar --&gt; ocm-status-addon     ocm-status-addon --&gt; kubestellar     ocm-transport-plugin --&gt; kubestellar     kubestellar --&gt; ocm-transport-plugin  <p>The references from ocm-status-addon to kubestellar are only in documentation and are in the process of being removed (no big difficulty is anticipated).</p>"},{"location":"direct/packaging/#kubeflex","title":"KubeFlex","text":"<p>See the GitHub repo.</p>"},{"location":"direct/packaging/#ocm-status-addon","title":"OCM Status Addon","text":"<p>The OCM Status Addon repo is the source of an Open Cluster Management Addon. It builds one image that has two subcommands that tell it which role to play in that framework: the controller (which runs in the OCM hub, the KubeStellar ITS) or the agent.</p>"},{"location":"direct/packaging/#outline-of-ocm-status-addon-publishing","title":"Outline of OCM status addon publishing","text":"flowchart LR     subgraph \"ocm-status-addon@GitHub\"     osa_code[OSA source code]     osa_hc_src[OSA Helm chart source]     end     osa_ctr_image[OSA container image] --&gt; osa_code     osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src     osa_hc_src -.-&gt; osa_ctr_image     osa_hc_repo -.-&gt; osa_ctr_image  <p>The dashed dependencies are at run time, not build time.</p> <p>\"OSA\" is OCM Status Addon.</p>"},{"location":"direct/packaging/#ocm-status-addon-container-image","title":"OCM status addon container image","text":"<p>There is a container image at ghcr.io/kubestellar/ocm-status-addon. This image can operate as either controller or agent.</p> <p>In its capacity as controller, the code in this image can emit YAML for a Deployment object that runs the OCM Status Add-On Agent. The compiled code has an embedded copy of <code>pkg/controller/manifests</code>, which includes the YAML source for the agent Deployment.</p> <p>The container image is built and published by that repository's release process, which is documented at its <code>docs/release.md</code> file.</p> <p>By our development practices and not doing any manual hacks, we maintain the association that a container image tagged with <code>$VERSION</code> is built from the Git commit that has the Git tag <code>v$VERSION</code>.</p> <p>To support testing, <code>make ko-local-build</code> will build a single-platform image and not push it, only leave it among your Docker images. The single platform's OS is Linux. The single platform's ISA is defined by the <code>make</code> variable <code>ARCH</code>, which defaults to what <code>go env GOARCH</code> prints.</p>"},{"location":"direct/packaging/#ocm-status-addon-helm-chart","title":"OCM status addon Helm chart","text":"<p>The OCM Status Add-On Controller is delivered by a Helm chart at ghcr.io/kubestellar/ocm-status-addon-chart. The chart references the container image.</p> <p>By our development practices and doing doing any manual hacks, we maintain the association that the OCI image tagged <code>v$VERSION</code> contains a Helm chart that declares its <code>version</code> and its <code>appVersion</code> to be <code>v$VERSION</code> and the templates in that chart include a Deployment for the OCM Status Add-On Agent using the container image <code>ghcr.io/kubestellar/ocm-status-addon:$VERSION</code>.</p>"},{"location":"direct/packaging/#ocm-transport-plugin","title":"OCM Transport Plugin","text":"<p>The source is the GitHub repo github.com/kubestellar/ocm-transport-plugin</p>"},{"location":"direct/packaging/#ocm-transport-container-image","title":"OCM Transport container image","text":"<p>This appears at ghcr.io/kubestellar/ocm-transport-plugin/transport-controller.</p> <p>TODO: document how the image is built and published, including explain versioning.</p>"},{"location":"direct/packaging/#kubestellar","title":"KubeStellar","text":""},{"location":"direct/packaging/#warning","title":"WARNING","text":"<p>Literal KubeStellar release numbers appear here, and are historical. The version of this document in a given release does not mention that release. See the release process for more details on what self-references are and are not handled.</p>"},{"location":"direct/packaging/#outline-of-publishing","title":"Outline of publishing","text":"<p>The following diagram shows most of it. For simplicity, this omits the clusteradm and the Helm CLI container images.</p> flowchart LR     osa_hc_repo[published OSA Helm Chart]     otp_code[\"OTP source in GitHub\"]     subgraph ks_repo[\"kubestellar@GitHub\"]     kcm_code[KCM source code]     gtc_code[\"generic transportcontroller code\"]     kcm_hc_src[KCM Helm chart source]     ksc_hc_src[KS Core Helm chart source]     ks_pch[kubestellar PostCreateHook]     ocm_pch[\"ocm PostCreateHook\"]     setup_ksc[\"example setupusing core\"]     setup_steps[\"example setupstep-by-step\"]     e2e_local[\"E2E setuplocal\"]     e2e_release[\"E2E setuprelease\"]     end     ocm_pch -.-&gt; osa_hc_repo     kcm_ctr_image[KCM container image] --&gt; kcm_code     kcm_hc_repo[published KCM Helm Chart] --&gt; kcm_hc_src     kcm_hc_src -.-&gt; kcm_ctr_image     kcm_hc_repo -.-&gt; kcm_ctr_image     ks_pch -.-&gt; kcm_hc_repo     ks_pch -.-&gt; otp_hc_repo[published OTP Helm chart]     ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src     ksc_hc_src -.-&gt; osa_hc_repo     ksc_hc_src -.-&gt; otp_hc_repo     ksc_hc_src -.-&gt; kcm_hc_repo     ksc_hc_repo -.-&gt; osa_hc_repo     ksc_hc_repo -.-&gt; otp_hc_repo     ksc_hc_repo -.-&gt; kcm_hc_repo     setup_steps -.-&gt; ocm_pch     setup_steps -.-&gt; ks_pch     setup_steps -.-&gt; KubeFlex     setup_ksc -.-&gt; ksc_hc_repo     setup_ksc -.-&gt; KubeFlex     e2e_local -.-&gt; ocm_pch     e2e_local -.-&gt; kcm_code     e2e_local -.-&gt; kcm_hc_src     e2e_local -.-&gt; gtc_code     e2e_local -.-&gt; otp_code     e2e_local -.-&gt; KubeFlex     e2e_release -.-&gt; ocm_pch     e2e_release -.-&gt; ks_pch     e2e_release -.-&gt; KubeFlex  <p>The following diagram shows the parts involving the clusteradm and Helm CLI container images.</p> flowchart LR     subgraph helm_repo[\"helm/helm@GitHub\"]     helm_src[\"helm source\"]     end     subgraph cladm_repo[\"ocm/clusteradm@GitHub\"]     cladm_src[\"clusteradm source\"]     end     subgraph ks_repo[\"kubestellar@GitHub\"]     ksc_hc_src[KS Core Helm chart source]     ks_pch[kubestellar PostCreateHook]     ocm_pch[\"ocm PostCreateHook\"]     setup_steps[\"example setupstep-by-step\"]     e2e_local[\"E2E setuplocal\"]     e2e_release[\"E2E setuprelease\"]     end     helm_image[\"ks/helm image\"] --&gt; helm_src     cladm_image[\"ks/clusteradm image\"] --&gt; cladm_src     ocm_pch -.-&gt; helm_image     ocm_pch -.-&gt; cladm_image     ks_pch -.-&gt; helm_image     ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src     ksc_hc_src -.-&gt; helm_image     ksc_hc_src -.-&gt; cladm_image     ksc_hc_repo -.-&gt; cladm_image     ksc_hc_repo -.-&gt; helm_image     setup_steps -.-&gt; ocm_pch     setup_steps -.-&gt; ks_pch     e2e_local -.-&gt; ocm_pch     e2e_release -.-&gt; ocm_pch     e2e_release -.-&gt; ks_pch  <p>The dashed dependencies are at run time, not build time.</p> <p>\"KCM\" is the KubeStellar controller-manager.</p> <p>NOTE: among the references to published artifacts, some have a   version that is maintained in Git while others have a placeholder in   Git that is replaced in the publishing process. See the release   document for more details. This is an on-going matter   of development.</p>"},{"location":"direct/packaging/#local-copy-of-kubestellar-git-repo","title":"Local copy of KubeStellar git repo","text":"<p>NOTE: Because of a restriction in one of the code generators that we use, a contributor needs to have their local copy of the git repo in a directory whose pathname ends with the Go package name --- that is, ends with <code>/github.com/kubestellar/kubestellar</code>.</p>"},{"location":"direct/packaging/#derived-files","title":"Derived files","text":"<p>Some files in the kubestellar repo are derived from other files there. Contributors are responsible for invoking the commands to (re)derive the derived files as necessary.</p> <p>Some of these derived files are derived by standard generators from the Kubernetes milieu. A contributor can use the following command to make all of those, or use the individual <code>make</code> commands described in the following subsubsections to update particular subsets.</p> <pre><code>make all-generated\n</code></pre> <p>The following command, which we aspire to check in CI, checks whether all those derived files have been correctly derived. It must be invoked in a state where the <code>git status</code> is clean, or at least the dirty files are irrelevant; the current commit is what is checked. This command has side-effects on the filesystem like <code>make all-generated</code>.</p> <pre><code>hack/verify-codegen.sh\n</code></pre>"},{"location":"direct/packaging/#files-generated-by-controller-gen","title":"Files generated by controller-gen","text":"<ul> <li> <p><code>make manifests</code> generates the CustomResourceDefinition files,   which exist in two places:   <code>config/crd/bases</code> and   <code>pkg/crd/files</code>.</p> </li> <li> <p><code>make generate</code> generates the deep copy code, which exists in   <code>zz_generated.deepcopy.go</code> next to the API source.</p> </li> </ul>"},{"location":"direct/packaging/#files-generated-by-code-generator","title":"Files generated by code-generator","text":"<p>The files in <code>pkg/generated</code> are generated by k/code-generator. This generation is done at development time by the command <code>make codegenclients</code>.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-container-image","title":"KubeStellar controller-manager container image","text":"<p>KubeStellar has one container image, for what is called the KubeStellar controller-manager. For each WDS, KubeStellar has a pod running that image. It installs the needed custom resource definition objects if they are not already present, and is a controller-manager hosting the per-WDS controllers (binding controller and status controller) from the kubestellar repo.</p> <p>The image repository is <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>.</p> <p>By our development practices and not doing any manual hacking we maintain the association that the container image tagged <code>$VERSION</code> is built from the Git commit having the Git tag <code>v$VERSION</code>.</p> <p>The release process builds and publishes that container image.</p> <p><code>make ko-build-controller-manager-local</code> will make a local image for just the local platform. This is used in local testing.</p>"},{"location":"direct/packaging/#clusteradm-container-image","title":"clusteradm container image","text":"<p>The kubestellar GitHub repository has a script, <code>hack/build-clusteradm-image.sh</code>, that creates and publishes a container image holding the <code>clusteradm</code> command from OCM. The source of the container image is read from the latest release of github.com/open-cluster-management-io/clusteradm, unless a command line flag says to use a specific version. This script also pushes the built container image to quay.io/kubestellar/clusteradm using a tag that equals the ocm/clusteradm version that the image was built from.</p> <p>This image is used by the ocm PostCreateHook to initialize an ITS as an Open Cluster Management hub.</p>"},{"location":"direct/packaging/#helm-cli-container-image","title":"Helm CLI container image","text":"<p>The container image at <code>quay.io/kubestellar/helm:3.14.0</code> was built by <code>hack/build-helm-image.sh</code>.</p>"},{"location":"direct/packaging/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>This Helm chart is instantiated in a pre-existing Kubernetes cluster and (1) makes it into a KubeFlex hosting cluster and (2) sets up a requested collection of WDSes and ITSes. See the core chart doc. This chart is defined in the <code>core-chart</code> directory and published to <code>ghcr.io/kubestellar/kubestellar/core-chart</code>.</p> <p>This Helm chart defines and uses two KubeFlex PostCreateHooks, as follows.</p> <ul> <li> <p><code>its</code> defines a Job with two containers. One container uses the clusteradm container image to initialize the target cluster as an OCM \"hub\". The other container uses the Helm CLI container image to instantiate the OCM Status Addon Helm chart. The version to use is defined in the <code>values.yaml</code> of the core chart. This PCH is used for every requested ITS.</p> </li> <li> <p><code>wds</code> defines a Job with two containers. One container uses the Helm CLI image to instantiate the KubeStellar controller-manager Helm chart. The other container uses the Helm CLI image to instantiate the OCM Transport Helm chart. For both of those subsidiary charts, the version to use is defined in the <code>values.yaml</code> of the core chart. This PCH is used for every requested WDS.</p> </li> </ul> <p>By our development practices and not doing any manual hacking, we maintain the association that the OCI image tagged <code>$VERSION</code> contains a Helm chart that declares its <code>version</code> and its <code>appVersion</code> to be <code>$VERSION</code> and instantiates version <code>$VERSION</code> of the KubeStellar controller-manager Helm chart.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-helm-chart","title":"KubeStellar controller-manager Helm Chart","text":"<p>There is a Helm chart that is designed to be instantiated in a KubeFlex hosting cluster, once per WDS. The focus of the chart is getting the KubeStellar controller-manager installed.</p> <p>The source for the Helm chart is in the <code>chart/</code> directory. <code>make chart</code> (re)derives it from local sources. This is not included in <code>make all-generated</code>.</p> <p>This chart creates (among other things) a <code>Deployment</code> object that runs a container from the KubeStellar controller-manager container image.</p> <p>The chart is published at the OCI repository <code>ghcr.io/kubestellar/kubestellar/controller-manager-chart</code>. A GitHub Actions workflow (in <code>.github/workflows/goreleaser.yml</code>) specializes and publishes this chart as part of the release process.</p> <p>By our development practices and not doing any manual hacking, we maintain the association that the OCI image tagged <code>$VERSION</code> contains a Helm chart that declares its <code>version</code> and its <code>appVersion</code> to be <code>$VERSION</code> and that chart has a Deployment that uses the kubestellar-controller-manager container image tagged <code>$VERSION</code>.</p>"},{"location":"direct/packaging/#kubeflex-postcreatehooks","title":"KubeFlex PostCreateHooks","text":"<p>In addition to the two PostCreateHooks in the core Helm chart described above, there are two more PostCreateHooks defined in the <code>config/postcreate-hooks/</code> directory.</p> <p>These two PostCreateHooks were used in the \"step-by-step\" variant of the example setup instructions (which are currently out of service). Those instructions told the user to use the PCH sources from a KubeStellar release on GitHub. The version is a literal in the instructions and is updated in the process of preparing for a KubeStellar release. The step-by-step variant is intended to be deleted (in favor of using the core Helm chart) by the time the next release is made.</p> <p>The setup script for E2E testing (<code>test/e2e/setup-kubestellar.sh</code>) uses the local copy of the <code>ocm</code> PCH unconditionally and uses the local copy of the <code>kubestellar</code> PCH when testing a release.</p>"},{"location":"direct/packaging/#ocm-postcreatehook","title":"ocm PostCreateHook","text":"<p>The PostCreateHook defined in <code>config/postcreate-hooks/ocm.yaml</code> gets used on an ITS to do two things: (1) initialize that space as an OCM \"hub\", using the image <code>quay.io/kubestellar/clusteradm:0.8.2</code>, and (2) install KubeStellar's OCM Status Add-On Controller there. The clusteradm image (described above) includes the <code>clusteradm</code> CLI release <code>v0.8.2</code> which is bundled with the OCM release <code>v0.13.2</code>. Part (2) uses the Helm CLI in the container image at <code>quay.io/kubestellar/helm:v3.14.0</code> to instantiate the OCM Status Add-On Helm chart that was published at <code>ghcr.io/kubestellar/ocm-status-addon-chart</code>.</p>"},{"location":"direct/packaging/#kubestellar-postcreatehook","title":"kubestellar PostCreateHook","text":"<p>The PostCreateHook defined in <code>config/postcreate-hooks/kubestellar.yaml</code> is intended to be used in the hosting cluster, once per WDS, and defines a <code>Job</code> that has two containers. One uses the Helm CLI image to instantiate the KubeStellar controller-manager Helm chart. The chart version appears as a literal in the PCH definition and is manually updated during the process of creating a release (see the release process document). The other container uses the Helm CLI image to instantiate OCM Transport Helm chart. The version to instantiate appears as a literal in the PCH definition and is manually updated after each OTP release (see the release process doc).</p>"},{"location":"direct/packaging/#scripts-and-instructions","title":"Scripts and instructions","text":"<p>There are instructions for using a release (Getting Started document) and a setup script for end-to-end testing(<code>test/e2e/common/setup-kubestellar.sh</code>). The end-to-end testing can either test the local copy/version of the kubestellar repo or test a release. So there are three cases to consider.</p>"},{"location":"direct/packaging/#example-setup-instructions","title":"Example setup instructions","text":"<p>There were two variants of the setup instructions for the examples: an older one --- which is out of service at the moment, is called \"step-by-step\", and uses the <code>ocm</code> and <code>kubestellar</code> PostCreateHooks --- and Getting Started, which uses the core Helm chart. The latter is the preferred method, and is the only one described here.</p> <p>The instructions are a Markdown file that displays commands for a user to execute. These start with commands that define environment variables that hold the release of ks/kubestellar and of ks/ocm-transport-plugin to use.</p> <p>The instructions display a command to instantiate the core Helm chart, at the version in the relevant environment variable, requesting the creation of one ITS and one WDS.</p> <p>The instructions display commands to update the user's kubeconfig file to have contexts for the ITS and the WDS created by the chart instance. These commands use the KubeFlex CLI (<code>kflex</code>). There is also a script under development that will do the job using <code>kubectl</code> instead of <code>kflex</code>; when it appears, the instructions will display a curl-to-bash command that fetches the script from GitHub using a version that appears as a literal in the instructions and gets manually updated as part of making a new release.</p>"},{"location":"direct/packaging/#e2e-setup-for-testing-a-release","title":"E2E setup for testing a release","text":"<p>When setting up to test a release, the setup script uses the following KubeStellar pieces.</p> <p>The script creates the <code>ocm</code> and <code>kubestellar</code> PCHes from the local YAML for them. The script uses these PCHes in <code>kflex</code> commands that create one ITS and one WDS, respectively.</p>"},{"location":"direct/packaging/#e2e-setup-for-testing-local-copyversion","title":"E2E setup for testing local copy/version","text":"<p>When setting up to test a release, the setup script uses the following KubeStellar pieces.</p> <p>The script creates the <code>ocm</code> PCH from the local YAML for it. The script uses this PCH in a <code>kflex</code> command that create one ITS.</p> <p>The scripts builds a local kubestellar controller-manager container image from local sources. Then the script loads that image into the KubeFlex hosting cluster (e.g., using <code>kind load</code>).</p> <p>The script temporarily updates the local kubestellar controller-manager Helm chart to reference the kubestellar controller-manager container image that was loaded into the hosting cluster. Then the script invokes the Helm CLI to instantiate that chart in the hosting cluster, configured to apply to the WDS being set up. Then the script partially undoes its temporary modification of the kubestellar controller-manager Helm chart, using <code>git checkout --</code>.</p> <p>The script builds a local container image for the OCM transport controller from (a) local sources for the generic part and (b) the transport plugin in a ks/ocm-transport-plugin release identified by a literal version number in the script. This version is updated as part of tracking a ks/OTP release. Then the script loads this container image into the hosting cluster. Then the setup script invokes <code>scripts/deploy-transport-controller.sh</code>, which creates a Deployment object that runs the published transport controller image using a version that appears as a literal in the script and is manually updated in the process of reacting to a new ks/OTP release.</p>"},{"location":"direct/packaging/#amalgamated-graph","title":"Amalgamated graph","text":"<p>Currently only showing kubestellar and ocm-status-addon.</p> <p>Again, omitting clusteradm and Helm CLI container images for simplicity.</p> <p>TODO: finish this</p> flowchart LR     otp_code[\"OTP source in GitHub\"]     subgraph osa_repo[\"ocm-status-addon@GitHub\"]     osa_code[OSA source code]     osa_hc_src[OSA Helm chart source]     end     osa_ctr_image[OSA container image] --&gt; osa_code     osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src     osa_hc_src -.-&gt; osa_ctr_image     osa_hc_repo -.-&gt; osa_ctr_image     subgraph ks_repo[\"kubestellar@GitHub\"]     kcm_code[KCM source code]     gtc_code[\"generic transportcontroller code\"]     kcm_hc_src[KCM Helm chart source]     ksc_hc_src[KS Core Helm chart source]     ks_pch[kubestellar PostCreateHook]     ocm_pch[\"ocm PostCreateHook\"]     setup_ksc[\"example setupusing core\"]     setup_steps[\"example setupstep-by-step\"]     e2e_local[\"E2E setuplocal\"]     e2e_release[\"E2E setuprelease\"]     end     osa_repo -.-&gt; ks_repo     ocm_pch -.-&gt; osa_hc_repo     kcm_ctr_image[KCM container image] --&gt; kcm_code     kcm_hc_repo[published KCM Helm Chart] --&gt; kcm_hc_src     kcm_hc_src -.-&gt; kcm_ctr_image     kcm_hc_repo -.-&gt; kcm_ctr_image     ks_pch -.-&gt; kcm_hc_repo     ks_pch -.-&gt; otp_hc_repo[published OTP Helm chart]     ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src     ksc_hc_src -.-&gt; osa_hc_repo     ksc_hc_src -.-&gt; kcm_hc_repo     ksc_hc_src -.-&gt; otp_hc_repo     ksc_hc_repo -.-&gt; osa_hc_repo     ksc_hc_repo -.-&gt; kcm_hc_repo     ksc_hc_repo -.-&gt; otp_hc_repo     setup_steps -.-&gt; ocm_pch     setup_steps -.-&gt; ks_pch     setup_steps -.-&gt; KubeFlex     setup_ksc -.-&gt; ksc_hc_repo     setup_ksc -.-&gt; KubeFlex     e2e_local -.-&gt; ocm_pch     e2e_local -.-&gt; kcm_code     e2e_local -.-&gt; kcm_hc_src     e2e_local -.-&gt; gtc_code     e2e_local -.-&gt; otp_code     e2e_local -.-&gt; KubeFlex     e2e_release -.-&gt; ocm_pch     e2e_release -.-&gt; ks_pch     e2e_release -.-&gt; KubeFlex  <p>Every dotted line is a reference that must be versioned. How do we keep all those versions right?</p> <p>Normally a git tag is an immutable reference to an immutable git commit. Let's not violate that.</p> <p>Can/should we say that an OCI image (or whatever) tag equals the tag of the commit that said image (or whatever) was built from? While keeping <code>main</code> always a working system?</p>"},{"location":"direct/pr-signoff/","title":"How to Sign-off on Your Pull Requests","text":"<p>In order to get your pull requests approved, you must first complete a DCO sign-off. This process is defined by the CNCF, and there are two cases: individual contributors and contributors that work for a corporate CNCF member. To do this as an individual contributor, you must have a GPG and SSH key. Basic setup instructions can be found below (For more detailed instructions, refer to the Github GPG and SSH setup pages):</p> <p>Before starting, make sure that your user email is verified on Github. To check for this:</p> <ol> <li>Login to Github and navigate to your Github Settings page</li> <li>In the sidebar, open the Emails tab</li> <li>Emails associated with Github should be listed at the top of the page under the \"Emails\" label</li> <li>An unverified email would have an \"Unverified\" label under it in orange text</li> <li>To verify, click Resend verification email and follow its prompts</li> <li>Navigate back to your Emails page, if the \"Unverified\" label is no longer there, then you're good to go!</li> </ol> <p></p> <p>Git Bash is also highly recommended.</p> <p></p>"},{"location":"direct/pr-signoff/#setting-up-the-gpg-key","title":"Setting up the GPG Key","text":"<ol> <li>Install GnuPG (the GPG command line tool).</li> <li>Binary releases for your specific OS can be found here after scrolling down to the Binary Releases section (i.e. Gpg4win on Windows, Mac GPG for macOS, etc).</li> <li> <p>After downloading the installer, follow the prompts to set up GnuPG.</p> </li> <li> <p>Open Git Bash (or your CLI of choice) and use the following command to generate your GPG key pair:     <pre><code>gpg --full-generate-key\n</code></pre></p> </li> <li>If prompted to specify the size, type, and duration of the key that you want, press <code>Enter</code> to select the default option.</li> <li>Once prompted, enter your user info and a passphrase:</li> <li>Make sure to list your email as the same one that's verified by Github</li> <li>Use the following command to list the long form of your generated GPG keys:    <pre><code>gpg --list-secret-keys --keyid-format=long\n</code></pre></li> <li>Your GPG key ID should be the characters on the output line starting with <code>sec</code>, beginning directly after the <code>/</code> and ending before the listed date.<ul> <li>For example, in the output below (from the Github GPG setup page), the GPG key ID would be <code>3AA5C34371567BD2</code> <pre><code>$ gpg --list-secret-keys --keyid-format=long\n /Users/hubot/.gnupg/secring.gpg\n ------------------------------------\n sec   4096R/3AA5C34371567BD2 2016-03-10 [expires: 2017-03-10]\nuid                          Hubot &lt;hubot@example.com&gt;\n ssb   4096R/4BB6D45482678BE3 2016-03-10\n</code></pre></li> </ul> </li> <li>Copy your GPG key ID and run the command below, replacing <code>[your_GPG_key_ID]</code> with the key ID you just copied:    <pre><code>gpg --armor --export [your_GPG_key_ID]\n</code></pre></li> <li>This should generate an output with your GPG key. Copy the characters starting from <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending at <code>--END PGP PUBLIC KEY BLOCK-----</code> (inclusive) to your clipboard.</li> <li>After copying or saving your GPG key, navigate to Settings in your Github</li> <li>Navigate to the SSH and GPG keys page under the Access section in the sidebar</li> <li>Under GPG keys, select New GPG key<ul> <li>Enter a suitable name for your key under \"Title\" and paste your GPG key that you copied/saved in Step 7 under \"Key\".</li> <li>Once done, click Add GPG key</li> </ul> </li> <li>Your new GPG key should now be displayed under GPG keys.</li> </ol> <p></p>"},{"location":"direct/pr-signoff/#setting-up-the-ssh-key","title":"Setting up the SSH Key","text":"<ol> <li> <p>Open Git Bash (or your CLI of choice) and use the following command to generate your new SSH key (make sure to replace <code>your_email</code> with your Github-verified email address):    <pre><code>ssh-keygen -t ed25519 -C \"your_email\"\n</code></pre></p> </li> <li> <p>Press <code>Enter</code> to select the default option if prompted to set a save-file or passphrase for the key (you may choose to enter a passphrase if desired; this will prompt you to enter the passphrase everytime you perform a DCO sign-off).</p> </li> <li>The following output should generate a randomart image </li> <li>Use the following command to copy the new SSH key to your clipboard:    <pre><code>clip &lt; ~/.ssh/id_ed25519.pub\n</code></pre></li> <li>After copying or saving your SSH key, navigate to Settings in your Github.</li> <li>Navigate to the SSH and GPG keys page under the Access section in the sidebar.</li> <li>Under SSH keys, select New SSH key.</li> <li>Enter a suitable name for your key under \"Title\"</li> <li>Open the dropdown menu under \"Key type\" and select Signing Key</li> <li>Paste your SSH key that you copied/saved in Step 3 under \"Key\"</li> <li>Your new SSH key should now be displayed under SSH keys.</li> <li>Optional: To test if your SSH key is connecting properly or not, run the following command in your CLI (more specific instructions can be found in the Github documentation):    <pre><code>ssh -T git@github.com\n</code></pre></li> <li>If given a warning saying something like <code>The authenticity of the host '[host IP]' can't be established</code> along with a key fingerprint and a prompt to continue, verify if the provided key fingerprint matches any of those listed here</li> <li>Once you've verified the match, type <code>yes</code></li> <li>If the resulting message says something along the lines of <code>Hi [User]! You've successfully authenticated, but GitHub does not provide shell access.</code>, then it means your SSH key is up and ready.</li> </ol> <p></p>"},{"location":"direct/pr-signoff/#creating-pull-requests","title":"Creating Pull Requests","text":"<p>Whether it's editing files from Kubestellar.io or directly from the Kubestellar Github, there are a couple steps to follow that streamlines the workflow of your PR:</p> <ol> <li>Changes made to any file are automatically committed to a new branch in your fork.</li> <li>When committing, make sure to specify the type of PR at the beginning of your commit message (i.e. :bug: if it addresses a bug-type issue)</li> <li> <p>If the PR addresses a specific issue that has already been opened in the github, make sure to include the opened issue in additional comments (i.e. \"fixes Issue #XXXX\")</p> </li> <li> <p>Click Propose Changes after writing the commit message, review your changes, and then create the PR.</p> </li> <li>If your PR addresses an already opened issue on the github, make sure to close the issue once your PR is approved and closed.</li> </ol> <p></p>"},{"location":"direct/pr-signoff/#pull-request-sign-off","title":"Pull Request Sign-off","text":"<p>NOTE: \"sign-off\" is different from \"signing\" a commit or tag (see the git book about signing). The former indicates your assent to the repo's terms for contributors, the latter adds a cryptograph checksum that is rarely displayed.</p> <p>Your submitted PR must pass the automated checks in order to be reviewed. This requires for you to perform a DCO sign-off for your PR. The following instructions provide a basic walkthrough if you have already set up your GPG and SSH keys:</p> <ol> <li> <p>Navigate to the Code page of the Kubestellar github.</p> </li> <li> <p>Click the Fork dropdown in the top right corner of the page.</p> </li> <li>Under \"Existing Forks\" click your fork (should look something like \"your_username/kubestellar\")</li> <li>Once in your fork, click the Code dropdown.</li> <li>Under the \"Local\" tab at the top of the dropdown, select the SSH tab</li> <li>Copy the SSH repo URL to your clipboard</li> <li>Open Git Bash (or your CLI of choice), create or change to a different directory if desired.</li> <li>Clone the repository using <code>git clone</code> followed by pasting the URL you just copied.</li> <li>Change your directory to the Kubestellar repo using <code>cd kubestellar</code>.</li> <li><code>git checkout</code> to the branch in your fork where the changes were committed.</li> <li>The branch name should be written at the top of your submitted PR page and looks something like \"patch-X\" (where \"X\" should be the number of PRs made on your fork to date)</li> <li>Once in your branch, type <code>git commit -s --amend</code> to sign off your PR.</li> <li>You may replace <code>--amend</code> with a <code>-m</code> followed by a commit message if you desire; the <code>--amend</code> simply uses the same commit message as the one you wrote when initially submitting the PR</li> <li>If prompted with a sign-off page in your Git Bash (or alternative CLI), type <code>:wq!</code> to exit the prompt</li> <li>Type <code>git push -f origin [branch_name]</code>, replacing <code>[branch_name]</code> with the actual name of your branch.</li> <li>Navigate back to your PR github page.<ul> <li>A green <code>dco-signoff: yes</code> label indicates that your PR is successfully signed</li> </ul> </li> </ol>"},{"location":"direct/pre-reqs/","title":"KubeStellar Prerequisites","text":"<p>The following prerequisites are required. You can use the check-pre-req script, to validate if all needed prerequisites are installed.</p>"},{"location":"direct/pre-reqs/#infrastructure-clusters","title":"Infrastructure (clusters)","text":"<p>Because of its multicluster architecture, KubeStellar requires that you have the necessary privileges and infrastructure access to create and/or configure the necessary Kubernetes clusters. These are the following; see the architecture document for more details.</p> <ul> <li>One cluster to serve as the KubeFlex hosting cluster.</li> <li>Any additional Kubernetes clusters that are not created by KubeFlex but you will use as a WDS or ITS.</li> <li>Your WECs.</li> </ul> <p>Our documentation has remarks about using the following sorts of clusters:</p> <ul> <li>kind</li> <li>k3s</li> <li>openshift </li> </ul>"},{"location":"direct/pre-reqs/#software-prerequisites-for-using-kubestellar","title":"Software Prerequisites: for Using KubeStellar","text":"<ul> <li> <p>kubeflex version 0.6.1 or higher     To install kubeflex go to https://github.com/kubestellar/kubeflex/blob/main/docs/users.md#installation. To upgrade from an existing installation, follow these instructions. At the end of the install make sure that the kubeflex CLI, kflex, is in your <code>$PATH</code>.</p> </li> <li> <p>OCM CLI (clusteradm)     To install OCM CLI use:</p> <pre><code>curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\n</code></pre> <p>Note that the default installation of clusteradm will install in /usr/local/bin which will require root access. If you prefer to avoid root, you can specify an alternative installation location using the INSTALL_DIR environment variable, as follows:</p> <pre><code>mkdir -p ocm\nexport INSTALL_DIR=\"$PWD/ocm\"\ncurl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\nexport PATH=$PWD/ocm:$PATH\n</code></pre> <p>At the end of the install make sure that the OCM CLI, clusteradm, is in your <code>$PATH</code>.</p> </li> <li> <p>helm - to deploy the Kubestellar and kubeflex charts</p> </li> <li>kubectl - to access the kubernetes clusters</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-the-getting-started-setup","title":"Additional Software for the Getting Started setup","text":"<ul> <li>kind</li> <li>docker (or compatible docker engine that works with kind)</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-running-the-examples","title":"Additional Software For Running the Examples","text":"<ul> <li>argocd - for the examples that use it</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-building-kubestellar-from-source","title":"Additional Software For Building KubeStellar from Source","text":"<ul> <li>go version 1.21 or higher - to build Kubestellar</li> <li>make - to build Kubestellar and create the Kubestellar container images</li> <li>ko - to create some of the Kubestellar container images</li> <li>docker (or equivalent that implements <code>docker buildx</code>) - to create other KubeStellar container images</li> </ul> <p>To build and test KubeStellar properly, you will also need</p> <ul> <li>kind</li> <li>OCP</li> </ul>"},{"location":"direct/pre-reqs/#automated-check-of-prerequisites-for-kubestellar","title":"Automated Check of Prerequisites for KubeStellar","text":"<p>The check_pre_req script offers a convenient way to check for the prerequisites needed for KubeStellar deployment and use.</p> <p>The script checks for a prerequisite presence in the <code>$PATH</code>, by using the <code>which</code> command, and it can optionally provide version and path information for prerequisites that are present, or installation information for missing prerequisites.</p> <p>We envision that this script could be useful for user-side debugging as well as for asserting the presence of prerequisites in higher-level automation scripts.</p> <p>The script accepts a list of optional flags and arguments.</p>"},{"location":"direct/pre-reqs/#supported-flags","title":"Supported flags:","text":"<ul> <li><code>-A|--assert</code>: exits with error code 2 upon finding the first missing prerequisite</li> <li><code>-L|--list</code>: prints a list of supported prerequisites</li> <li><code>-V|--verbose</code>: displays version and path information for installed prerequisites or installation information for missing prerequisites</li> <li><code>-X</code>: enable <code>set -x</code> for debugging the script</li> </ul>"},{"location":"direct/pre-reqs/#supported-arguments","title":"Supported arguments:","text":"<p>The script accepts a list of specific prerequisites to check, among the list of available ones:</p> <pre><code>$ check_pre_req.sh --list\nargo brew docker go helm jq kflex kind ko kubectl make ocm yq\n</code></pre>"},{"location":"direct/pre-reqs/#examples","title":"Examples","text":"<p>For example, list of prerequisites required by KubeStellar can be checked with the command below (add the <code>-V</code> flag to get the version of each program and a suggestions on how to install missing prerequisites):</p> <pre><code>$ hack/check_pre_req.sh\nChecking pre-requisites for using KubeStellar:\n\u2714 Docker\n\u2714 kubectl\n\u2714 KubeFlex\n\u2714 OCM CLI\n\u2714 Helm\nChecking additional pre-requisites for running the examples:\n\u2714 Kind\nX ArgoCD CLI\nChecking pre-requisites for building KubeStellar:\n\u2714 GNU Make\n\u2714 Go\n\u2714 KO\n</code></pre> <p>In another example, a specific list of prerequisites could be asserted by a higher-level script, while providing some installation information, with the command below (note that the script will terminate upon finding a missing prerequisite):</p> <pre><code>$ check_pre_req.sh --assert --verbose helm argo docker kind\nChecking KubeStellar pre-requisites:\n\u2714 Helm\n  version: version.BuildInfo{Version:\"v3.14.0\", GitCommit:\"3fc9f4b2638e76f26739cd77c7017139be81d0ea\", GitTreeState:\"clean\", GoVersion:\"go1.21.5\"}\npath: /usr/sbin/helm\nX ArgoCD CLI\n  how to install: https://argo-cd.readthedocs.io/en/stable/cli_installation/\n</code></pre>"},{"location":"direct/release-notes/","title":"Release notes","text":"<p>The following sections list the known issues for each release. The issue list is not differential (i.e., compared to previous releases) but a full list representing the overall state of the specific release. </p>"},{"location":"direct/release-notes/#0240-and-its-candidates-and-alphas","title":"0.24.0 and its candidates and alphas","text":"<p>The main change from 0.23.X is the completion of the status combination and introduction of the create-only feature. There is also further work on the organization of the website.</p>"},{"location":"direct/release-notes/#0240-alpha2","title":"0.24.0-alpha.2","text":"<p>The first of several releases required to get the create-only feature implemented in both the ks/ks and ks/OTP repositories. The API for this feature is present but the implementation is incomplete: the <code>Binding</code> objects are correctly derived from the <code>BindingPolicy</code> objects but the transport controller does not implement the feature.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0240","title":"Remaining limitations in 0.24.0","text":"<ul> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0231","title":"0.23.1","text":"<p>The main change from 0.23.0 is a re-organization of the website, which is still a work in progress, and archival of all website content that is outdated.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0231","title":"Remaining limitations in 0.23.1","text":"<ul> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0230-and-its-release-candidates","title":"0.23.0 and its release candidates","text":"<p>The main change is introduction of the an all-in-one chart, called the core chart, for installing KubeStellar in a given hosting cluster and creating an initial set of WDSes and ITSes.</p> <p>This release also introduces a preliminary API for combining workload object reported state from the WECs --- BUT THE IMPLEMENTATION IS NOT DONE*. The control objects can be created but the designed response is not there. The design of the control objects is likely to change in the future too (without change in the Kubernetes API group's version string). In short, stay away from this feature in this release.</p> <p>This release also features better observability (<code>/metrics</code> and <code>/debug/pprof</code>) and control over client-side self-restraint (request QPS and burst).</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0230-and-its-release-candidates","title":"Remaining limitations in 0.23.0 and its release candidates","text":"<ul> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0220-and-its-release-candidates","title":"0.22.0 and its release candidates","text":"<p>The changes include adding the following features.</p> <ul> <li>Custom WEC-independent transformations of workload objects on their way from WDS to WEC.</li> <li>WEC-dependent Go template expansion in the strings of a workload object on its way from WDS to WEC.</li> <li><code>PriorityClass</code> objects (from API group <code>scheduling.k8s.io</code>) propagate now.</li> <li>Support multiple WDSes.</li> <li>Allow multiple ITSes.</li> <li>Use the new Helm chart from kubestellar/ocm-transport-plugin for deploying the transport controller.</li> </ul> <p>Prominent bug fixes include more discerning cleaning of workload objects on their way from WDS to WEC. This includes keeping a \"headless\" <code>Service</code> headless and removing the <code>spec.suspend</code> field from a <code>Job</code>.</p> <p>See the changelogs on GitHub for full details.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0220-and-its-release-candidates","title":"Remaining limitations in 0.22.0 and its release candidates","text":"<ul> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0212-and-its-release-candidates","title":"0.21.2 and its release candidates","text":"<p>The changes since 0.21.1 include efficiency improvements, reducing costs of running the kubestellar-controller-manager for a WDS that is an OpenShift cluster. There are also bug fixes and documentation improvements.</p>"},{"location":"direct/release-notes/#0211","title":"0.21.1","text":"<p>This release mainly updates the documentation exposed under kubestellar.io.</p>"},{"location":"direct/release-notes/#0210-and-its-release-candidates","title":"0.21.0 and its release candidates","text":""},{"location":"direct/release-notes/#major-changes-for-0210-and-its-release-candidates","title":"Major changes for 0.21.0 and its release candidates","text":"<ul> <li>This release introduces pluggable transport. Currently the only plugin is the OCM transport plugin.</li> </ul>"},{"location":"direct/release-notes/#bug-fixes-in-0210-and-its-release-candidates","title":"Bug fixes in 0.21.0 and its release candidates","text":"<ul> <li>dynamic changes to WECs are supported. Existing Bindings and ManifestWorks will be updated when new WECs are added/updated/delete or when labels are added/updated/deleted on existing WECs</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is handled correctly.</li> <li>These changes that happen while a controller is down are handled correctly:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set;</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0210-and-its-release-candidates","title":"Remaining limitations in 0.21.0 and its release candidates","text":"<ul> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0200-and-its-release-candidates","title":"0.20.0 and its release candidates","text":"<ul> <li>Dynamic changes to WECs are not supported. Existing ManifestWorks will not be updated when new WECs are added or when labels are added/deleted on existing WECs</li> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is not handled correctly.</li> <li>Some operations are not handled correctly while the controller is down:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set, it will not be handled correctly.</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets is not handled correctly.</li> </ul>"},{"location":"direct/release-testing/","title":"Testing a new KubeStellar Release","text":"<p>The following testing process should be applied to every new KubeStellar release in order to validate it, this include both regular releases and release candidates. All the tests should be done while the KubeStellar code is still under code-freeze and new code shouldn't be merged into the main branch until all tests are passed and the release is officially declared as ready. In case the release tests fail (even one of them), the release should be declared as unstable and a fix through a new release candidate should be worked on ASAP. The KubeStellar code-freeze should be lifted only after all tests are passed and a the release was completed. To reduce the exposure of unstable releases the update of the KubeStellar site kubestellar.io should be done only once all release tests passed successfully. </p>"},{"location":"direct/release-testing/#release-tests","title":"Release tests","text":"<p>The following section describe the tests that must be executed for each release.</p> <p>Our release tests consists of:    * Automatic tests running on Ubuntu X86 (see below)    * Manually initiated tests running on OCP (TODO: add specific version and machine details)</p> <p>Due to the lack of OCP based automatic testing, these tests will be performed only once a release candidate passed all other tests and is a candidate to become a regular release. </p> <p>Note:  We plan to automate all release tests in the future</p>"},{"location":"direct/release-testing/#automatic-github-based-release-tests","title":"Automatic (github based) release tests","text":"<p>KubeStellar CICD automatically runs a set of e2e tests on each new release. Currently these tests include 2 main test types bash based e2e tests and ginkgo based e2e tests. The bash test basically tests the scenario of  multi-cluster workload deployment with kubectl. The ginkgo test cover the Singleton status test, and several other tests that are listed in the test README. Note, however, that the content of the releases tests may be changed in the future. We will refer to those tests as the e2e release tests.  The automatic tests are running on github hosted runners of type Ubuntu latest (currently 22.04) X86 64 bit  Note: When a new release is created please verify that the automatic tests indeed executed and passed. </p>"},{"location":"direct/release-testing/#e2e-release-tests-on-ocp","title":"e2e release tests on OCP","text":"<p>As many of the KubeStellar customers are using OCP, the release tests should be executed on an OCP cluster as well. Currently these tests should be initiated manually on a dedicated OCP cluster that is reserved for the release testing process. </p> <p>TODO: The details on how to setup and run the test </p>"},{"location":"direct/release-testing/#other-platforms","title":"Other platforms","text":"<p>KubeStellar is also used on other platforms such as ARM64, MacOS, etc.. Currently these platforms are not part of the routine release testing, however the KubeStellar team will try its best to help and solve issues detected on other platforms as well. Users should go through the regular procedure of opening issues against the KubeStellar project .</p>"},{"location":"direct/release/","title":"Making KubeStellar Releases","text":"<p>This document defines how releases of the KubeStellar repository are made. This document is a work-in-progress. In particular, the dependency cycle between the <code>kubestellar</code> and <code>ocm-tansport-plugin</code> repos is not well documented and we do not have a good way to deal with it.</p> <p>This document starts with step-by-step instructions for the current procedure, then proceeds with the thinking behind them.</p> <p>See the associated packaging and delivery doc for some clues about the problem.</p> <p>Every release should pass all release tests before it can be officially declare as a new stable release. Please see the details in release-testing.</p>"},{"location":"direct/release/#step-by-step","title":"Step-by-Step","text":""},{"location":"direct/release/#reacting-to-a-new-ocm-transport-plugin-release","title":"Reacting to a new ocm-transport-plugin release","text":"<p>Between each release of ks/OTP and the next release of ks/ks, the following steps should be done in ks/ks.</p> <ul> <li> <p>Edit <code>scripts/deploy-transport-controller.sh</code>: update the tag in the default transport controller image setting (<code>export TRANSPORT_CONTROLLER_IMAGE...</code>) to the latest release of ks/OTP.</p> </li> <li> <p>Edit <code>config/postcreate-hooks/kubestellar.yaml</code>: update the version of the OTP Helm chart.</p> </li> <li> <p>Edit <code>core-chart/values.yaml</code>: update the OTP version.</p> </li> <li> <p>Edit <code>docs/content/direct/examples.md</code> (<code>docs/content/direct/common-setup-core-chart.md</code>, <code>docs/content/direct/common-setup-step-by-step.md</code>): update the version in the <code>export OCM_TRANSPORT_PLUGIN=...</code> statement to the latest release of ks/OTP.</p> </li> <li> <p>Edit <code>test/e2e/common/setup-kubestellar.sh</code>: update the setting of <code>OCM_TRANSPORT_PLUGIN_RELEASE</code> to the latest.</p> </li> </ul>"},{"location":"direct/release/#reacting-to-a-new-ocm-status-addon-release","title":"Reacting to a new ocm-status-addon release","text":"<p>Between each release of ks/OSA and the next release of ks/ks, update the references to the ocm-status-addon release in the following files.</p> <ul> <li><code>config/postcreate-hooks/ocm.yaml</code></li> <li><code>core-chart/values.yaml</code></li> </ul>"},{"location":"direct/release/#making-a-new-kubestellar-release","title":"Making a new kubestellar release","text":"<p>Making a new kubestellar release requires a contributor to do the following things. Here <code>$version</code> is the semver identifier for the release (e.g., <code>1.2.3-rc2</code>).</p> <ul> <li> <p>If not already in effect, declare a code freeze. There should be nothing but bug fixes and doc improvements while working towards a regular release.</p> </li> <li> <p>Edit <code>docs/mkdocs.yml</code> and update the definition of <code>ks_latest_release</code> to <code>$version</code> (e.g., <code>'0.23.0-rc42'</code>). If this is a regular release then also update the definition of <code>ks_latest_regular_release</code>.</p> </li> <li> <p>Edit the source for the KCM PCH (in <code>config/postcreate-hooks/kubestellar.yaml</code>) and update the tag in the reference to the KCM container image (it appears in the last object, a <code>Job</code>).</p> </li> <li> <p>Update the version in the core chart defaults, <code>core-chart/values.yaml</code>.</p> </li> <li> <p>Until we have our first stable release, edit the oldocs README(<code>oldocs/README.md</code>, section \"latest-stable-release\") where it wishes it could cite a stable release but instead cites the latest release, to refer to the coming release.</p> </li> <li> <p>Edit the release notes in <code>docs/content/direct/release-notes.md</code>.</p> </li> <li> <p>Make a new Git commit with those changes and get it into the right branch in the shared repo (through the regular PR process if not authorized to cheat).</p> </li> <li> <p>Wait for successful completion of the testing after that merge.</p> </li> <li> <p>Apply the Git tag <code>v$version</code> to that new commit in the shared repo.</p> </li> <li> <p>After that, the \"goreleaser\" GitHub workflow then creates and publishes the artifacts for that release (as discussed above) and then the \"Test latest release\" workflow will run the E2E tests using those artifacts. </p> </li> <li> <p>Verify that the automatic tests indeed executed and passed (see more details in CICD release testing)</p> </li> <li> <p>After the release artifacts have been published, create and push to the shared repo a branch named <code>release-$version</code>. This will also trigger the workflow that tests the latest release. Every push to a branch with such a name triggers that workflow, in case there has been a change in an E2E test for that release.</p> </li> <li> <p>Follow the procedure in OCP testing, to verify that the release is functional on OCP.</p> </li> <li> <p>If the test results are good and the release is regular (not an RC) then declare the code freeze over.</p> </li> <li> <p>If the testing results are good, update ks/OTP to refer to the new ks/ks release and make a new release of ks/OTP.</p> </li> </ul>"},{"location":"direct/release/#goals-and-limitations","title":"Goals and limitations","text":"<p>The release process has the following goals.</p> <ul> <li>A release is identified using semantic versioning. This means that the associated semantics are followed, in terms of what sort of changes to the repo require what sort of changes to the release identifier.</li> <li>A user can pick up and use a given existing release without being perturbed by on-going contributor work. A release is an immutable thing.</li> <li>A release with a given semver identifier is built from a commit of this Git repository tagged with a tag whose name is \"v\" followed by the release identifier.</li> <li>The contents of <code>main</code> always work. This includes passing CI tests. This includes documentation being accurate. We allow point-in-time specific documentation, such as a document that says \"Here is how to use release 1.2.3\" --- which would refer to a release made in the past. We do not require the documentation in <code>main</code> to document all releases.</li> <li>A git tag is immutable. Once associated with a given Git commit, that association is not changed later.</li> <li>We do not put self-references into Git. For example, making release <code>1.2.3</code> does not require changing any file in Git to have the string <code>1.2.3</code> in it.</li> </ul> <p>We have the following limitations.</p> <ul> <li>The only way to publish artifacts (broadly construed, not (necessarily) GitHub \"release artifacts\") is to make a release.</li> <li>The only way to test published artifacts is to make a release and test it.</li> <li>Thus, it is necessary to keep users clearly appraised of the quality (or status of evaluating the quality) of each release.</li> <li>Because of the lack of self references, most user instructions (e.g., examples) and tests do not have concrete release identifiers in them; instead, the user has to chose and supply the release identifier. There can also be documentation of a specific past release (e.g., the latest stable release) that uses the literal identifier for that past release.</li> <li>PAY ATTENTION TO THIS ONE: Because of the prohibition of self references, Git will not contain the exact bytes of our Helm chart definitions. Where a Helm chart states its own version or has a container image reference to an image built from the same release, the bytes in Git have a placeholder for that image's tag and the process of creating the published release artifacts fills in that placeholder. Think of this as being analogous to the linking done when building a binary executable file.</li> <li>The design below falls short of the goal of not putting self-references in files under Git control. One way is in the KubeFlex PostCreateHook that installs the kubestellar-controller-manager (KCM), where the version of the container image for the KCM appears. Another is in the examples document, which also holds references to its own release. Additional self-references are in the <code>docs/content/direct/README.md</code> and <code>docs/content/direct/deploy-on-k3d.md</code>.</li> </ul>"},{"location":"direct/release/#dependency-cycle-with-ksotp","title":"Dependency cycle with ks/OTP","text":"<p>The ks/ks repo and the ks/OTP repo reference each other. Thus, making consistent immutable recursive-self-reference-free releases is impossible. We have to compromise somehow. There is some discussion in ks/ks Issue 1786. We currently seem to be following the staggered release approach.</p>"},{"location":"direct/release/#technology","title":"Technology","text":"<p>There is a GitHub workflow that creates the published artifacts for each Git tag whose name starts with \"v\". The rest of the tag name is required to be a semver release identifier. Note that this document does not (yet, anyway) specify how that GitHub workflow gets its job done. This workflow is confusingly named \"goreleaser\" and in a file named \"goreleaser.yml\" and has a job named \"goreleaser\" despite the fact that it does more than use goreleaser.</p> <p>For each tag <code>v$version</code> the following published artifacts will be created.</p> <ul> <li>The container image for the kubestellar-controller-manager (KCM), at <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>. Image tag will be <code>$version</code>. This GitHub \"package\" will be connected to the ks/ks repo (this connection is something that an admin will do once, it will stick for all versions).</li> <li>The Helm chart (for installing the KCM for a WDS), at <code>ghcr.io/kubestellar/kubestellar/controller-manager-chart</code> with version <code>$version</code> and Helm \"appVersion\" <code>$version</code>. This GitHub \"package\" will also be connected to the ks/ks repo. The chart has a reference to container image for the KCM and that reference is <code>ghcr.io/kubestellar/kubestellar/controller-manager:$version</code>. In Git the chart has only placeholders in these places, not <code>$version</code>; the <code>$version</code> is inserted into a distinct copy by the GitHub workflow, which then publishes this specialized copy.</li> <li>Note that there is no automation (yet) concerning the KubeFlex PostCreateHook that installs the KCM.</li> </ul>"},{"location":"direct/release/#website","title":"Website","text":"<p>We use <code>mike</code> and <code>MkDocs</code> to derive and publish GitHub pages. See <code>docs/README.md</code> for details.</p> <p>The published GitHub pages are organized into \"releases\".  Each release in the GitHub pages corresponds to a git branch whose name begins with \"release-\" or is \"main\".</p> <p>Our documentation is, mostly, viewable in either of two ways. The source documents can be viewed directly through GitHub's web UI for files. The other way is through the website.</p>"},{"location":"direct/release/#testing-and-examples","title":"Testing and Examples","text":"<p>The unit tests (of which we have almost none right now), integration tests (of which we also have very few), and end-to-end (E2E) tests in this repository are run in the context of a local copy of this repository and test that version of this repository --- not using any published release artifacts. Additionally, some E2E tests have the option to test published artifacts instead of the local copy of this repo.</p> <p>The end-to-end tests include ones written in <code>bash</code>, and these are the only documentation telling a user how to use the present version of this repository. Again, these tests do not use any published artifacts from a release of this repo.</p> <p>We have another category of tests, release tests. These test a given release, using the published artifacts of that release. Currently all the release tests are a subset of the E2E tests --- those that can be told to test published artifacts. In particular, they can test the published artifacts reached through the kubestellar PostCreateHook, which contains an explicit reference to one particular release (as explained elsewhere in this document).</p> <p>We have GitHub workflows that exercise the E2E tests, normally on the copy of the repo that the workflow applies to. However, these workflows are parameterized and can be told to test the released artifacts instead.</p> <p>We also have a GitHub workflow, named \"Test latest release\" in <code>.github/workflows/test-latest-release.yml</code>, that invokes those E2E tests on the latest release. This workflow can be triggered manually, and is also configured to run after completion of the workflow (\"goreleaser\") that publishes release artifacts.</p> <p>We will maintain a document that lists releases that pass our quality bar. The latest of those is thus the latest stable release. This document is updated in <code>main</code> as quality evaluations come in.</p> <p>We used to maintain a statement of what is the latest stable release in <code>docs/content/direct/README.md</code>.</p> <p>We maintain a Getting Started document that tells users how to exercise the release that the document appears in. This requires a self-reference that is updated as part of the release process.</p>"},{"location":"direct/release/#policy","title":"Policy","text":"<p>We aim for all regular releases to be working. In order to do that, we have to make test releases and test them. The widely recognized pattern for doing that is to make \"release candidates\" (i.e., releases for testing purposes) <code>1.2.3-rc0</code>, <code>1.2.3-rc1</code>, <code>1.2.3-rc2</code>, and so on, while trying to get to a quality release <code>1.2.3</code>. Once one of them is judged to be of passing quality, we make a release without the <code>-rc&lt;N&gt;</code> suffix. Due to the self-reference in the KCM PostCreateHook, this will involve making a new commit.</p> <p>Right after making a release we test it thoroughly.</p>"},{"location":"direct/release/#deliberately-feature-incomplete-releases","title":"Deliberately feature-incomplete releases","text":"<p>We plan a few deliberately feature-incomplete releases. They will be regular releases as far as the technology here is concerned. They will be announced only to selected users who acknowledge that they are getting something that is incomplete. In GitHub, these will be marked as \"pre-releases\". The status of these releases will be made clear in their documentation (which currently appears in the release notes.</p>"},{"location":"direct/release/#website_1","title":"Website","text":"<p>We aim to keep the documents viewable both through the website and GitHub's web UI for viewing files. We aim for all of the documentation to be reachable on the website and in the GitHub file UI starting from the repository's README.md.</p> <p>We create a release in the GitHub pages for every release. A patch release is a release. A test release is a release. Creating that GitHub pages release is done by creating a git branch named <code>release-$version</code>.</p>"},{"location":"direct/release/#future-process-development","title":"Future Process Development","text":"<p>We intend to get rid of the self-reference in the KCM PCH, as follows. Define a Helm chart for installing the PCH. Update the release workflow to specialize that Helm chart, similarly to the specialization done for the KCM Helm chart.</p>"},{"location":"direct/release/#open-questions","title":"Open questions","text":"<p>Exactly when does a new release branch diverge from <code>main</code>? What about cherry-picking between <code>main</code> and the latest (or also earlier?) release branch?</p> <p>What about the clusteradm container image?</p>"},{"location":"direct/setup-limitations/","title":"Setup Limitations","text":"<p>Note:  This section is under construction and includes partial information.</p>"},{"location":"direct/setup-limitations/#size-considerations","title":"Size considerations","text":"<p>As KubeStellar is built on top of Kubernetes all Kubernetes limitations and recommendations apply to KubeStellar as well. These recommendations can be found in Kubernetes Considerations for large clusters.</p>"},{"location":"direct/setup-overview/","title":"Setting up KubeStellar","text":"<p>\"Setup\" is a porous grouping of some of the steps in the full outline, and comprises the following. Also, bear in mind the Setup limitations.</p> <ul> <li>Install software prerequisites. See prerequisites.</li> <li>KubeFlex Hosting cluster<ul> <li>Acquire the ability to use a Kubernetes cluster to serve as the KubeFlex hosting cluster. See Acquire cluster for KubeFlex hosting.</li> <li>Initialize that cluster as a KubeFlex hosting cluster.</li> </ul> </li> <li>Core Spaces<ul> <li>Create an Inventory and Transport Space (ITS).</li> <li>Create a Workload Description Space (WDS).</li> </ul> </li> <li>Core Helm Chart (covering three of the above topics).</li> <li>Workload Execution Clusters<ul> <li>Create a Workload Execution Cluster (WEC).</li> <li>Register the WEC in the ITS.</li> </ul> </li> </ul>"},{"location":"direct/transforming/","title":"Transforming Desired State","text":"<p>This document is for users of a release. Examples of using the latest release are in the example scenarios document. This document adds information not conveyed in the examples.</p> <p>KubeStellar has two kinds of transformations of desired workload state on its way from WDS to WEC: one kind is independent of the WEC, and the other supports variation from WEC to WEC.</p>"},{"location":"direct/transforming/#wec-independent-workload-object-transformation","title":"WEC-independent workload object transformation","text":"<p>KubeStellar does some transformation of workload objects on their way from WDS to WEC. First, there are transformations that are independent of the destination; these are described in this section. Second, there is customization to the WEC, described later.</p> <p>The WEC-independent transformations are removal of certain content.</p> <p>There are three categories of these transformations, as follows. They are applied in this order.</p> <ol> <li>Transformations that are built into KubeStellar and apply to all workload objects.</li> <li>Transformations that are built into KubeStellar and apply to specific kinds of workload objects.</li> <li>Transformations that are configured by control objects and apply to specific kinds of workload objects.</li> </ol>"},{"location":"direct/transforming/#transformations-for-all-workload-objects","title":"Transformations for all workload objects","text":"<p>The following are applied to every workload object.</p> <ol> <li>Remove the following fields from <code>metadata</code>: <code>managedFields</code>, <code>finalizers</code>, <code>generation</code>, <code>ownerReferences</code>, <code>selfLink</code>, <code>resourceVersion</code>, <code>UID</code>, <code>generateName</code>.</li> <li>Remove the annotation named <code>kubectl.kubernetes.io/last-applied-configuration</code>.</li> <li>Remove the <code>status</code>.</li> </ol>"},{"location":"direct/transforming/#built-in-transformations-of-specific-kinds-of-workload-object","title":"Built-in transformations of specific kinds of workload object","text":"<p>In a <code>Service</code> (core API group) object:</p> <ol> <li> <p>remove the following fields from <code>spec</code>: <code>ipFamilies</code>, <code>externalTrafficPolicy</code>, <code>internalTrafficPolicy</code>, <code>ipFamilyPolicy</code>, <code>sessionAffinity</code>. Also remove the <code>nodePort</code> field from every port unless the annotation <code>kubestellar.io/annotations/preserve=nodeport</code> is present;</p> </li> <li> <p>in the <code>spec</code> remove the field <code>clusterIP</code> unless it is present with value \"None\".</p> </li> <li> <p>in the <code>spec</code>: if the field <code>clusterIPs</code> (which holds an array of strings) is present and those strings include \"None\" then keep it present holding only \"None\", otherwise remove that field if it is present.</p> </li> </ol> <p>In a <code>Job</code> (API group <code>batch</code>) object, remove the following things.</p> <ol> <li> <p><code>spec.selector</code></p> </li> <li> <p><code>spec.suspended</code></p> </li> <li> <p>In <code>metadata</code>, the annotation named <code>batch.kubernetes.io/job-tracking</code></p> </li> <li> <p>In <code>metadata</code> and in <code>spec.template.metadata</code>, the labels named <code>controller-uid</code> or <code>batch.kubernetes.io/controller-uid</code>.</p> </li> </ol>"},{"location":"direct/transforming/#configured-transformation-of-workload-objects","title":"Configured transformation of workload objects","text":"<p>The user can configure additional transformations of workload objects by putting <code>CustomTransform</code> (in the <code>control.kubestellar.io</code> API group) objects in the WDS. Each <code>CustomTransform</code> object binds to certain workload objects and specifies certain transformations.</p> <p>Currently the binding is simply by naming the workload object's API group and \"resource\" name in the <code>CustomTransform</code>'s <code>spec</code>. The transformations from all of the bound <code>CustomTransform</code> objects are applied to the workload object. There should be at most one <code>CustomTransform</code> object that specifies a given API group and resource.</p> <p>Currently the only available transformations are removals of specified content. The content to be removed is identified by a small subset of JSONPath (which was originally and somewhat loosely defined in an article by Stefan Goessner and later defined more carefully in RFC 9535). In the subset accepted here: the root node identifier (<code>$</code>) must be followed by a positive number of segments, where each segment is either (a) <code>.</code> and a name (a <code>member-name-shorthand</code>, in the grammar of the RFC) or (b) <code>[</code>, a string literal, and <code>]</code>; no more of the grammar is allowed, not even whitespace. The allowed names and string literals are as specified in RFC 9535, except that only double-quoted strings are allowed.</p> <p>For example, the following <code>CustomTransform</code> object says to remove the <code>spec</code> field named <code>suspend</code> from <code>Job</code> objects (in the API group <code>batch</code>).</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: CustomTransform\nmetadata:\nname: example\nspec:\napiGroup: batch\nresource: jobs\nremove:\n- \"$.spec.suspend\"\n</code></pre>"},{"location":"direct/transforming/#rule-based-customization","title":"Rule-based customization","text":"<p>KubeStellar can distribute one workload object to multiple WECs, and it is common for users to need some customization to each WEC. By rule based we mean that the customization is not expressed via one or more literal expressions but rather can refer to properties of each WEC by property name. As KubeStellar distributes or transports a workload object from WDS to a WEC, the object can be transformed in a way that depends on those properties.</p> <p>At its current level of development, KubeStellar has a simple but limited way to specify rule-based customization, called \"template expansion\".</p>"},{"location":"direct/transforming/#template-expansion","title":"Template Expansion","text":"<p>Template expansion is an optional feature that a user can request on an object-by-object basis. The way to request this feature on an object is to put the following annotation on the object.</p> <pre><code>    control.kubestellar.io/expand-templates: \"true\"\n</code></pre> <p>The customization that template expansion does when distributing an object from a WDS to a WEC is applied independently to each leaf string of the object and is based on the \"text/template\" standard package of Go. The string is parsed as a template and then replaced with the result of expanding the template. Errors from this process are reported in the status field of the Binding object involved. Errors during template expansion usually produce broken YAML, in which case no corresponding object will be created in the WEC.</p> <p>The data used when expanding the template are properties of the WEC. These properties are collected from the following four sources, which are listed in decreasing order of precedence.</p> <ol> <li>The ConfigMap object, if any, that is in the namespace named \"customization-properties\" in the ITS and has the same name as the inventory object for the WEC. In particular, the ConfigMap string and binary data items whose name is valid as a Go language identifier supply properties.</li> <li>The annotations of the inventory item for the WEC supply properties if the annotation's name (AKA key) is valid as a Go language identifier.</li> <li>The labels of the inventory item for the WEC supply properties if the label's name (AKA key) is valid as a Go language identifier.</li> <li>There is a pre-defined property whose name is \"clusterName\" and whose value is the name of the inventory item (i.e., the <code>ManagedCluster</code> object) for the WEC.</li> </ol> <p>A Binding object's <code>status</code> section has a field holding a slice of error message strings reporting user errors that arose the last time the transport controller processed that Binding, along with the <code>observedGeneration</code> reporting the <code>metadata.generation</code> that was processed. For each workload object that the Binding references: if template expansion reports errors for any destinations, the errors reported for the first such destination are included in the Binding object's status.</p> <p>Any failure in any template expansion for a given Binding suppresses propagation of desired state from that Binding; the previously propagated desired state from that Binding, if any, remains in place in the WEC.</p> <p>Template expansion can only be applied when and where the un-expanded leaf strings pass the validation that the WDS applies, and can only express substring replacements.</p> <p>For example, consider the following example workload object.</p> <pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\nname: instance\nnamespace: openshift-logging\nannotations:\ncontrol.kubestellar.io/expand-templates: \"true\"\nspec:\noutputs:\n- name: remote-loki\ntype: loki\nurl: \"https://my.loki.server.com/{\\u007B .clusterName }}-{\\u007B.clusterHash}}\"\n...\n</code></pre> <p>(Note: \"{\\u007B\" is JSON for a string consisting of two consecutive left curly brackets --- which mkdocs does not have a way to quote inside a fenced code block.)</p> <p>The following ConfigMap in the ITS provides a value for the <code>clusterHash</code> property.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nnamespace: customization-properties\nname: virgo\ndata:\nclusterHash: 1001-dead-beef\n...\n</code></pre> <p>When distributed to the virgo WEC, that ClusterLogForwarder would say the following.</p> <pre><code>...\nurl: \"https://my.loki.server.com/virgo-1001-dead-beef\"\n...\n</code></pre>"},{"location":"direct/troubleshooting/","title":"Troubleshooting","text":"<p>This guide is a work in progress.</p>"},{"location":"direct/troubleshooting/#debug-log-levels","title":"Debug log levels","text":"<p>The KubeStellar controllers take an optional command line flag that sets the level of debug logging to emit. Each debug log message is associated with a log level, which is a non-negative integer. Higher numbers correspond to messages that appear more frequently and/or give more details. The flag's name is <code>-v</code> and its value sets the highest log level that gets emitted; higher level messages are suppressed.</p> <p>The KubeStellar debug log messages are assigned to log levels roughly according to the following rules. Note that the various Kubernetes libraries used in these controllers also emit leveled debug log messages, according to their own numbering conventions. The KubeStellar rules are designed to not be very inconsistent with the Kubernetes practice.</p> <ul> <li>0: messages that appear O(1) times per run.</li> <li>1: more detailed messages that appear O(1) times per run.</li> <li>2: messages that appear O(1) times per lifecycle event of an API object or important conjunction of them (e.g., when a Binding associates a workload object with a WEC).</li> <li>3: more detailed messages that appear O(1) times per lifecycle event of an API object or important conjunction of them.</li> <li>4: messages that appear O(1) times per sync. A sync is when a controller reads the current state of one API object and reacts to that.</li> <li>5: more detailed messages that appear O(1) times per sync.</li> </ul> <p>The core Helm chart has \"values\" that set the verbosity (<code>-v</code>) of various controllers.</p>"},{"location":"direct/troubleshooting/#things-to-look-at","title":"Things to look at","text":"<ul> <li>Remember that for each of your BindingPolicy objects, there is a corresponding Binding object that reports what is matching the policy object.</li> <li>Although not part of the interface, when debugging you can look at the ManifestWork and WorkStatus objects in the ITS.</li> <li>Look at logs of controllers. If they have had container restarts that look relevant, look also at the previous logs. Do not forget OCM controllers. Do not forget that some Pods have more than one interesting container.</li> <li>If a controller's <code>-v</code> is not at least 5, increase it.</li> <li>Remember that Kubernetes controllers tend to report transient problems as errors without making it clear that the problem is transient and tend to not make it clear if/when the problem has been resolved (sigh).</li> </ul>"},{"location":"direct/troubleshooting/#making-a-good-trouble-report","title":"Making a good trouble report","text":"<p>Basic configuration information.</p> <ul> <li>Include the versions of all the relevant software; do not forget the OCM pieces.</li> <li>Report on each Kubernetes/OCP cluster involved. What sort of cluster is it (kind, k3d, OCP, ...)? What version of that?</li> <li>For each WDS and ITS involved, report on what sort of thing is playing that role (remember that a Space is a role) --- a new KubeFlex control plane (report type) or an existing cluster (report which one).</li> </ul> <p>Do a simple clean demonstration of the problem, if possible.</p> <p>Show the particulars of something going wrong.</p> <ul> <li>Report timestamps of when salient changes happened. Make it clear which timezone is involved in each one. Particularly interesting times are when KubeStellar did the wrong thing or failed to do anything at all in response to something.</li> <li>Look at the Binding and ManifestWork and WorkStatus objects and the controller logs. Include both in a problem report. Show the relevant workload objects, from WDS and from WEC. When the problem is behavior over time, show the objects contents from before and after the misbehavior.</li> <li>When reporting kube API object contents, include the <code>meta.managedFields</code>. For example, when using <code>kubectl get</code>, include <code>--show-managed-fields</code>.</li> </ul>"},{"location":"direct/usage-limitations/","title":"Usage Limitations","text":"<p>Note:  This section is under construction and includes partial information.</p>"},{"location":"direct/usage-limitations/#size-considerations","title":"Size considerations","text":"<p>The KubeStellar Transport Plugin is built on top of OCM, so KubeStellar also comply to some of OCM's limitations. Users should take into account the following restrictions:</p> <ul> <li>The ManifestWork shouldn't exceed 500KB</li> </ul>"},{"location":"direct/user-guide-intro/","title":"KubeStellar User Guide","text":"<p>This document is an overview of the User Guide. See the KubeStellar overview for architecture and other information.</p> <p>This user guide is an ongoing project. If you find errors, please point them out in our Slack channel or open an issue in our github repository!</p>"},{"location":"direct/user-guide-intro/#simple-example","title":"Simple Example","text":"<p>If you want to try a simple installation process and example then you can try out Getting Started, which uses kind and a helm chart. The helm chart supports many options; the instructions on the Getting Started page show only the chart's usage in that recipe.</p>"},{"location":"direct/user-guide-intro/#in-brief","title":"In Brief","text":"<p>If you want a simple rough grouping, you can divide the concepts here into:</p> <ul> <li>\"setup\" (steps 1--7 below), exemplified in the Setup section of Getting Started, and</li> <li>\"usage\" (the remaining steps), illustrated by the example scenarios document.</li> </ul> <p>However, you do not need to follow that dichotomy. As noted below, the relevant components can be organized more flexibly.</p>"},{"location":"direct/user-guide-intro/#the-full-story","title":"The Full Story","text":"<p>Installing and using KubeStellar progresses through the following steps.</p> <ol> <li>Install software prerequisites. See prerequisites.</li> <li>Acquire the ability to use a Kubernetes cluster to serve as the KubeFlex hosting cluster. See Acquire cluster for KubeFlex hosting.</li> <li>Initialize that cluster as a KubeFlex hosting cluster.</li> <li>Create an Inventory and Transport Space (ITS).</li> <li>Create a Workload Description Space (WDS).</li> <li>Create a Workload Execution Cluster (WEC).</li> <li>Register the WEC in the ITS.</li> <li>Maintain workload desired state in the WDS.</li> <li>Maintain control objects in the WDS to bind workload with WEC and modulate the state propagation back and forth. The API reference documents all of them. There are control objects for the following topics.<ol> <li>Binding workload with WEC(s).</li> <li>Transforming desired state as it travels from WDS to WEC.</li> <li>Summarizing reported state from WECs into WDS.</li> </ol> </li> <li>Enjoy the effects of workloads being propagated to the WEC.</li> <li>Consume reported state from WDS.</li> </ol> <p>By \"maintain\" we mean create, read, update, delete, list, and watch as you like, over time. KubeStellar is eventually consistent: you can change your inputs as you like over time, and KubeStellar continually strives to achieve what you are currently asking it to do.</p> <p>There is some flexibility in the ordering of those steps. The following flowchart shows the key ordering constraints. </p> <p> Ordering among installation and usage actions </p> <p>You can have multiple ITSes, WDSes, and WECs, created and deleted over time as you like.</p> <p>Besides \"Start\", the other green items in that graph are entry points for extending usage at any later time. You could also see them as distinct user roles or authorities.</p> <p>KubeStellar's Core Helm chart combines initializing the KubeFlex hosting cluster, creating some ITSes, and creating some WDSes.</p> <p>You can find an example run through of steps 2--7 in Getting Started. This dovetails with the example scenarios document, which shows examples of the later steps.</p>"},{"location":"direct/user-guide-intro/#troubleshooting","title":"Troubleshooting","text":"<p>See the Troubleshooting guide.</p>"},{"location":"direct/wds/","title":"Workload Description Spaces","text":"<p>This document will tell users what they need to know about these.</p> <p>Including the fact that the KubeFlex hosting cluster can play the role of WDS, and how to make that happen.</p> <p>TODO: write this.</p>"},{"location":"direct/wec-registration/","title":"Registering a Workload Execution Cluster","text":"<p>This document will tell users what how to register a WEC in an ITS.</p> <p>TODO: write this.</p>"},{"location":"direct/wec/","title":"Workload Execution Clusters","text":"<p>This document will tell users what they need to know about these. What make a cluster suitable to be a WEC.</p> <p>TODO: write this.</p>"},{"location":"direct/images/image-files-readme/","title":"How to edit these pictures","text":"<p>The pictures have been created with draw.io, and have been saved in an editable format. You can use draw.io to modify and save back these pictures using an editable format.</p>"}]}